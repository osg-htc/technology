{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OSG Technology Area \u00b6 Welcome to the home page of the OSG Technology Team documentation area! If you are looking for site administrator documentation, please visit the OSG Documentation page . The Team \u00b6 Software and Release Technology Brian Lin (software manager) Brian Bockelman (manager) (15%) Mat Selmeci Derek Weitzel (50%) Matt Westphall (50%) Diego Davila (50%) Tim Theisen (release manager) (50%) Contact Us \u00b6 software-discuss@osg-htc.org - General discussion of the OSG Software stack ( subscribe ) Slack channel - if you can't create an account, send an e-mail to help@osg-htc.org Meetings \u00b6 When: Every Tuesday, 9:30 a.m. (U.S. Central) Where: +1-415-655-0002, PIN: 146 266 9392, https://morgridge.webex.com/webappng/sites/morgridge/meeting/info/791d9dddc5464eb6a73fc7746331d06c (password sent separately) Meeting note archives can be found directly in the GitHub repository .","title":"Home"},{"location":"#osg-technology-area","text":"Welcome to the home page of the OSG Technology Team documentation area! If you are looking for site administrator documentation, please visit the OSG Documentation page .","title":"OSG Technology Area"},{"location":"#the-team","text":"Software and Release Technology Brian Lin (software manager) Brian Bockelman (manager) (15%) Mat Selmeci Derek Weitzel (50%) Matt Westphall (50%) Diego Davila (50%) Tim Theisen (release manager) (50%)","title":"The Team"},{"location":"#contact-us","text":"software-discuss@osg-htc.org - General discussion of the OSG Software stack ( subscribe ) Slack channel - if you can't create an account, send an e-mail to help@osg-htc.org","title":"Contact Us"},{"location":"#meetings","text":"When: Every Tuesday, 9:30 a.m. (U.S. Central) Where: +1-415-655-0002, PIN: 146 266 9392, https://morgridge.webex.com/webappng/sites/morgridge/meeting/info/791d9dddc5464eb6a73fc7746331d06c (password sent separately) Meeting note archives can be found directly in the GitHub repository .","title":"Meetings"},{"location":"documentation/markdown-migration/","text":"Migrating to Markdown \u00b6 As part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to Markdown . The following document outlines the conversion process and conventions. Choosing the git repository \u00b6 First you will need to choose which git repoository you will be working with: If you are converting a document from... Use this github repository... SoftwareTeam technology Release3 docs Once you've chosen the target repository for your document, move onto the next section and pick your conversion method. Automatic TWiki conversion \u00b6 Note If you are only archiving the documents, skip to this section . Choose one of the following methods for converting TWiki documents: Using our own docker conversion image (recommended) Directly using pandoc and mkdocs on your own machine Using docker \u00b6 The twiki-converter docker image can be used to preview the document tree via a mkdocs development server, archive TWiki documents, and convert documents to Markdown via pandoc . The image is available on osghost , otherwise, it is availble on dockerhub . user@host $ docker pull opensciencegrid/docker-twiki-converter Requirements \u00b6 To perform a document migration using docker, you will need the following tools and accounts: Fork and clone the repository that you chose in the above section A host with a running docker service sudo or membership in the docker group If you cannot install the above tools locally, they are available on osghost . Speak with Brian L for access. Preparing the git repository \u00b6 cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/technology.git Create a branch for the document you plan to convert: user@host $ git branch <BRANCH NAME> master Replace <BRANCH NAME> with a name of your choice Change to the branch you just created user@host $ git checkout <BRANCH NAME> Replace <BRANCH NAME> with the name you chose in the step above Previewing the document tree \u00b6 When starting a twiki-converter docker container, it expects your local github repository to be mounted in /source so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image: Create a container from the image with the following command: user@host $ docker run -d -v <PATH TO LOCAL GITHUB REPO>:/source -p 8000 opensciencegrid/docker-twiki-converter Change <PATH TO LOCAL GITHUB REPO> for the directory where you have cloned the repo. The above command should return the container ID, which will be used in subsequent commands. Note If the docker container exits immediately, remove the -d option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode. To find the port that your development server is listening on, use the container ID (you should only need the first few chars of the ID) returned from the previous command: user@host $ docker port <CONTAINER ID> Change <CONTAINER ID> for the value returned by the execution of the previous command Access the development server in your browser via http://osghost.chtc.wisc.edu:<PORT> or localhost:<PORT> for containers run on osghost or locally, respectively. osghost has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the osghost port to a local port. Converting documents \u00b6 The docker image contains a convenience script, convert-twiki for saving archives and converting them to Markdown. To run the script in a running container, run the following command: user@host $ docker exec <CONTAINER ID> convert-twiki <TWIKI URL> Where <CONTAINER ID> is the docker container ID and <TWIKI URL> is the link to the TWiki document that you want to convert, e.g. https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess . This will result in an archive of the twiki doc, archive/SoftwareDevelopmentProcess , in your local repo and a converted copy, SoftwareDevelopmentProcess.md , placed into the root of your local github repository. If the twiki url is for a specific revision of the document, a .rNN will be included in the output filenames. Warning If the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See Troubleshooting conversion for next steps. To see the converted document in your browser: Rename, move the converted document into a folder in docs/ . Document file names should be lowercase, - delimited, and descriptive but concise, e.g. markdown-migration.md or cutting-release.md It's not important to get the name/location correct on the first try as this can be discussed in the pull request sudo chown the archived and converted documents to be owned by you Add the document to the pages: section of mkdocs.yml in APA-style title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' Refresh the document tree in your browser Once you can view the converted document in your browser, move onto the next section Troubleshooting conversion \u00b6 Pandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion. Copy the archive of the document into the root of your git repository Kill the process in the docker container: user@host $ docker exec <CONTAINER ID> pkill -9 pandoc Where <CONTAINER ID> is the docker container ID Remove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy) Run pandoc manually: user@host $ docker exec <CONTAINER ID> pandoc -f twiki -t markdown_github <ARCHIVE COPY> > <MARKDOWN FILE> Where <CONTAINER ID> is the docker container ID, <ARCHIVE COPY> is the the file you copied in the first step and <MARKDOWN FILE> is the resulting .md file Repeat steps 2-4 until you've narrowed down the problematic section Manually convert the offending section Conversion without Docker \u00b6 If you've already used the docker method , skip to the section about completing the conversion . Requirements \u00b6 This method requires the following: Fork and clone the repository that you chose in the above section pandoc (> 1.16) mkdocs MarkdownHighlight pygments Preparing the git repository \u00b6 cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/technology.git Create a branch for the document you plan to convert: user@host $ git branch <BRANCH NAME> master Replace <BRANCH NAME> with a name of your choice 5. Change to the branch you just created user@host $ git checkout <BRANCH NAME> Replace <BRANCH NAME> with the name you chose in the step above Archiving the TWiki document \u00b6 Follow the instructions for archival then continue to the next section to convert the document with pandoc. Initial conversion with Pandoc \u00b6 Pandoc is a tool that's useful for automated conversion of markdown languages. Once installed (alternatively, run pandoc via docker ), run the following command to convert TWiki to Markdown: $ pandoc -f twiki -t markdown_github <TWIKI FILE> > <MARKDOWN FILE> Where <TWIKI FILE> is the path to initial document in raw TWiki and <MARKDOWN FILE> is the path to the resulting document in GitHub Markdown. Note If you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section. Previewing your document(s) with Mkdocs \u00b6 Mkdocs has a development mode that can be used to preview documents as you work on them and is available via package manager or pip . Once installed , add your document(s) to the pages section of mkdocs.yml and launch the mkdocs server with the following command from the dir containing mkdocs.yml : $ PYTHONPATH = src/ mkdocs serve Access the server at http://127.0.0.1:8000 and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two. Completing the conversion \u00b6 Manual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents. Visit the style guide to ensure that the document meets all style guidelines. Archiving Documents \u00b6 If the document is slated for archival (check if it says \"yes\" in the \"archived\" column of the spreadsheet), just download the document to the archive folder of your local git repository: user@host $ cd technology/ user@host $ curl '<TWIKI URL>?raw=text' | iconv -f windows-1252 > archive/<TWIKI TITLE> Where <TWIKI URL> is the link to the TWiki document that you want to download and <TWIKI TITLE> is the name that will receive the archived file For example: user@host $ cd technology user@host $ curl 'https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text' | iconv -f windows-1252 > archive/SHA2Compliance After downloading the document, continue onto the next section to walk through pull request submission. Submitting the pull request \u00b6 Stage the archived raw TWiki (as well as the converted Markdown document(s) and mkdocs.yml if you are converting the document): user@host $ git add mkdocs.yml archive/<TWIKI ARCHIVE> <PATH TO CONVERTED DOC> Where <TWIKI ARCHIVE> is the name of the archived document and <PATH TO CONVERTED DOC> is the path to the .md file Commit and push your changes to your GitHub repo: user@host $ git commit -m \"<COMMIT MSG>\" user@host $ git push origin <BRANCH NAME> Change <COMMIT MSG> for a meaningful text that describes the conversion done and <BRANCH NAME> with the name chosen in the 3rd step of the Preparing the git repository section Open your browser and navigate to your GitHub fork Submit a pull request containing with the following body: <LINK TO TWIKI DOCUMENT> - [ ] Enter date into \"Migrated\" column of google sheet An example of <LINK TO TWIKI DOCUMENT> is: https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess If you are migrating a document, also add this task: - [ ] Add migration header to TWiki document If you are archiving a document, add this task: - [ ] Move TWiki document to the trash See an example pull request here . After the pull request \u00b6 After the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document: <div style=\"border: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600;\"> This document has been migrated to !GitHub (<LINK TO GITHUB DOCUMENT>). If you wish to see the old TWiki document, use the TWiki history below. Background: At the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below: * Release3: https://www.opensciencegrid.org/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]]) * !SoftwareTeam: https://www.opensciencegrid.org/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]]) </div> If you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document: - [X] Enter date into \"Migrated\" column of google sheet - [X] Add migration div to TWiki document Currently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL. This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub. This advice may change in the future as we gain experience with this transition. Reviewing pull requests \u00b6 To review pull requests, cd into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's cut-sw-release branch of the technology repository: # Add the requester 's repo as a remote if you haven' t already user@host $ git remote add blin https://www.github.com/brianhlin/technology.git user@host $ git fetch --all user@host $ git checkout blin/cut-sw-release Refresh your browser and navigate to the document in the request.","title":"Migrating Documents to Markdown"},{"location":"documentation/markdown-migration/#migrating-to-markdown","text":"As part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to Markdown . The following document outlines the conversion process and conventions.","title":"Migrating to Markdown"},{"location":"documentation/markdown-migration/#choosing-the-git-repository","text":"First you will need to choose which git repoository you will be working with: If you are converting a document from... Use this github repository... SoftwareTeam technology Release3 docs Once you've chosen the target repository for your document, move onto the next section and pick your conversion method.","title":"Choosing the git repository"},{"location":"documentation/markdown-migration/#automatic-twiki-conversion","text":"Note If you are only archiving the documents, skip to this section . Choose one of the following methods for converting TWiki documents: Using our own docker conversion image (recommended) Directly using pandoc and mkdocs on your own machine","title":"Automatic TWiki conversion"},{"location":"documentation/markdown-migration/#using-docker","text":"The twiki-converter docker image can be used to preview the document tree via a mkdocs development server, archive TWiki documents, and convert documents to Markdown via pandoc . The image is available on osghost , otherwise, it is availble on dockerhub . user@host $ docker pull opensciencegrid/docker-twiki-converter","title":"Using docker"},{"location":"documentation/markdown-migration/#requirements","text":"To perform a document migration using docker, you will need the following tools and accounts: Fork and clone the repository that you chose in the above section A host with a running docker service sudo or membership in the docker group If you cannot install the above tools locally, they are available on osghost . Speak with Brian L for access.","title":"Requirements"},{"location":"documentation/markdown-migration/#preparing-the-git-repository","text":"cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/technology.git Create a branch for the document you plan to convert: user@host $ git branch <BRANCH NAME> master Replace <BRANCH NAME> with a name of your choice Change to the branch you just created user@host $ git checkout <BRANCH NAME> Replace <BRANCH NAME> with the name you chose in the step above","title":"Preparing the git repository"},{"location":"documentation/markdown-migration/#previewing-the-document-tree","text":"When starting a twiki-converter docker container, it expects your local github repository to be mounted in /source so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image: Create a container from the image with the following command: user@host $ docker run -d -v <PATH TO LOCAL GITHUB REPO>:/source -p 8000 opensciencegrid/docker-twiki-converter Change <PATH TO LOCAL GITHUB REPO> for the directory where you have cloned the repo. The above command should return the container ID, which will be used in subsequent commands. Note If the docker container exits immediately, remove the -d option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode. To find the port that your development server is listening on, use the container ID (you should only need the first few chars of the ID) returned from the previous command: user@host $ docker port <CONTAINER ID> Change <CONTAINER ID> for the value returned by the execution of the previous command Access the development server in your browser via http://osghost.chtc.wisc.edu:<PORT> or localhost:<PORT> for containers run on osghost or locally, respectively. osghost has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the osghost port to a local port.","title":"Previewing the document tree"},{"location":"documentation/markdown-migration/#converting-documents","text":"The docker image contains a convenience script, convert-twiki for saving archives and converting them to Markdown. To run the script in a running container, run the following command: user@host $ docker exec <CONTAINER ID> convert-twiki <TWIKI URL> Where <CONTAINER ID> is the docker container ID and <TWIKI URL> is the link to the TWiki document that you want to convert, e.g. https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess . This will result in an archive of the twiki doc, archive/SoftwareDevelopmentProcess , in your local repo and a converted copy, SoftwareDevelopmentProcess.md , placed into the root of your local github repository. If the twiki url is for a specific revision of the document, a .rNN will be included in the output filenames. Warning If the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See Troubleshooting conversion for next steps. To see the converted document in your browser: Rename, move the converted document into a folder in docs/ . Document file names should be lowercase, - delimited, and descriptive but concise, e.g. markdown-migration.md or cutting-release.md It's not important to get the name/location correct on the first try as this can be discussed in the pull request sudo chown the archived and converted documents to be owned by you Add the document to the pages: section of mkdocs.yml in APA-style title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' Refresh the document tree in your browser Once you can view the converted document in your browser, move onto the next section","title":"Converting documents"},{"location":"documentation/markdown-migration/#troubleshooting-conversion","text":"Pandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion. Copy the archive of the document into the root of your git repository Kill the process in the docker container: user@host $ docker exec <CONTAINER ID> pkill -9 pandoc Where <CONTAINER ID> is the docker container ID Remove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy) Run pandoc manually: user@host $ docker exec <CONTAINER ID> pandoc -f twiki -t markdown_github <ARCHIVE COPY> > <MARKDOWN FILE> Where <CONTAINER ID> is the docker container ID, <ARCHIVE COPY> is the the file you copied in the first step and <MARKDOWN FILE> is the resulting .md file Repeat steps 2-4 until you've narrowed down the problematic section Manually convert the offending section","title":"Troubleshooting conversion"},{"location":"documentation/markdown-migration/#conversion-without-docker","text":"If you've already used the docker method , skip to the section about completing the conversion .","title":"Conversion without Docker"},{"location":"documentation/markdown-migration/#requirements_1","text":"This method requires the following: Fork and clone the repository that you chose in the above section pandoc (> 1.16) mkdocs MarkdownHighlight pygments","title":"Requirements"},{"location":"documentation/markdown-migration/#preparing-the-git-repository_1","text":"cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/technology.git Create a branch for the document you plan to convert: user@host $ git branch <BRANCH NAME> master Replace <BRANCH NAME> with a name of your choice 5. Change to the branch you just created user@host $ git checkout <BRANCH NAME> Replace <BRANCH NAME> with the name you chose in the step above","title":"Preparing the git repository"},{"location":"documentation/markdown-migration/#archiving-the-twiki-document","text":"Follow the instructions for archival then continue to the next section to convert the document with pandoc.","title":"Archiving the TWiki document"},{"location":"documentation/markdown-migration/#initial-conversion-with-pandoc","text":"Pandoc is a tool that's useful for automated conversion of markdown languages. Once installed (alternatively, run pandoc via docker ), run the following command to convert TWiki to Markdown: $ pandoc -f twiki -t markdown_github <TWIKI FILE> > <MARKDOWN FILE> Where <TWIKI FILE> is the path to initial document in raw TWiki and <MARKDOWN FILE> is the path to the resulting document in GitHub Markdown. Note If you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section.","title":"Initial conversion with Pandoc"},{"location":"documentation/markdown-migration/#previewing-your-documents-with-mkdocs","text":"Mkdocs has a development mode that can be used to preview documents as you work on them and is available via package manager or pip . Once installed , add your document(s) to the pages section of mkdocs.yml and launch the mkdocs server with the following command from the dir containing mkdocs.yml : $ PYTHONPATH = src/ mkdocs serve Access the server at http://127.0.0.1:8000 and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two.","title":"Previewing your document(s) with Mkdocs"},{"location":"documentation/markdown-migration/#completing-the-conversion","text":"Manual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents. Visit the style guide to ensure that the document meets all style guidelines.","title":"Completing the conversion"},{"location":"documentation/markdown-migration/#archiving-documents","text":"If the document is slated for archival (check if it says \"yes\" in the \"archived\" column of the spreadsheet), just download the document to the archive folder of your local git repository: user@host $ cd technology/ user@host $ curl '<TWIKI URL>?raw=text' | iconv -f windows-1252 > archive/<TWIKI TITLE> Where <TWIKI URL> is the link to the TWiki document that you want to download and <TWIKI TITLE> is the name that will receive the archived file For example: user@host $ cd technology user@host $ curl 'https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text' | iconv -f windows-1252 > archive/SHA2Compliance After downloading the document, continue onto the next section to walk through pull request submission.","title":"Archiving Documents"},{"location":"documentation/markdown-migration/#submitting-the-pull-request","text":"Stage the archived raw TWiki (as well as the converted Markdown document(s) and mkdocs.yml if you are converting the document): user@host $ git add mkdocs.yml archive/<TWIKI ARCHIVE> <PATH TO CONVERTED DOC> Where <TWIKI ARCHIVE> is the name of the archived document and <PATH TO CONVERTED DOC> is the path to the .md file Commit and push your changes to your GitHub repo: user@host $ git commit -m \"<COMMIT MSG>\" user@host $ git push origin <BRANCH NAME> Change <COMMIT MSG> for a meaningful text that describes the conversion done and <BRANCH NAME> with the name chosen in the 3rd step of the Preparing the git repository section Open your browser and navigate to your GitHub fork Submit a pull request containing with the following body: <LINK TO TWIKI DOCUMENT> - [ ] Enter date into \"Migrated\" column of google sheet An example of <LINK TO TWIKI DOCUMENT> is: https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess If you are migrating a document, also add this task: - [ ] Add migration header to TWiki document If you are archiving a document, add this task: - [ ] Move TWiki document to the trash See an example pull request here .","title":"Submitting the pull request"},{"location":"documentation/markdown-migration/#after-the-pull-request","text":"After the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document: <div style=\"border: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600;\"> This document has been migrated to !GitHub (<LINK TO GITHUB DOCUMENT>). If you wish to see the old TWiki document, use the TWiki history below. Background: At the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below: * Release3: https://www.opensciencegrid.org/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]]) * !SoftwareTeam: https://www.opensciencegrid.org/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]]) </div> If you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document: - [X] Enter date into \"Migrated\" column of google sheet - [X] Add migration div to TWiki document Currently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL. This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub. This advice may change in the future as we gain experience with this transition.","title":"After the pull request"},{"location":"documentation/markdown-migration/#reviewing-pull-requests","text":"To review pull requests, cd into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's cut-sw-release branch of the technology repository: # Add the requester 's repo as a remote if you haven' t already user@host $ git remote add blin https://www.github.com/brianhlin/technology.git user@host $ git fetch --all user@host $ git checkout blin/cut-sw-release Refresh your browser and navigate to the document in the request.","title":"Reviewing pull requests"},{"location":"documentation/publish-osg-pages/","text":"Publishing OSG Pages with MkDocs \u00b6 The OSG uses MkDocs for site documentation and team-specific web pages (e.g. https://osg-htc.org/technology/ ). This document contains instructions for creating a new OSG area through GitHub and transitioning an existing MkDocs GitHub repository from Travis CI to GitHub Actions . Creating New Pages \u00b6 Need assistance? If you need any assistance with setting up your GitHub repository, please reach out to help@osg-htc.org . This document assumes that you are an administrator of the opensciencegrid GitHub organization. Before starting, make sure that you have the git and gem tools installed. Create a new public repository in the opensciencegrid organization (referred to as <REPO NAME> in the rest of this document) Check the box marked Initialize this repository with a README Identify the repository as using mkdocs: On the repository home page (i.e., https://github.com/opensciencegrid/<REPO NAME> ), click the gear button in the top right (next to \"About\") In the topics field, add mkdocs Click the \u201cSave Changes\u201d button Clone the repository and cd into the directory: git clone https://github.com/opensciencegrid/<REPO NAME>.git cd <REPO NAME> Create a gh-pages branch in the GitHub repository: git push origin main:gh-pages Update the contents of README.md and populate the LICENSE file with a Creative Commons Attribution 4.0 license : wget https://creativecommons.org/licenses/by/4.0/legalcode.txt > LICENSE Create and encrypt the repository deploy key Generate the repository deploy key: ssh-keygen -t rsa -b 4096 -C \"help@osg-htc.org\" -f deploy-key -N '' Install the travis gem: gem install travis Login using your GitHub credentials: travis login --com Enable the repository in Travis: travis enable --com -r opensciencegrid/<REPO NAME> Encrypt the deploy key and temporarily save the output (you will need the hashes later for .travis.env ): travis encrypt-file --com deploy-key Stage and commit your files: git add LICENSE README.md deploy-key.enc git commit -m \"Prepare the repository for Travis-CI deployment\" Danger Do NOT commit the unencrypted deploy-key ! Add the contents of deploy-key.pub to your repository's list of deploy keys . Make sure to check Allow write access . Follow these instructions to add the doc-ci-scripts sub-module Use the mkdocs-v1 branch of the doc-ci-scripts submodule. Create mkdocs.yml containing the following: site_name: <TITLE OF YOUR SITE> site_url: https://osg-htc.org/<REPO NAME> repo_name: https://github.com/osg-htc/<REPO NAME> theme: name: material nav: - Home: 'index.md' markdown_extensions: - admonition - codehilite: guess_lang: False - meta - toc: permalink: True Create a docs directory containing an index.md that will be your home page. Stage and commit these changes: git add mkdocs.yml docs/index.md git commit -m \"Staging initial web page contents\" Push local changes to the GitHub repository: git push origin main Your documents should be shortly available at https://www.opensciencegrid.org/<REPO NAME> Contact help@osg-htc.org to request repository backups to UW-Madison. Creating an ITB Area \u00b6 This section describes creating an ITB repository for a documentation area created in the previous section Create a new repository in the opensciencegrid organization and name it <REPO NAME>-itb . For example, an ITB area for the docs repository has a repository name of docs-itb . The ITB repository will be referred to as <ITB REPO NAME> in the rest of this document. Check the box marked Initialize this repository with a README Once created, add the mkdocs topic by clicking on the \"Add topics\" button Clone the repository and cd into the directory: git clone git@github.com:opensciencegrid/<ITB REPO NAME> cd <ITB REPO NAME> Create a gh-pages branch in the GitHub repository: git push origin main:gh-pages Update the contents of README.md In the non-ITB repository, create and encrypt the ITB repository deploy key cd into the non-ITB repository and generate the ITB deploy key cd <REPO NAME> ssh-keygen -t rsa -b 4096 -C \"help@osg-htc.org\" -f deploy-itb Install the travis gem: gem install travis Encrypt the deploy key: travis encrypt-file deploy-itb Update .travis.env with the appropriate ITB values Add and commit your files: git add .travis.env deploy-itb.enc git commit -m \"Add ITB deployment\" Danger Do NOT commit the unencrypted deploy-itb ! Add deploy-itb.pub to the ITB repository's list of deploy keys . Make sure to check Allow write access . Still in the non-ITB repository, push your local changes to the GitHub repository git push origin main Your documents should be shortly available at https://www.opensciencegrid.org/<REPO NAME> Transitioning to GitHub Actions \u00b6 Need assistance? If you need any assistance with transitioning your repository to GitHub actions, please reach out to help@osg-htc.org . When originally developed, OSG MkDocs repositories were set up to automatically publish web page changes through Travis CI . But in November 2020, Travis CI changed their pricing model so we are moving the automatic publishing infrastructure to GitHub Actions and using this opportunity to also upgrade the version of MkDocs. To ensure that your pages continue to be autmoatically published you will need to prepare your repository for the new version of MkDocs, disable Travis CI, and enable GitHub Actions. Preparing the repository \u00b6 Before upgrading, you must fix the following incompatibilities: Rename the pages: section of mkdocs.yml to nav: . The section contents are identical; only the name is changing. Update all of the links in the documents as follows: Ensure links end in .md Ensure links are document-relative, not site-relative For example, links should be of the form ../software/development-process.md instead of /software/development-process . Push your changes to your fork and create a pull request Previewing your pages with Docker If you would like to ensure the correctness of your fixes, run the following command: docker run -it -v ${PWD}/docs -p 8000:8000 squidfunk/mkdocs-material:6.1.4 After running this command, enter localhost:8000 in your browser to preview your pages. Saved changes made to .md files are automatically updated in your browser! Disabling Travis CI \u00b6 After you've prepared your repository for the transition, disable Travis CI by removing related files from it. Perform the following actions from the command-line: Create a fresh clone of the repository that is still using Travis CI: git clone https://github.com/opensciencegrid/<GIT REPOSITORY> Important For the rest of the steps in this documentation to work, you should create a new clone of the repository using the above command, even if you already have an existing copy! cd into the directory containing the repository (should be the same as <GIT REPOSITORY> by default) Remove all of the Travs CI related files: git rm ci git rm .travis.env deploy-key.enc .travis.yml Commit your changes: git commit -am \"Disable Travis CI\" Push your changes (you will be prompted for your GitHub credentials): git push origin master Enabling GitHub Actions \u00b6 The new method for validating and publishing OSG pages for a MkDocs repository can be enabled entirely through the GitHub web interface : Navigate to the opensciencegrid fork of the GitHub repository in your web browser, e.g. https://github.com/opensciencegrid/docs/ Click on the Actions tab: Find the Publish MkDocs static HTML workflow by the Open Science Grid and click the Set up this workflow button: Click on the Start commit drop-down button then click Commit new file : Navigate to the Actions tab to verify your transition. If you see green check marks, your pages are now being published with GitHub Actions! (Optional) Enable the Validate MkDocs Static HTML workflow to check for broken links and markdown correctness of pull requests to the repository. Navigate to the Actions tab as before Click on the New Workflow button: Find the Validate MkDocs static HTML workflow by the Open Science Grid and click Set up this workflow As before, click on the Start commit drop-down button then click Commit new file","title":"Publishing OSG pages"},{"location":"documentation/publish-osg-pages/#publishing-osg-pages-with-mkdocs","text":"The OSG uses MkDocs for site documentation and team-specific web pages (e.g. https://osg-htc.org/technology/ ). This document contains instructions for creating a new OSG area through GitHub and transitioning an existing MkDocs GitHub repository from Travis CI to GitHub Actions .","title":"Publishing OSG Pages with MkDocs"},{"location":"documentation/publish-osg-pages/#creating-new-pages","text":"Need assistance? If you need any assistance with setting up your GitHub repository, please reach out to help@osg-htc.org . This document assumes that you are an administrator of the opensciencegrid GitHub organization. Before starting, make sure that you have the git and gem tools installed. Create a new public repository in the opensciencegrid organization (referred to as <REPO NAME> in the rest of this document) Check the box marked Initialize this repository with a README Identify the repository as using mkdocs: On the repository home page (i.e., https://github.com/opensciencegrid/<REPO NAME> ), click the gear button in the top right (next to \"About\") In the topics field, add mkdocs Click the \u201cSave Changes\u201d button Clone the repository and cd into the directory: git clone https://github.com/opensciencegrid/<REPO NAME>.git cd <REPO NAME> Create a gh-pages branch in the GitHub repository: git push origin main:gh-pages Update the contents of README.md and populate the LICENSE file with a Creative Commons Attribution 4.0 license : wget https://creativecommons.org/licenses/by/4.0/legalcode.txt > LICENSE Create and encrypt the repository deploy key Generate the repository deploy key: ssh-keygen -t rsa -b 4096 -C \"help@osg-htc.org\" -f deploy-key -N '' Install the travis gem: gem install travis Login using your GitHub credentials: travis login --com Enable the repository in Travis: travis enable --com -r opensciencegrid/<REPO NAME> Encrypt the deploy key and temporarily save the output (you will need the hashes later for .travis.env ): travis encrypt-file --com deploy-key Stage and commit your files: git add LICENSE README.md deploy-key.enc git commit -m \"Prepare the repository for Travis-CI deployment\" Danger Do NOT commit the unencrypted deploy-key ! Add the contents of deploy-key.pub to your repository's list of deploy keys . Make sure to check Allow write access . Follow these instructions to add the doc-ci-scripts sub-module Use the mkdocs-v1 branch of the doc-ci-scripts submodule. Create mkdocs.yml containing the following: site_name: <TITLE OF YOUR SITE> site_url: https://osg-htc.org/<REPO NAME> repo_name: https://github.com/osg-htc/<REPO NAME> theme: name: material nav: - Home: 'index.md' markdown_extensions: - admonition - codehilite: guess_lang: False - meta - toc: permalink: True Create a docs directory containing an index.md that will be your home page. Stage and commit these changes: git add mkdocs.yml docs/index.md git commit -m \"Staging initial web page contents\" Push local changes to the GitHub repository: git push origin main Your documents should be shortly available at https://www.opensciencegrid.org/<REPO NAME> Contact help@osg-htc.org to request repository backups to UW-Madison.","title":"Creating New Pages"},{"location":"documentation/publish-osg-pages/#creating-an-itb-area","text":"This section describes creating an ITB repository for a documentation area created in the previous section Create a new repository in the opensciencegrid organization and name it <REPO NAME>-itb . For example, an ITB area for the docs repository has a repository name of docs-itb . The ITB repository will be referred to as <ITB REPO NAME> in the rest of this document. Check the box marked Initialize this repository with a README Once created, add the mkdocs topic by clicking on the \"Add topics\" button Clone the repository and cd into the directory: git clone git@github.com:opensciencegrid/<ITB REPO NAME> cd <ITB REPO NAME> Create a gh-pages branch in the GitHub repository: git push origin main:gh-pages Update the contents of README.md In the non-ITB repository, create and encrypt the ITB repository deploy key cd into the non-ITB repository and generate the ITB deploy key cd <REPO NAME> ssh-keygen -t rsa -b 4096 -C \"help@osg-htc.org\" -f deploy-itb Install the travis gem: gem install travis Encrypt the deploy key: travis encrypt-file deploy-itb Update .travis.env with the appropriate ITB values Add and commit your files: git add .travis.env deploy-itb.enc git commit -m \"Add ITB deployment\" Danger Do NOT commit the unencrypted deploy-itb ! Add deploy-itb.pub to the ITB repository's list of deploy keys . Make sure to check Allow write access . Still in the non-ITB repository, push your local changes to the GitHub repository git push origin main Your documents should be shortly available at https://www.opensciencegrid.org/<REPO NAME>","title":"Creating an ITB Area"},{"location":"documentation/publish-osg-pages/#transitioning-to-github-actions","text":"Need assistance? If you need any assistance with transitioning your repository to GitHub actions, please reach out to help@osg-htc.org . When originally developed, OSG MkDocs repositories were set up to automatically publish web page changes through Travis CI . But in November 2020, Travis CI changed their pricing model so we are moving the automatic publishing infrastructure to GitHub Actions and using this opportunity to also upgrade the version of MkDocs. To ensure that your pages continue to be autmoatically published you will need to prepare your repository for the new version of MkDocs, disable Travis CI, and enable GitHub Actions.","title":"Transitioning to GitHub Actions"},{"location":"documentation/publish-osg-pages/#preparing-the-repository","text":"Before upgrading, you must fix the following incompatibilities: Rename the pages: section of mkdocs.yml to nav: . The section contents are identical; only the name is changing. Update all of the links in the documents as follows: Ensure links end in .md Ensure links are document-relative, not site-relative For example, links should be of the form ../software/development-process.md instead of /software/development-process . Push your changes to your fork and create a pull request Previewing your pages with Docker If you would like to ensure the correctness of your fixes, run the following command: docker run -it -v ${PWD}/docs -p 8000:8000 squidfunk/mkdocs-material:6.1.4 After running this command, enter localhost:8000 in your browser to preview your pages. Saved changes made to .md files are automatically updated in your browser!","title":"Preparing the repository"},{"location":"documentation/publish-osg-pages/#disabling-travis-ci","text":"After you've prepared your repository for the transition, disable Travis CI by removing related files from it. Perform the following actions from the command-line: Create a fresh clone of the repository that is still using Travis CI: git clone https://github.com/opensciencegrid/<GIT REPOSITORY> Important For the rest of the steps in this documentation to work, you should create a new clone of the repository using the above command, even if you already have an existing copy! cd into the directory containing the repository (should be the same as <GIT REPOSITORY> by default) Remove all of the Travs CI related files: git rm ci git rm .travis.env deploy-key.enc .travis.yml Commit your changes: git commit -am \"Disable Travis CI\" Push your changes (you will be prompted for your GitHub credentials): git push origin master","title":"Disabling Travis CI"},{"location":"documentation/publish-osg-pages/#enabling-github-actions","text":"The new method for validating and publishing OSG pages for a MkDocs repository can be enabled entirely through the GitHub web interface : Navigate to the opensciencegrid fork of the GitHub repository in your web browser, e.g. https://github.com/opensciencegrid/docs/ Click on the Actions tab: Find the Publish MkDocs static HTML workflow by the Open Science Grid and click the Set up this workflow button: Click on the Start commit drop-down button then click Commit new file : Navigate to the Actions tab to verify your transition. If you see green check marks, your pages are now being published with GitHub Actions! (Optional) Enable the Validate MkDocs Static HTML workflow to check for broken links and markdown correctness of pull requests to the repository. Navigate to the Actions tab as before Click on the New Workflow button: Find the Validate MkDocs static HTML workflow by the Open Science Grid and click Set up this workflow As before, click on the Start commit drop-down button then click Commit new file","title":"Enabling GitHub Actions"},{"location":"documentation/reviewing-documentation/","text":"Reviewing Software Documentation \u00b6 To maintain quality documentation, we regularly review our documentation for clarity and correctness. Performing a Review \u00b6 When reviewing a document, follow the instructions of the document to a tee as if you were completely new to the OSG. Some common things to note and/or fix: After completing the instructions in the document: Does the document inform you how to use the product? Does the document tell you how to verify that the product is functioning? Does the product work? Incorrect or out of date instructions Steps that may be particularly conducive to software or default configuration as a solution Lack of clarity or any other confusion you may have Additionally, ensure that the document meets our style and layout guidelines, as well as correct spelling and grammar. Completing a Review \u00b6 Upon completion of the review: Update (or add) the review date in the metadata at the top of the document above the h1 : DateReviewed: YYYY-MM-DD Submit a pull request with the above change and any other improvements from the review","title":"Reviewing Documentation"},{"location":"documentation/reviewing-documentation/#reviewing-software-documentation","text":"To maintain quality documentation, we regularly review our documentation for clarity and correctness.","title":"Reviewing Software Documentation"},{"location":"documentation/reviewing-documentation/#performing-a-review","text":"When reviewing a document, follow the instructions of the document to a tee as if you were completely new to the OSG. Some common things to note and/or fix: After completing the instructions in the document: Does the document inform you how to use the product? Does the document tell you how to verify that the product is functioning? Does the product work? Incorrect or out of date instructions Steps that may be particularly conducive to software or default configuration as a solution Lack of clarity or any other confusion you may have Additionally, ensure that the document meets our style and layout guidelines, as well as correct spelling and grammar.","title":"Performing a Review"},{"location":"documentation/reviewing-documentation/#completing-a-review","text":"Upon completion of the review: Update (or add) the review date in the metadata at the top of the document above the h1 : DateReviewed: YYYY-MM-DD Submit a pull request with the above change and any other improvements from the review","title":"Completing a Review"},{"location":"documentation/style-guide/","text":"Markdown Style Guide \u00b6 This document contains markdown conventions that are used in OSG Software documentation. Meta \u00b6 Run a spellchecker to catch any obvious spelling mistakes. Use official capitalizations when referring to titles (i.e., HTTPS, HTCondor) Start each new sentence on a new line. Lines should not exceed 120 characters, except in the case of link text , but may be split at earlier points (e.g. punctuation). Headings \u00b6 Use the following conventions for headings: The title should be the only level 1 heading Level 1 headings should use the ==== format Level 2 headings should use the ---- format Use APA-style title case for level 1 and level 2 headings. Only capitalize the first word for all other headings. Other heading levels should use the appropriate number of # Go no deeper than of level 5 headings Spin-off a new document or re-organize the existing document if you find that you regularly need level 5 headings. Links \u00b6 Only use document relative links in MkDocs 1.0.0 and newer MkDocs 1.0.0 does not support site-relative links (e.g. /software/development-process.md ). You must use document-relative links (e.g. ../software/development-process.md ) instead. Earlier versions of this guide recommended site-relative links; these only worked in earlier versions of MkDocs. Document-relative links work in all versions of MkDocs. Please convert any site-relative links to document-relative links before updating the doc-ci-scripts submodule to use MkDocs 1.0.0+. Links to internal pages must have the .md extension. (Earlier versions of this guide said links should not have the .md extension, but adding the .md extension is required for MkDocs 1.0.0+. Links with the .md extension work in all versions of MkDocs.) Links to the area's homepage (e.g. https://osg-htc.org/technology/) need to be of the form [link text](/) Links to other areas (like from https://osg-htc.org/technology/ to https://osg-htc.org/operations/) need to be absolute links (i.e. include the domain name) Section links \u00b6 To link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple sections with the same name you can link the subsequent sections by appending _N where N is the section's ordinal number minus one, e.g. append _1 for the second section. For example, if you have three sections named \"Optional Configuration\", link them like so: [1st section](#optional-configuration) [2nd section](#optional-configuration_1) [3rd section](#optional-configuration_2) Command blocks and file snippets \u00b6 Command blocks and file snippets outside of lists should be wrapped in three back-ticks (```) followed by an optional code highlighting format: ```console # stuff ``` Command blocks and file snippets inside of a list should use the appropriate number of spaces before three colons followed by an optional code highlighting format: # stuff See the lists section for details on properly formatting command blocks within a list. We use the Pygments highlighting library for syntax; it knows about 100 different languages. The Pygments website contains a live renderer if you want to see how your text will come out. Please use the console language for shell sessions. Root and user prompts \u00b6 When specifying instructions for the command-line, indicate to users whether the commands can be run as root ( root@host # ) or as an unprivileged user ( user@host $ ). For example: root@host # useradd -m osguser root@host # su - osguser user@host $ whoami osguser It can provide helpful context to use a more specific hostname in the prompt than host . For example, if you're writing a doc for setting up a Storage Element and a command is run as root on the SE, use root@se # . Or if you're testing the SE from the client side and the command is run as a normal user on a client, use user@client $ . Highlighting user input \u00b6 Use descriptive, all-caps text wrapped in angle brackets to to highlight areas that users would have to insert text specific to their site, e.g. <REMOTE SSH HOSTNAME> . The same text should be cited verbatim in surrounding prose with further explanation with examples of appropriate values. For additional visual highlighting, use hl_lines=\"N\" , where N can indicate multiple line numbers: ```console hl_lines=\"1 3\" root@condor-ce # yum install htcondor-ce # this is a comment root@condor-ce # condor_ce_trace -d <CE HOSTNAME> ```` Similarly, you may also specify :::console hl_lines=\"N\" for indented command blocks, replacing console with any language supported by Pygments . The above block is rendered below: root@condor-ce # yum install htcondor-ce # this is a comment root@condor-ce # condor_ce_trace -d <CE HOSTNAME> Lists \u00b6 When constructing lists, use the following guidelines: Use 1. for each item in a numbered list To make sure the contents of code blocks, file snippets, and subsequent paragraphs are indented properly, use the following formatting: For code blocks or file snippets, add an empty line after any regular text, then insert (N+1)*4 spaces at the beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code block with :::<FORMAT> ; see this page for details, including possible highlighting formats. For an example of formatting a code section inside a list, see the release series document . For additional text (i.e. after a code block), insert N*4 spaces at the beginning of each line, where N is the level of the item in the list. For example: 1. Foo - Bar :::console COMMAND BLOCK text associated with Bar text associated with Foo 1. Baz FILE SNIPPET There are 12 spaces and 8 spaces in front of the command block and text associated with Bar , respectively; 4 spaces in front of the text associated with Foo ; and 8 spaces in front of the file snippet associated with Baz . The above block is rendered below: Foo Bar COMMAND BLOCK text associated with Bar text associated with Foo Baz FILE SNIPPET Notes \u00b6 To catch the user's attention for important items or pitfalls, we used %NOTE% TWiki macros, these can be replaced with admonition-style notes and warnings: !!! note things to note or !!! warning if a user doesn't do this thing, bad stuff will happen The above blocks are rendered below as an example. Note things to note and Warning if a user doesn't do this thing, bad stuff will happen For a full list of admonition styles, see the documentation here .","title":"Markdown Style Guide"},{"location":"documentation/style-guide/#markdown-style-guide","text":"This document contains markdown conventions that are used in OSG Software documentation.","title":"Markdown Style Guide"},{"location":"documentation/style-guide/#meta","text":"Run a spellchecker to catch any obvious spelling mistakes. Use official capitalizations when referring to titles (i.e., HTTPS, HTCondor) Start each new sentence on a new line. Lines should not exceed 120 characters, except in the case of link text , but may be split at earlier points (e.g. punctuation).","title":"Meta"},{"location":"documentation/style-guide/#headings","text":"Use the following conventions for headings: The title should be the only level 1 heading Level 1 headings should use the ==== format Level 2 headings should use the ---- format Use APA-style title case for level 1 and level 2 headings. Only capitalize the first word for all other headings. Other heading levels should use the appropriate number of # Go no deeper than of level 5 headings Spin-off a new document or re-organize the existing document if you find that you regularly need level 5 headings.","title":"Headings"},{"location":"documentation/style-guide/#links","text":"Only use document relative links in MkDocs 1.0.0 and newer MkDocs 1.0.0 does not support site-relative links (e.g. /software/development-process.md ). You must use document-relative links (e.g. ../software/development-process.md ) instead. Earlier versions of this guide recommended site-relative links; these only worked in earlier versions of MkDocs. Document-relative links work in all versions of MkDocs. Please convert any site-relative links to document-relative links before updating the doc-ci-scripts submodule to use MkDocs 1.0.0+. Links to internal pages must have the .md extension. (Earlier versions of this guide said links should not have the .md extension, but adding the .md extension is required for MkDocs 1.0.0+. Links with the .md extension work in all versions of MkDocs.) Links to the area's homepage (e.g. https://osg-htc.org/technology/) need to be of the form [link text](/) Links to other areas (like from https://osg-htc.org/technology/ to https://osg-htc.org/operations/) need to be absolute links (i.e. include the domain name)","title":"Links"},{"location":"documentation/style-guide/#section-links","text":"To link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple sections with the same name you can link the subsequent sections by appending _N where N is the section's ordinal number minus one, e.g. append _1 for the second section. For example, if you have three sections named \"Optional Configuration\", link them like so: [1st section](#optional-configuration) [2nd section](#optional-configuration_1) [3rd section](#optional-configuration_2)","title":"Section links"},{"location":"documentation/style-guide/#command-blocks-and-file-snippets","text":"Command blocks and file snippets outside of lists should be wrapped in three back-ticks (```) followed by an optional code highlighting format: ```console # stuff ``` Command blocks and file snippets inside of a list should use the appropriate number of spaces before three colons followed by an optional code highlighting format: # stuff See the lists section for details on properly formatting command blocks within a list. We use the Pygments highlighting library for syntax; it knows about 100 different languages. The Pygments website contains a live renderer if you want to see how your text will come out. Please use the console language for shell sessions.","title":"Command blocks and file snippets"},{"location":"documentation/style-guide/#root-and-user-prompts","text":"When specifying instructions for the command-line, indicate to users whether the commands can be run as root ( root@host # ) or as an unprivileged user ( user@host $ ). For example: root@host # useradd -m osguser root@host # su - osguser user@host $ whoami osguser It can provide helpful context to use a more specific hostname in the prompt than host . For example, if you're writing a doc for setting up a Storage Element and a command is run as root on the SE, use root@se # . Or if you're testing the SE from the client side and the command is run as a normal user on a client, use user@client $ .","title":"Root and user prompts"},{"location":"documentation/style-guide/#highlighting-user-input","text":"Use descriptive, all-caps text wrapped in angle brackets to to highlight areas that users would have to insert text specific to their site, e.g. <REMOTE SSH HOSTNAME> . The same text should be cited verbatim in surrounding prose with further explanation with examples of appropriate values. For additional visual highlighting, use hl_lines=\"N\" , where N can indicate multiple line numbers: ```console hl_lines=\"1 3\" root@condor-ce # yum install htcondor-ce # this is a comment root@condor-ce # condor_ce_trace -d <CE HOSTNAME> ```` Similarly, you may also specify :::console hl_lines=\"N\" for indented command blocks, replacing console with any language supported by Pygments . The above block is rendered below: root@condor-ce # yum install htcondor-ce # this is a comment root@condor-ce # condor_ce_trace -d <CE HOSTNAME>","title":"Highlighting user input"},{"location":"documentation/style-guide/#lists","text":"When constructing lists, use the following guidelines: Use 1. for each item in a numbered list To make sure the contents of code blocks, file snippets, and subsequent paragraphs are indented properly, use the following formatting: For code blocks or file snippets, add an empty line after any regular text, then insert (N+1)*4 spaces at the beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code block with :::<FORMAT> ; see this page for details, including possible highlighting formats. For an example of formatting a code section inside a list, see the release series document . For additional text (i.e. after a code block), insert N*4 spaces at the beginning of each line, where N is the level of the item in the list. For example: 1. Foo - Bar :::console COMMAND BLOCK text associated with Bar text associated with Foo 1. Baz FILE SNIPPET There are 12 spaces and 8 spaces in front of the command block and text associated with Bar , respectively; 4 spaces in front of the text associated with Foo ; and 8 spaces in front of the file snippet associated with Baz . The above block is rendered below: Foo Bar COMMAND BLOCK text associated with Bar text associated with Foo Baz FILE SNIPPET","title":"Lists"},{"location":"documentation/style-guide/#notes","text":"To catch the user's attention for important items or pitfalls, we used %NOTE% TWiki macros, these can be replaced with admonition-style notes and warnings: !!! note things to note or !!! warning if a user doesn't do this thing, bad stuff will happen The above blocks are rendered below as an example. Note things to note and Warning if a user doesn't do this thing, bad stuff will happen For a full list of admonition styles, see the documentation here .","title":"Notes"},{"location":"documentation/writing-documentation/","text":"Writing OSG Documentation \u00b6 Many OSG pages are written in markdown , built using MkDocs , and served via GitHub Pages . To contribute content , submit a pull request to the relevant GitHub repository, which are tagged with \"mkdocs\". List of documentation repos This document contains instructions, recommendations, and guidelines for writing OSG content. Contributing Content \u00b6 To contribute minor content changes (e.g., fixing typos, changing a couple of sentences), we recommend using the GitHub web interface to submit a pull request. To contribute major content changes to one of the above OSG areas, make sure you and the machine you'll be working on meet the following requirements: Have a Github account Installations of the following tools: Docker git Preparing the git repository \u00b6 Before making any content changes, you will need to prepare a local git clone: Fork and clone the GitHub repository that you'd like to contribute to Add the upstream Github repository as a remote . For example, if you are working on the User School 2018 pages: $ git remote add upstream https://github.com/opensciencegrid/user-school-2018 Previewing the pages \u00b6 To preview the pages, start a MkDocs development server. The development server will automatically detect any content changes and make them viewable in your browser. cd into the directory containing the local clone of your GitHub fork Start a MkDocs development server to preview your changes: $ docker run --rm -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material:7.1.0 To preview your changes visit localhost:8000 in the browser of your choice. The server can be stopped with Ctrl-C . Making content changes \u00b6 To contribute content to the OSG, follow these steps to submit a pull request with your desired changes: cd into the directory containing the local clone of your Github fork Create a branch based on a branch from the upstream repository: $ git fetch --all $ git checkout -b <BRANCH NAME> upstream/<UPSTREAM BRANCH NAME> Replace <BRANCH NAME> with a name of your choice and <UPSTREAM BRANCH NAME> with a branch name from the upstream repository. For example, instructors for the 2018 User School should use the materials branch: $ git checkout -b example_branch_name upstream/materials If you do not know which upstream branch to use, pick master . Make your changes in the docs/ directory of your local clone, following the style guide : If you are making changes to an existing page: Open mkdocs.yml and find the location of the file relative to the docs/ directory Make your changes to that file and move onto the next step If you are contributing a new page: Name the page. Page file names should be lowercase, - delimited, and concise but descriptive, e.g. markdown-migration.md or cutting-release.md Place the page in the relevant sub-folder of the docs/ directory. If you are unsure of the appropriate location, note that in the description of the pull request. Add the document to the nav: section of mkdocs.yml in APA-style title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' Note If mkdocs.yml contains does not contain a nav: section, add the above to the pages: section instead. This means that the repository is using an older version of MkDocs and will need to be transitioned to GitHub Actions . If you are writing site administrator documentation, following the suggested document layout If you haven't already, start a Mkdocs development server to preview your changes . Continue making changes until you are satisfied with the preview, then stage your changes in git: $ git add <YOUR FILE> <YOUR 2nd FILE>...<YOUR Nth FILE> Where <YOUR * FILE> is any file that contains changes that you'd wish to make. If you are adding a new page, one of the files should be mkdocs.yml . Commit your changes and push them to your Github fork: $ git commit -m \"<DESCRIPTIVE COMMIT MESSAGE>\" $ git push origin Where <DESCRIPTIVE COMMIT MESSAGE> is a meaningful short text that identifies the changes applied, it is a good practice, to concatenate the ticket number associated e.g. \"Removing color macros (SOFTWARE-3739)\" From your Github fork, submit a pull request Document Layout \u00b6 This section contains suggested layouts of externally-facing, site administrator documentation . The introduction is the only layout requirement for documents except for installation guides. Introductions \u00b6 All documents should start with an introduction that explains what the document contains, what the product does, and why someone may want to use it. In the past, document introductions were included in About this... sections due to the layout of the table of contents. Since the table of contents is included in the sidebar this is unnecessary and introduction content should go directly below the title heading without any second-level headings. The HTCondor-CE installation guide is an example that meet all of the above criteria. Installation guides \u00b6 In addition to the introduction above, installation documents should have the following sections: Before Starting: This section should contain information for any prepatory work that the site administrator should do or consider before proceeding with the installation ( example ). Installation: Procedural instructions that tell the user how to install the software ( example ) Validation: How does the user make sure their installation is functional? Optionally, the following sections should be included as necessary. Overview: if the introduction becomes large and unwieldy, extract the details of what the product does into an overview section Configuration: required configuration steps ( example ) as well as a sub-section for optional configurations. For long optional configuration sections, consider creating alist of contents at the top of the sub-section ( example ). Troubleshooting: common issues that users encounter and their fixes Reference: Details about configuration and log files, unix users, certificates, networking, links to relevant upstream documentation, etc. ( example ) If any of the sections become too large, consider separating them out and linking to the new documents ( example ). Tips for Writing Procedural Instructions \u00b6 Title the procedure with the user goal, usually starting with a gerund; e.g.: Installing the Frobnosticator Number all steps (as opposed to using bullets) List steps in order in which they are performed Each step should begin with a single-line instruction in plain English, in command form; e.g.: Make sure that the Frobnosticator configuration file is world-writable If the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.: chmod a+x /usr/share/frobnosticator/frob.conf Put clarifying information in separate paragraphs within the step Put critical information about the whole procedure in one or more paragraphs before the numbered steps Put supplemental information about the whole procedure in one or more paragraphs after the numbered steps Avoid pronouns when writing technical articles or documentation e.g., install foo rather than install it . Avoid superfluous statements like you will want , you want , you should e.g., install foo rather than you will want to install foo . Use the imperative form in step-by-step instructions, e.g. install package foo rather than the package foo should be installed","title":"Writing Documentation"},{"location":"documentation/writing-documentation/#writing-osg-documentation","text":"Many OSG pages are written in markdown , built using MkDocs , and served via GitHub Pages . To contribute content , submit a pull request to the relevant GitHub repository, which are tagged with \"mkdocs\". List of documentation repos This document contains instructions, recommendations, and guidelines for writing OSG content.","title":"Writing OSG Documentation"},{"location":"documentation/writing-documentation/#contributing-content","text":"To contribute minor content changes (e.g., fixing typos, changing a couple of sentences), we recommend using the GitHub web interface to submit a pull request. To contribute major content changes to one of the above OSG areas, make sure you and the machine you'll be working on meet the following requirements: Have a Github account Installations of the following tools: Docker git","title":"Contributing Content"},{"location":"documentation/writing-documentation/#preparing-the-git-repository","text":"Before making any content changes, you will need to prepare a local git clone: Fork and clone the GitHub repository that you'd like to contribute to Add the upstream Github repository as a remote . For example, if you are working on the User School 2018 pages: $ git remote add upstream https://github.com/opensciencegrid/user-school-2018","title":"Preparing the git repository"},{"location":"documentation/writing-documentation/#previewing-the-pages","text":"To preview the pages, start a MkDocs development server. The development server will automatically detect any content changes and make them viewable in your browser. cd into the directory containing the local clone of your GitHub fork Start a MkDocs development server to preview your changes: $ docker run --rm -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material:7.1.0 To preview your changes visit localhost:8000 in the browser of your choice. The server can be stopped with Ctrl-C .","title":"Previewing the pages"},{"location":"documentation/writing-documentation/#making-content-changes","text":"To contribute content to the OSG, follow these steps to submit a pull request with your desired changes: cd into the directory containing the local clone of your Github fork Create a branch based on a branch from the upstream repository: $ git fetch --all $ git checkout -b <BRANCH NAME> upstream/<UPSTREAM BRANCH NAME> Replace <BRANCH NAME> with a name of your choice and <UPSTREAM BRANCH NAME> with a branch name from the upstream repository. For example, instructors for the 2018 User School should use the materials branch: $ git checkout -b example_branch_name upstream/materials If you do not know which upstream branch to use, pick master . Make your changes in the docs/ directory of your local clone, following the style guide : If you are making changes to an existing page: Open mkdocs.yml and find the location of the file relative to the docs/ directory Make your changes to that file and move onto the next step If you are contributing a new page: Name the page. Page file names should be lowercase, - delimited, and concise but descriptive, e.g. markdown-migration.md or cutting-release.md Place the page in the relevant sub-folder of the docs/ directory. If you are unsure of the appropriate location, note that in the description of the pull request. Add the document to the nav: section of mkdocs.yml in APA-style title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' Note If mkdocs.yml contains does not contain a nav: section, add the above to the pages: section instead. This means that the repository is using an older version of MkDocs and will need to be transitioned to GitHub Actions . If you are writing site administrator documentation, following the suggested document layout If you haven't already, start a Mkdocs development server to preview your changes . Continue making changes until you are satisfied with the preview, then stage your changes in git: $ git add <YOUR FILE> <YOUR 2nd FILE>...<YOUR Nth FILE> Where <YOUR * FILE> is any file that contains changes that you'd wish to make. If you are adding a new page, one of the files should be mkdocs.yml . Commit your changes and push them to your Github fork: $ git commit -m \"<DESCRIPTIVE COMMIT MESSAGE>\" $ git push origin Where <DESCRIPTIVE COMMIT MESSAGE> is a meaningful short text that identifies the changes applied, it is a good practice, to concatenate the ticket number associated e.g. \"Removing color macros (SOFTWARE-3739)\" From your Github fork, submit a pull request","title":"Making content changes"},{"location":"documentation/writing-documentation/#document-layout","text":"This section contains suggested layouts of externally-facing, site administrator documentation . The introduction is the only layout requirement for documents except for installation guides.","title":"Document Layout"},{"location":"documentation/writing-documentation/#introductions","text":"All documents should start with an introduction that explains what the document contains, what the product does, and why someone may want to use it. In the past, document introductions were included in About this... sections due to the layout of the table of contents. Since the table of contents is included in the sidebar this is unnecessary and introduction content should go directly below the title heading without any second-level headings. The HTCondor-CE installation guide is an example that meet all of the above criteria.","title":"Introductions"},{"location":"documentation/writing-documentation/#installation-guides","text":"In addition to the introduction above, installation documents should have the following sections: Before Starting: This section should contain information for any prepatory work that the site administrator should do or consider before proceeding with the installation ( example ). Installation: Procedural instructions that tell the user how to install the software ( example ) Validation: How does the user make sure their installation is functional? Optionally, the following sections should be included as necessary. Overview: if the introduction becomes large and unwieldy, extract the details of what the product does into an overview section Configuration: required configuration steps ( example ) as well as a sub-section for optional configurations. For long optional configuration sections, consider creating alist of contents at the top of the sub-section ( example ). Troubleshooting: common issues that users encounter and their fixes Reference: Details about configuration and log files, unix users, certificates, networking, links to relevant upstream documentation, etc. ( example ) If any of the sections become too large, consider separating them out and linking to the new documents ( example ).","title":"Installation guides"},{"location":"documentation/writing-documentation/#tips-for-writing-procedural-instructions","text":"Title the procedure with the user goal, usually starting with a gerund; e.g.: Installing the Frobnosticator Number all steps (as opposed to using bullets) List steps in order in which they are performed Each step should begin with a single-line instruction in plain English, in command form; e.g.: Make sure that the Frobnosticator configuration file is world-writable If the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.: chmod a+x /usr/share/frobnosticator/frob.conf Put clarifying information in separate paragraphs within the step Put critical information about the whole procedure in one or more paragraphs before the numbered steps Put supplemental information about the whole procedure in one or more paragraphs after the numbered steps Avoid pronouns when writing technical articles or documentation e.g., install foo rather than install it . Avoid superfluous statements like you will want , you want , you should e.g., install foo rather than you will want to install foo . Use the imperative form in step-by-step instructions, e.g. install package foo rather than the package foo should be installed","title":"Tips for Writing Procedural Instructions"},{"location":"meetings/TechAreaTemplate/","text":"OSG Technology Area Meeting, 17 July 2017 \u00b6 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Jeff, Marian, Marty, Mat, Suchandra, TimC, TimT, Xin Announcements \u00b6 Triage Duty \u00b6 This week: Next week: ( ) open tickets JIRA \u00b6 # of tickets \u0394 State 141 -17 Open 33 +9 In Progress 4 +2 Ready for Testing 0 -12 Ready for Release Release Schedule \u00b6 Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team \u00b6 Discussions \u00b6 Support Update \u00b6 OSG Release Team \u00b6 3.3.27 \u0394 Both \u0394 3.4.2 \u0394 Total \u0394 Status 1 +0 4 +0 2 +0 7 +0 Open 0 +0 10 +0 4 +0 14 +0 In Progress 2 +0 1 +0 1 +0 4 +0 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +0 15 +0 7 +0 25 +0 Total Discussions \u00b6 OSG Investigations Team \u00b6 Last Week \u00b6 This Week \u00b6 Ongoing \u00b6","title":"OSG Technology Area Meeting, 17 July 2017"},{"location":"meetings/TechAreaTemplate/#osg-technology-area-meeting-17-july-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Jeff, Marian, Marty, Mat, Suchandra, TimC, TimT, Xin","title":"OSG Technology Area Meeting, 17 July 2017"},{"location":"meetings/TechAreaTemplate/#announcements","text":"","title":"Announcements"},{"location":"meetings/TechAreaTemplate/#triage-duty","text":"This week: Next week: ( ) open tickets","title":"Triage Duty"},{"location":"meetings/TechAreaTemplate/#jira","text":"# of tickets \u0394 State 141 -17 Open 33 +9 In Progress 4 +2 Ready for Testing 0 -12 Ready for Release","title":"JIRA"},{"location":"meetings/TechAreaTemplate/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/TechAreaTemplate/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/TechAreaTemplate/#discussions","text":"","title":"Discussions"},{"location":"meetings/TechAreaTemplate/#support-update","text":"","title":"Support Update"},{"location":"meetings/TechAreaTemplate/#osg-release-team","text":"3.3.27 \u0394 Both \u0394 3.4.2 \u0394 Total \u0394 Status 1 +0 4 +0 2 +0 7 +0 Open 0 +0 10 +0 4 +0 14 +0 In Progress 2 +0 1 +0 1 +0 4 +0 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +0 15 +0 7 +0 25 +0 Total","title":"OSG Release Team"},{"location":"meetings/TechAreaTemplate/#discussions_1","text":"","title":"Discussions"},{"location":"meetings/TechAreaTemplate/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/TechAreaTemplate/#last-week","text":"","title":"Last Week"},{"location":"meetings/TechAreaTemplate/#this-week","text":"","title":"This Week"},{"location":"meetings/TechAreaTemplate/#ongoing","text":"","title":"Ongoing"},{"location":"meetings/2024/TechArea20240102/","text":"OSG Technology Area Meeting, 2 January 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Marco, Mat, Matt, TimT Announcements \u00b6 BrianL OOO Dec 20 - Jan 5 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: BrianL 8 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 245 +1 Open 28 +5 Selected for Dev 24 +0 In Progress 18 +0 Dev Complete 2 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Brian B was unable to reproduce the failures that the osg-ca-certs package change is meant to work around (and so unable to verify that the workaround worked). We will need to ask the original reporter to reproduce it AI (Mat): add Institution to OSG ID mapping Topology endpoint AI (Mat): release xrdcl-pelican AI (Matt): replace the deprecated save-state and set-output commands in GitHub Actions with Environment files Discussion \u00b6 GlideinWMS 3.10.6 RC is planned for the end of this week, containing mostly bugfixes: Fix for the Python-based match policy function not being correctly imported Compatibility with new versions of the M2Crypto library New factory config options by Marco Mascheroni HTCondor 23.3.0/23.0.3 scheduled for Thursday Support Update \u00b6 Mat (KAGRA): Tokyo site reporting issues while testing origin authentication; Mat will continue to debug DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing ospool-ep 1.0 Ready for Release Nothing yet Discussion \u00b6","title":"January 2, 2024"},{"location":"meetings/2024/TechArea20240102/#osg-technology-area-meeting-2-january-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Marco, Mat, Matt, TimT","title":"OSG Technology Area Meeting, 2 January 2024"},{"location":"meetings/2024/TechArea20240102/#announcements","text":"BrianL OOO Dec 20 - Jan 5","title":"Announcements"},{"location":"meetings/2024/TechArea20240102/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: BrianL 8 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240102/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 245 +1 Open 28 +5 Selected for Dev 24 +0 In Progress 18 +0 Dev Complete 2 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240102/#osg-software-team","text":"Brian B was unable to reproduce the failures that the osg-ca-certs package change is meant to work around (and so unable to verify that the workaround worked). We will need to ask the original reporter to reproduce it AI (Mat): add Institution to OSG ID mapping Topology endpoint AI (Mat): release xrdcl-pelican AI (Matt): replace the deprecated save-state and set-output commands in GitHub Actions with Environment files","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240102/#discussion","text":"GlideinWMS 3.10.6 RC is planned for the end of this week, containing mostly bugfixes: Fix for the Python-based match policy function not being correctly imported Compatibility with new versions of the M2Crypto library New factory config options by Marco Mascheroni HTCondor 23.3.0/23.0.3 scheduled for Thursday","title":"Discussion"},{"location":"meetings/2024/TechArea20240102/#support-update","text":"Mat (KAGRA): Tokyo site reporting issues while testing origin authentication; Mat will continue to debug","title":"Support Update"},{"location":"meetings/2024/TechArea20240102/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240102/#osg-release-team","text":"Ready for Testing ospool-ep 1.0 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240102/#discussion_1","text":"","title":"Discussion"},{"location":"meetings/2024/TechArea20240109/","text":"OSG Technology Area Meeting, 9 January 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian L, Marco, Mat, Matt, Tim T Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: Mat 9 (+1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 245 +1 Open 28 +5 Selected for Dev 24 +0 In Progress 18 +0 Dev Complete 2 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes hackathon: - AI (BrianL): clean up last Flux v1 Helm Charts, OSG 23 container image roundup - AI (Matt): Add notifications to xDD MongoDB backups - AI (Matt): Continue work on OSG institution frontend + OAuth2 - AI (Mat): new Yum repos on Tempest, auto-update EPs on the PATh Facility - AI (Mat): Investigate Kubernetes 1.27 upgrade changes (especially Kustomize-related) Release: - AI (Mat): Assist Carl Vuosalo in regaining access to Koji (so he can build Frontier Squid) - AI (Matt): build VOMS for EL9 Miscellaneous: - AI (Matt): replace the deprecated save-state and set-output commands in GitHub Actions with Environment files Discussion \u00b6 Marco: - Release candidate 3.10.6 expected soon: - CMS has asked for additional factory attributes - Bugfix for match policy - Added compatibility for new versions of m2crypto - The feature for sending arbitrary tokens along with the Glideins (for example for testing authenticated access in the pilot startup scripts) is in the devel (3.11) series. HTCondor code freezes will be on Tuesday instead of Friday to give CHTC additional time for testing new release candidates before deploying them on OSPool. There was some discussion with regards to versions and testing on both at the same time; Brian L, Mat, and Tim T will meet for further discussion. Support Update \u00b6 Mat: Continued assisting with Kagra origin DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing yet Ready for Release Nothing yet Discussion \u00b6","title":"January 9, 2024"},{"location":"meetings/2024/TechArea20240109/#osg-technology-area-meeting-9-january-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian L, Marco, Mat, Matt, Tim T","title":"OSG Technology Area Meeting, 9 January 2024"},{"location":"meetings/2024/TechArea20240109/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240109/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: Mat 9 (+1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240109/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 245 +1 Open 28 +5 Selected for Dev 24 +0 In Progress 18 +0 Dev Complete 2 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240109/#osg-software-team","text":"Kubernetes hackathon: - AI (BrianL): clean up last Flux v1 Helm Charts, OSG 23 container image roundup - AI (Matt): Add notifications to xDD MongoDB backups - AI (Matt): Continue work on OSG institution frontend + OAuth2 - AI (Mat): new Yum repos on Tempest, auto-update EPs on the PATh Facility - AI (Mat): Investigate Kubernetes 1.27 upgrade changes (especially Kustomize-related) Release: - AI (Mat): Assist Carl Vuosalo in regaining access to Koji (so he can build Frontier Squid) - AI (Matt): build VOMS for EL9 Miscellaneous: - AI (Matt): replace the deprecated save-state and set-output commands in GitHub Actions with Environment files","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240109/#discussion","text":"Marco: - Release candidate 3.10.6 expected soon: - CMS has asked for additional factory attributes - Bugfix for match policy - Added compatibility for new versions of m2crypto - The feature for sending arbitrary tokens along with the Glideins (for example for testing authenticated access in the pilot startup scripts) is in the devel (3.11) series. HTCondor code freezes will be on Tuesday instead of Friday to give CHTC additional time for testing new release candidates before deploying them on OSPool. There was some discussion with regards to versions and testing on both at the same time; Brian L, Mat, and Tim T will meet for further discussion.","title":"Discussion"},{"location":"meetings/2024/TechArea20240109/#support-update","text":"Mat: Continued assisting with Kagra origin","title":"Support Update"},{"location":"meetings/2024/TechArea20240109/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240109/#osg-release-team","text":"Ready for Testing Nothing yet Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240109/#discussion_1","text":"","title":"Discussion"},{"location":"meetings/2024/TechArea20240116/","text":"OSG Technology Area Meeting, 16 January 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Matt 8 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 250 +5 Open 28 +0 Selected for Dev 21 -3 In Progress 17 -1 Dev Complete 2 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Release: - Frontier Squid + CarlV osg-build status? - AI (Matt): build VOMS for EL9 - AI (Mat): release xrdcl-pelican Miscellaneous: - AI (Matt): replace the deprecated save-state and set-output commands in GitHub Actions with Environment files - AI (BrianL, Mat) Investigate automatic build failures in osg-software-base over the weekend - Timeout issues with koji? - Potential issues with space usage - Marco: New GlideinWMS release is coming soon Discussion \u00b6 None this week Support Update \u00b6 Rachel and Showmic noted issue in user enrollment - unable to submit form due to required fields \"missing\" Matt W - BNL contact issue resolved via topology PR at end of last week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion \u00b6","title":"January 16, 2024"},{"location":"meetings/2024/TechArea20240116/#osg-technology-area-meeting-16-january-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 16 January 2024"},{"location":"meetings/2024/TechArea20240116/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240116/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Matt 8 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240116/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 250 +5 Open 28 +0 Selected for Dev 21 -3 In Progress 17 -1 Dev Complete 2 +1 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240116/#osg-software-team","text":"Release: - Frontier Squid + CarlV osg-build status? - AI (Matt): build VOMS for EL9 - AI (Mat): release xrdcl-pelican Miscellaneous: - AI (Matt): replace the deprecated save-state and set-output commands in GitHub Actions with Environment files - AI (BrianL, Mat) Investigate automatic build failures in osg-software-base over the weekend - Timeout issues with koji? - Potential issues with space usage - Marco: New GlideinWMS release is coming soon","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240116/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240116/#support-update","text":"Rachel and Showmic noted issue in user enrollment - unable to submit form due to required fields \"missing\" Matt W - BNL contact issue resolved via topology PR at end of last week","title":"Support Update"},{"location":"meetings/2024/TechArea20240116/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240116/#osg-release-team","text":"Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240116/#discussion_1","text":"","title":"Discussion"},{"location":"meetings/2024/TechArea20240123/","text":"OSG Technology Area Meeting, 23 January 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian, Marco, Mat, Matt, Tim Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: Tim T 8 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 250 +5 Open 28 +0 Selected for Dev 21 -3 In Progress 17 -1 Dev Complete 2 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes hackathon - Potential 1.27 upgrade tasks - Matt: Enable shoveler on PATh Facility origin Release: - Frontier Squid status? - AI (Mat): Release XRootD 5.6.5+ - AI (Mat): release xrdcl-pelican - AI (Matt): Update oidc-agent to 5.1.0+; test against the token renewer OSG repo tasks: - Remove HTCondor RC syncing to upcoming-testing. Maybe we should do the same for testing and have the dev team create an updates repo for the LTS series? - Add the ability to remove HTCondor RPMs Discussion \u00b6 Tim T: HTCondor 23.3.1 will be released with Pelican 7.4.0 HTCondor 23.4.0 in progress; recent builds fixed the central manager memory leak issue Marco: GlideinWMS will be built in OSG development today Support Update \u00b6 Kagra origin (Mat): Authfiles weren't getting local changes because they had an outdated mirror of the OSG repos and therefore and old version of the authfile updater DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion \u00b6","title":"January 23, 2024"},{"location":"meetings/2024/TechArea20240123/#osg-technology-area-meeting-23-january-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian, Marco, Mat, Matt, Tim","title":"OSG Technology Area Meeting, 23 January 2024"},{"location":"meetings/2024/TechArea20240123/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240123/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: Tim T 8 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240123/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 250 +5 Open 28 +0 Selected for Dev 21 -3 In Progress 17 -1 Dev Complete 2 +1 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240123/#osg-software-team","text":"Kubernetes hackathon - Potential 1.27 upgrade tasks - Matt: Enable shoveler on PATh Facility origin Release: - Frontier Squid status? - AI (Mat): Release XRootD 5.6.5+ - AI (Mat): release xrdcl-pelican - AI (Matt): Update oidc-agent to 5.1.0+; test against the token renewer OSG repo tasks: - Remove HTCondor RC syncing to upcoming-testing. Maybe we should do the same for testing and have the dev team create an updates repo for the LTS series? - Add the ability to remove HTCondor RPMs","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240123/#discussion","text":"Tim T: HTCondor 23.3.1 will be released with Pelican 7.4.0 HTCondor 23.4.0 in progress; recent builds fixed the central manager memory leak issue Marco: GlideinWMS will be built in OSG development today","title":"Discussion"},{"location":"meetings/2024/TechArea20240123/#support-update","text":"Kagra origin (Mat): Authfiles weren't getting local changes because they had an outdated mirror of the OSG repos and therefore and old version of the authfile updater","title":"Support Update"},{"location":"meetings/2024/TechArea20240123/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240123/#osg-release-team","text":"Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240123/#discussion_1","text":"","title":"Discussion"},{"location":"meetings/2024/TechArea20240130/","text":"OSG Technology Area Meeting, 30 January 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Upgrading Flux on Tiger today Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Tim T Next week: BrianL (?) 7 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 251 -2 Open 25 -1 Selected for Dev 21 +2 In Progress 17 -1 Dev Complete 3 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Doc focus afternoon of Feb 9 Release: - AI (Mat): Release XRootD 5.6.5+ - AI (Mat): release xrdcl-pelican - AI (Matt): Update oidc-agent to 5.1.0+; test against the token renewer OSG Yum repo: - AI (Matt): Remove HTCondor RC syncing to upcoming-testing. Maybe we should do the same for testing and have the dev team create an updates repo for the LTS series? - AI (Matt): serve 23-internal from repo Miscellaneous: - AI (Matt): create Pelican cache chart (focusing on OSDF) - AI (BrianL): move OSPool EP gratia probe to OSG 23 (requires 23-internal) Discussion \u00b6 None this week Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion \u00b6 None this week","title":"January 30, 2024"},{"location":"meetings/2024/TechArea20240130/#osg-technology-area-meeting-30-january-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 30 January 2024"},{"location":"meetings/2024/TechArea20240130/#announcements","text":"Upgrading Flux on Tiger today","title":"Announcements"},{"location":"meetings/2024/TechArea20240130/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Tim T Next week: BrianL (?) 7 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240130/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 251 -2 Open 25 -1 Selected for Dev 21 +2 In Progress 17 -1 Dev Complete 3 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240130/#osg-software-team","text":"Doc focus afternoon of Feb 9 Release: - AI (Mat): Release XRootD 5.6.5+ - AI (Mat): release xrdcl-pelican - AI (Matt): Update oidc-agent to 5.1.0+; test against the token renewer OSG Yum repo: - AI (Matt): Remove HTCondor RC syncing to upcoming-testing. Maybe we should do the same for testing and have the dev team create an updates repo for the LTS series? - AI (Matt): serve 23-internal from repo Miscellaneous: - AI (Matt): create Pelican cache chart (focusing on OSDF) - AI (BrianL): move OSPool EP gratia probe to OSG 23 (requires 23-internal)","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240130/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240130/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240130/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240130/#osg-release-team","text":"Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240130/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240206/","text":"OSG Technology Area Meeting, 6 February 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: MattW Next week: Mat (?) 7 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 251 -2 Open 25 -1 Selected for Dev 21 +2 In Progress 17 -1 Dev Complete 3 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Doc focus afternoon of Feb 9 Kubernetes Hackathon: - AI (BrianL): move OSPool EP gratia probe to OSG 23 - AI (Mat): assist William with 7.5.0 upgrades, review OSDF Pelican cache and origin bases. Keep an eye toward splitting out a non-OSDF Pelican cache/origin base. - AI (Matt): Kick off xDD mongodb migration to k8s Release: - AI (Mat): troubleshoot OSDF cache tests blocking XRootD and xrdcl-pelican releases - AI (Matt): Cut a new release of osg-token-renewer - AI (Matt): Release XRootD 5.6.7 - Carry over BrianB's patches from 5.6.6 Discussion \u00b6 None this week Support Update \u00b6 Hosted CEs (BrianL, Mat): ran into issues with %ghost dirs in osg-configure not being created in the version of RPM available on EL9 (and maybe EL8?) Fix: Explicitly add ghost directories to file list Issue may occur in other packages as well Emails (BrianL): Google and other email providers are starting to get stricter about SPF records meeting RFC requirements. We are getting bitten by the 10 DNS lookups in the SPF record max. Freshdesk alone uses 10. XRootD (Matt W): New upstream might fix long-running JLab issue DevOps \u00b6 Derek: Profiling performance of XRootD monitoring shoveler, DNS lookups and logging found to be a choke-point Derek/BrianL: Configure a new Traefik instance GRACC conversion to Tiger OpenSearch is ongoing. Derek did some profiling and optimization of xrootd collector: https://derekweitzel.com/2024/01/31/profiling-xrootd-collector/ OSG Release Team \u00b6 Ready for Testing osg-configure, frontier-squid, voms, vault frontier-squid is high priority, contains a number of CVEs Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion topic: Running OSG tests prior to promoting daily condor releases The current config supports this, osg-development already pulls from condor daily repos Discussion \u00b6 None this week","title":"February 6, 2024"},{"location":"meetings/2024/TechArea20240206/#osg-technology-area-meeting-6-february-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 6 February 2024"},{"location":"meetings/2024/TechArea20240206/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240206/#triage-duty","text":"Triage duty shifts Tue-Mon This week: MattW Next week: Mat (?) 7 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240206/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 251 -2 Open 25 -1 Selected for Dev 21 +2 In Progress 17 -1 Dev Complete 3 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240206/#osg-software-team","text":"Doc focus afternoon of Feb 9 Kubernetes Hackathon: - AI (BrianL): move OSPool EP gratia probe to OSG 23 - AI (Mat): assist William with 7.5.0 upgrades, review OSDF Pelican cache and origin bases. Keep an eye toward splitting out a non-OSDF Pelican cache/origin base. - AI (Matt): Kick off xDD mongodb migration to k8s Release: - AI (Mat): troubleshoot OSDF cache tests blocking XRootD and xrdcl-pelican releases - AI (Matt): Cut a new release of osg-token-renewer - AI (Matt): Release XRootD 5.6.7 - Carry over BrianB's patches from 5.6.6","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240206/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240206/#support-update","text":"Hosted CEs (BrianL, Mat): ran into issues with %ghost dirs in osg-configure not being created in the version of RPM available on EL9 (and maybe EL8?) Fix: Explicitly add ghost directories to file list Issue may occur in other packages as well Emails (BrianL): Google and other email providers are starting to get stricter about SPF records meeting RFC requirements. We are getting bitten by the 10 DNS lookups in the SPF record max. Freshdesk alone uses 10. XRootD (Matt W): New upstream might fix long-running JLab issue","title":"Support Update"},{"location":"meetings/2024/TechArea20240206/#devops","text":"Derek: Profiling performance of XRootD monitoring shoveler, DNS lookups and logging found to be a choke-point Derek/BrianL: Configure a new Traefik instance GRACC conversion to Tiger OpenSearch is ongoing. Derek did some profiling and optimization of xrootd collector: https://derekweitzel.com/2024/01/31/profiling-xrootd-collector/","title":"DevOps"},{"location":"meetings/2024/TechArea20240206/#osg-release-team","text":"Ready for Testing osg-configure, frontier-squid, voms, vault frontier-squid is high priority, contains a number of CVEs Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion topic: Running OSG tests prior to promoting daily condor releases The current config supports this, osg-development already pulls from condor daily repos","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240206/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240213/","text":"OSG Technology Area Meeting, 13 February 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, TimT Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 5 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 250 -1 Open 28 +3 Selected for Dev 23 -4 In Progress 17 +0 Dev Complete 5 +2 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Release: - AI (Mat): Cut a new release of osg-token-renewer - AI (BrianL): Update OSG 3.6 and 23 release notes with upcoming EOL warnings Discussion \u00b6 TimT: HTCondor 23.5 being deployed on CHTC today for testing; release planned for early March; will coordinate with Pelican team to include Pelican 7.6 in the release BrianB spotted SHA1 code in HTCondor, which should not work in EL9, but does not appear to cause problems in practice There was some discussion regarding aligning HTCondor-CE releases with HTCondor releases but more investigation needs to be done in order to come up with a solution that meets everybody's needs without much increase in workload Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion \u00b6 None this week","title":"February 13, 2024"},{"location":"meetings/2024/TechArea20240213/#osg-technology-area-meeting-13-february-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, TimT","title":"OSG Technology Area Meeting, 13 February 2024"},{"location":"meetings/2024/TechArea20240213/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240213/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 5 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240213/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 250 -1 Open 28 +3 Selected for Dev 23 -4 In Progress 17 +0 Dev Complete 5 +2 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240213/#osg-software-team","text":"Release: - AI (Mat): Cut a new release of osg-token-renewer - AI (BrianL): Update OSG 3.6 and 23 release notes with upcoming EOL warnings","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240213/#discussion","text":"TimT: HTCondor 23.5 being deployed on CHTC today for testing; release planned for early March; will coordinate with Pelican team to include Pelican 7.6 in the release BrianB spotted SHA1 code in HTCondor, which should not work in EL9, but does not appear to cause problems in practice There was some discussion regarding aligning HTCondor-CE releases with HTCondor releases but more investigation needs to be done in order to come up with a solution that meets everybody's needs without much increase in workload","title":"Discussion"},{"location":"meetings/2024/TechArea20240213/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240213/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240213/#osg-release-team","text":"Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240213/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240220/","text":"OSG Technology Area Meeting, 20 February 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, TimT Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 5 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 250 -1 Open 28 +3 Selected for Dev 23 -4 In Progress 17 +0 Dev Complete 5 +2 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Release: - AI (Mat): Cut a new release of osg-token-renewer - AI (BrianL): Update OSG 3.6 and 23 release notes with upcoming EOL warnings Discussion \u00b6 TimT: HTCondor 23.5 being deployed on CHTC today for testing; release planned for early March; will coordinate with Pelican team to include Pelican 7.6 in the release BrianB spotted SHA1 code in HTCondor, which should not work in EL9, but does not appear to cause problems in practice There was some discussion regarding aligning HTCondor-CE releases with HTCondor releases but more investigation needs to be done in order to come up with a solution that meets everybody's needs without much increase in workload Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion \u00b6 None this week","title":"February 20, 2024"},{"location":"meetings/2024/TechArea20240220/#osg-technology-area-meeting-20-february-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, TimT","title":"OSG Technology Area Meeting, 20 February 2024"},{"location":"meetings/2024/TechArea20240220/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240220/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 5 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240220/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 250 -1 Open 28 +3 Selected for Dev 23 -4 In Progress 17 +0 Dev Complete 5 +2 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240220/#osg-software-team","text":"Release: - AI (Mat): Cut a new release of osg-token-renewer - AI (BrianL): Update OSG 3.6 and 23 release notes with upcoming EOL warnings","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240220/#discussion","text":"TimT: HTCondor 23.5 being deployed on CHTC today for testing; release planned for early March; will coordinate with Pelican team to include Pelican 7.6 in the release BrianB spotted SHA1 code in HTCondor, which should not work in EL9, but does not appear to cause problems in practice There was some discussion regarding aligning HTCondor-CE releases with HTCondor releases but more investigation needs to be done in order to come up with a solution that meets everybody's needs without much increase in workload","title":"Discussion"},{"location":"meetings/2024/TechArea20240220/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240220/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240220/#osg-release-team","text":"Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240220/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240227/","text":"OSG Technology Area Meeting, 27 February 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, MattW, TimT Announcements \u00b6 MattW out until Thursday TimT out at the end of March Mat out at the end of March/beginning of April Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: TimT Next week: TimT 4 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 247 -2 Open 25 +1 Selected for Dev 23 +1 In Progress 15 -1 Dev Complete 3 -3 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 AI (Mat): Prometheus metrics for Topology AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster Discussion \u00b6 HTCondor 23.5.0 is on CHTC and working well, including thick pool provisioning. JLab (BrianB): issues with their OSPool flocking AP coinciding with the update to HTCondor 23. Root cause appears to come from GlideinWMS packaging setting DC_DAEMON_LIST = + $(DAEMON_LIST) Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases Discussion \u00b6 None this week","title":"February 27, 2024"},{"location":"meetings/2024/TechArea20240227/#osg-technology-area-meeting-27-february-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, MattW, TimT","title":"OSG Technology Area Meeting, 27 February 2024"},{"location":"meetings/2024/TechArea20240227/#announcements","text":"MattW out until Thursday TimT out at the end of March Mat out at the end of March/beginning of April","title":"Announcements"},{"location":"meetings/2024/TechArea20240227/#triage-duty","text":"Triage duty shifts Tue-Mon This week: TimT Next week: TimT 4 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240227/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 247 -2 Open 25 +1 Selected for Dev 23 +1 In Progress 15 -1 Dev Complete 3 -3 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240227/#osg-software-team","text":"AI (Mat): Prometheus metrics for Topology AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240227/#discussion","text":"HTCondor 23.5.0 is on CHTC and working well, including thick pool provisioning. JLab (BrianB): issues with their OSPool flocking AP coinciding with the update to HTCondor 23. Root cause appears to come from GlideinWMS packaging setting DC_DAEMON_LIST = + $(DAEMON_LIST)","title":"Discussion"},{"location":"meetings/2024/TechArea20240227/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240227/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240227/#osg-release-team","text":"Ready for Testing vault, htvault-config both low priority packages Ready for Release Nothing yet Outstanding question: how to announce Pelican releases","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240227/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240305/","text":"OSG Technology Area Meeting, 5 March 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 TimT out of office Mar 18 - 29 Mat out of office Mar 25 - Apr 8 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: TimT Next week: Mat 3 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 241 -5 Open 25 +0 Selected for Dev 21 -2 In Progress 15 +0 Dev Complete 0 -3 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Release: - Release new vo-client with new k8s-based IAM at CERN (awaiting response from Maarten Litmaath) Miscellaneous: - AI (Matt): Kubernetes Gratia accsounting - AI (BrianL, Mat): migrate Harbor to Tiger - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster Discussion \u00b6 None this week Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing yet Ready for Release Nothing yet Discussion \u00b6 None this week","title":"March 5, 2024"},{"location":"meetings/2024/TechArea20240305/#osg-technology-area-meeting-5-march-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 5 March 2024"},{"location":"meetings/2024/TechArea20240305/#announcements","text":"TimT out of office Mar 18 - 29 Mat out of office Mar 25 - Apr 8","title":"Announcements"},{"location":"meetings/2024/TechArea20240305/#triage-duty","text":"Triage duty shifts Tue-Mon This week: TimT Next week: Mat 3 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240305/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 241 -5 Open 25 +0 Selected for Dev 21 -2 In Progress 15 +0 Dev Complete 0 -3 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240305/#osg-software-team","text":"Release: - Release new vo-client with new k8s-based IAM at CERN (awaiting response from Maarten Litmaath) Miscellaneous: - AI (Matt): Kubernetes Gratia accsounting - AI (BrianL, Mat): migrate Harbor to Tiger - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240305/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240305/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240305/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240305/#osg-release-team","text":"Ready for Testing Nothing yet Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240305/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240312/","text":"OSG Technology Area Meeting, 12 March 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 TimT out of office Mar 18 - 29 Mat out of office Mar 25 - Apr 8 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Matt 1 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 241 +0 Open 23 -2 Selected for Dev 20 -1 In Progress 15 +0 Dev Complete 1 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Doc focus this afternoon Release: - AI (Mat, BrianL to create ticket): Release new vo-client with new k8s-based IAM at CERN - AI (Matt): Release new IGTF cert update - AI (Mat): Release new vo-client - Provide certificate information for non-production servers - AI: XRootD 5.6.9 Miscellaneous: - Pascal to visit Madison on Wednesday - AI (Matt): Kubernetes Gratia accsounting - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster Discussion \u00b6 None this week Support Update \u00b6 Marco: Condor bug triggered by GlideinWMS multi-line error log message, fix in the works BrianL: Fix PATh after moving harbor from river to tempest which caused networking issues BrianL: File transfer plugin issues with JLab, may need to update condor version for specific group DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing osg-xrootd Ready for Release Nothing yet Planned for this week: igtf-ca-certs, vo-client, osg-xrootd Discussion \u00b6 OSPool Ops meeting CHTC team suggests blessed RCs for testing in OSPool Jeff Dost is pushing back, suggests only allowing full releases Blessed RCs are similar to previous updates repo, should be a non-disruptive change","title":"March 12, 2024"},{"location":"meetings/2024/TechArea20240312/#osg-technology-area-meeting-12-march-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 12 March 2024"},{"location":"meetings/2024/TechArea20240312/#announcements","text":"TimT out of office Mar 18 - 29 Mat out of office Mar 25 - Apr 8","title":"Announcements"},{"location":"meetings/2024/TechArea20240312/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Matt 1 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240312/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 241 +0 Open 23 -2 Selected for Dev 20 -1 In Progress 15 +0 Dev Complete 1 +1 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240312/#osg-software-team","text":"Doc focus this afternoon Release: - AI (Mat, BrianL to create ticket): Release new vo-client with new k8s-based IAM at CERN - AI (Matt): Release new IGTF cert update - AI (Mat): Release new vo-client - Provide certificate information for non-production servers - AI: XRootD 5.6.9 Miscellaneous: - Pascal to visit Madison on Wednesday - AI (Matt): Kubernetes Gratia accsounting - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240312/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240312/#support-update","text":"Marco: Condor bug triggered by GlideinWMS multi-line error log message, fix in the works BrianL: Fix PATh after moving harbor from river to tempest which caused networking issues BrianL: File transfer plugin issues with JLab, may need to update condor version for specific group","title":"Support Update"},{"location":"meetings/2024/TechArea20240312/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240312/#osg-release-team","text":"Ready for Testing osg-xrootd Ready for Release Nothing yet Planned for this week: igtf-ca-certs, vo-client, osg-xrootd","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240312/#discussion_1","text":"OSPool Ops meeting CHTC team suggests blessed RCs for testing in OSPool Jeff Dost is pushing back, suggests only allowing full releases Blessed RCs are similar to previous updates repo, should be a non-disruptive change","title":"Discussion"},{"location":"meetings/2024/TechArea20240319/","text":"OSG Technology Area Meeting, 19 March 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Matt Announcements \u00b6 TimT out of office Mar 18 - 29 Mat out of office Mar 25 - Apr 8 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: BrianL 5 (+4) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 241 +0 Open 23 -2 Selected for Dev 20 -1 In Progress 15 +0 Dev Complete 1 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes hackathon this afternoon - AI (Mat): Update base pelican-cache kustomization to follow best practices - AI (BrianL): Investigate RBAC issues Release: - AI (Mat): Deploy Pelican 7.6.1+ in the OSPool - AI (Mat): Release new vo-client with new k8s-based IAM at CERN - AI (Matt): Release new osg-scitokens-mapfile update - AI (Mat): XRootD 5.6.9 - AI (Mat, BrianL): Fix xcache config to use grid-mapfile pulled from Topology - AI (Mat, BrianL): Temporarily remove minimum Stash/OSDF plugin version transform from PATh Facility APs since many EPs do not have the necessary attribute Miscellaneous: - AI (BrianL, Mat, Matt): send Eduardo CSRs for access to the NET2 k8s cluster Discussion \u00b6 None this week Support Update \u00b6 IceCube: Assist IceCube with frontend configuration so they can run pilots on their HPC allocations. (Mat to discuss with Mats and Jeff Dost what needs to be done.) DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing osg-xrootd Ready for Release Nothing yet Planned for this week: igtf-ca-certs, vo-client, osg-xrootd Discussion \u00b6 None this week","title":"March 19, 2024"},{"location":"meetings/2024/TechArea20240319/#osg-technology-area-meeting-19-march-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Matt","title":"OSG Technology Area Meeting, 19 March 2024"},{"location":"meetings/2024/TechArea20240319/#announcements","text":"TimT out of office Mar 18 - 29 Mat out of office Mar 25 - Apr 8","title":"Announcements"},{"location":"meetings/2024/TechArea20240319/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: BrianL 5 (+4) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240319/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 241 +0 Open 23 -2 Selected for Dev 20 -1 In Progress 15 +0 Dev Complete 1 +1 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240319/#osg-software-team","text":"Kubernetes hackathon this afternoon - AI (Mat): Update base pelican-cache kustomization to follow best practices - AI (BrianL): Investigate RBAC issues Release: - AI (Mat): Deploy Pelican 7.6.1+ in the OSPool - AI (Mat): Release new vo-client with new k8s-based IAM at CERN - AI (Matt): Release new osg-scitokens-mapfile update - AI (Mat): XRootD 5.6.9 - AI (Mat, BrianL): Fix xcache config to use grid-mapfile pulled from Topology - AI (Mat, BrianL): Temporarily remove minimum Stash/OSDF plugin version transform from PATh Facility APs since many EPs do not have the necessary attribute Miscellaneous: - AI (BrianL, Mat, Matt): send Eduardo CSRs for access to the NET2 k8s cluster","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240319/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240319/#support-update","text":"IceCube: Assist IceCube with frontend configuration so they can run pilots on their HPC allocations. (Mat to discuss with Mats and Jeff Dost what needs to be done.)","title":"Support Update"},{"location":"meetings/2024/TechArea20240319/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240319/#osg-release-team","text":"Ready for Testing osg-xrootd Ready for Release Nothing yet Planned for this week: igtf-ca-certs, vo-client, osg-xrootd","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240319/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240402/","text":"OSG Technology Area Meeting, 2 April 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Mat out of office Mar 25 - Apr 8 Kubernetes in-person hackathon Register ASAP for this, need to work out details based on skillsets of registrants Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: MattW 4 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 255 +0 Open 23 +0 Selected for Dev 20 +1 In Progress 15 +0 Dev Complete 1 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 OSG 3.6 is in critical/security bug fix only mode Kubernetes hackathon this afternoon - AI (BrianL): add ClusterRoleBindings for querying namespaces and nodes - AI (Matt): Stand up JLab CM in osgdev (SOFTWARE-5805) Release: - AI (Matt): Build XRootD with IO time gstream monitoring patch (SOFTWARE-5850) Miscellaneous: - AI (Matt): start work on Gratia + KAPEL (learn how to install KAPEL, find GRACC ITB locations) - Need a solid foundation prior to the hackathon - Get local install of KAPEL going, point at GRACC ITB instance - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster - Marco: Working on new GlideinWMS release, testing multiplatform podman/docker images - CHTC will be getting an ARM machine in the coming months, better support for multiplatform testing - AI (Matt): Debug lack of shoveler messages for tiger OSDF origin (path-1) Discussion \u00b6 None this week Support Update \u00b6 AI (Matt): Support Saad in adding new SE for HPC4L DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet Discussion \u00b6 None this week","title":"April 2, 2024"},{"location":"meetings/2024/TechArea20240402/#osg-technology-area-meeting-2-april-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 2 April 2024"},{"location":"meetings/2024/TechArea20240402/#announcements","text":"Mat out of office Mar 25 - Apr 8 Kubernetes in-person hackathon Register ASAP for this, need to work out details based on skillsets of registrants","title":"Announcements"},{"location":"meetings/2024/TechArea20240402/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: MattW 4 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240402/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 255 +0 Open 23 +0 Selected for Dev 20 +1 In Progress 15 +0 Dev Complete 1 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240402/#osg-software-team","text":"OSG 3.6 is in critical/security bug fix only mode Kubernetes hackathon this afternoon - AI (BrianL): add ClusterRoleBindings for querying namespaces and nodes - AI (Matt): Stand up JLab CM in osgdev (SOFTWARE-5805) Release: - AI (Matt): Build XRootD with IO time gstream monitoring patch (SOFTWARE-5850) Miscellaneous: - AI (Matt): start work on Gratia + KAPEL (learn how to install KAPEL, find GRACC ITB locations) - Need a solid foundation prior to the hackathon - Get local install of KAPEL going, point at GRACC ITB instance - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster - Marco: Working on new GlideinWMS release, testing multiplatform podman/docker images - CHTC will be getting an ARM machine in the coming months, better support for multiplatform testing - AI (Matt): Debug lack of shoveler messages for tiger OSDF origin (path-1)","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240402/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240402/#support-update","text":"AI (Matt): Support Saad in adding new SE for HPC4L","title":"Support Update"},{"location":"meetings/2024/TechArea20240402/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240402/#osg-release-team","text":"Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240402/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240409/","text":"OSG Technology Area Meeting, 9 April 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, MatS, MattW Announcements \u00b6 Kubernetes in-person hackathon Apr 24 - 26 Planning to leave Madison mid-afternoon on Tue Apr 23, return the afternoon of Fri Apr 26 Book hotel in Concur (Sophy South Side, Hyatt Place South, The Study) Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: MattW 4 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 234 -1 Open 24 +1 Selected for Dev 22 +2 In Progress 15 +0 Dev Complete 1 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Release - Build patched XRootD 5.6.9 for Pelican - Build oasis-server 3.12 Miscellaneous - AI (Matt): start work on Gratia + KAPEL (learn how to install KAPEL, find GRACC ITB locations) - Need a solid foundation prior to the hackathon - Get local install of KAPEL going, point at GRACC ITB instance - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster - AI (Mat): migrate repo to EL9 Discussion \u00b6 GlideinWMS 3.11 expected in a couple of weeks; includes authentication changes and cvmfsexec. Support Update \u00b6 AI (Matt): Support Saad in adding new SE for HPC4L Marco: Assisting MIT with transition to EL9. The TRUST_DOMAIN of their HTCondor daemons no longer matches what their tokens were created with (a known change). DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet Discussion \u00b6 None this week","title":"April 9, 2024"},{"location":"meetings/2024/TechArea20240409/#osg-technology-area-meeting-9-april-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, MatS, MattW","title":"OSG Technology Area Meeting, 9 April 2024"},{"location":"meetings/2024/TechArea20240409/#announcements","text":"Kubernetes in-person hackathon Apr 24 - 26 Planning to leave Madison mid-afternoon on Tue Apr 23, return the afternoon of Fri Apr 26 Book hotel in Concur (Sophy South Side, Hyatt Place South, The Study)","title":"Announcements"},{"location":"meetings/2024/TechArea20240409/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: MattW 4 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240409/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 234 -1 Open 24 +1 Selected for Dev 22 +2 In Progress 15 +0 Dev Complete 1 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240409/#osg-software-team","text":"Release - Build patched XRootD 5.6.9 for Pelican - Build oasis-server 3.12 Miscellaneous - AI (Matt): start work on Gratia + KAPEL (learn how to install KAPEL, find GRACC ITB locations) - Need a solid foundation prior to the hackathon - Get local install of KAPEL going, point at GRACC ITB instance - AI (BrianL, Mat): send Eduardo CSRs for access to the NET2 k8s cluster - AI (Mat): migrate repo to EL9","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240409/#discussion","text":"GlideinWMS 3.11 expected in a couple of weeks; includes authentication changes and cvmfsexec.","title":"Discussion"},{"location":"meetings/2024/TechArea20240409/#support-update","text":"AI (Matt): Support Saad in adding new SE for HPC4L Marco: Assisting MIT with transition to EL9. The TRUST_DOMAIN of their HTCondor daemons no longer matches what their tokens were created with (a known change).","title":"Support Update"},{"location":"meetings/2024/TechArea20240409/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240409/#osg-release-team","text":"Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240409/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240416/","text":"OSG Technology Area Meeting, 16 April 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Cancel next week's meeting due to the IGWN face-to-face MattW OOO Mon Apr 23 BrianL OOO Apr 29-30, May 6-7 Kubernetes in-person hackathon Apr 24 - 26 Planning to leave Madison mid-afternoon on Tue Apr 23, return the afternoon of Fri Apr 26 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: MattW Next week: TimT 9 (+5) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 233 -1 Open 24 +0 Selected for Dev 22 +0 In Progress 15 +0 Dev Complete 0 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes Hackathon: - AI (BrianL): refactor S3 origins - AI (Matt): KAPEL + gratia-probe - AI (Mat): set up osg-services-dev Nautilus and UWDF / OSDF Tiger namespaces Release - AI (Matt): oasis-server 3.12 for devops + osg-23-internal - AI (Mat): osdf-server 7.7? Miscellaneous - AI (Matt): start work on Gratia + KAPEL (learn how to install KAPEL, find GRACC ITB locations) - AI (Mat): migrate repo to EL9 Discussion \u00b6 None this week Support Update \u00b6 AI (Matt): Support Saad in adding new SE for HPC4L DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet Discussion \u00b6 None this week","title":"April 16, 2024"},{"location":"meetings/2024/TechArea20240416/#osg-technology-area-meeting-16-april-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 16 April 2024"},{"location":"meetings/2024/TechArea20240416/#announcements","text":"Cancel next week's meeting due to the IGWN face-to-face MattW OOO Mon Apr 23 BrianL OOO Apr 29-30, May 6-7 Kubernetes in-person hackathon Apr 24 - 26 Planning to leave Madison mid-afternoon on Tue Apr 23, return the afternoon of Fri Apr 26","title":"Announcements"},{"location":"meetings/2024/TechArea20240416/#triage-duty","text":"Triage duty shifts Tue-Mon This week: MattW Next week: TimT 9 (+5) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240416/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 233 -1 Open 24 +0 Selected for Dev 22 +0 In Progress 15 +0 Dev Complete 0 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240416/#osg-software-team","text":"Kubernetes Hackathon: - AI (BrianL): refactor S3 origins - AI (Matt): KAPEL + gratia-probe - AI (Mat): set up osg-services-dev Nautilus and UWDF / OSDF Tiger namespaces Release - AI (Matt): oasis-server 3.12 for devops + osg-23-internal - AI (Mat): osdf-server 7.7? Miscellaneous - AI (Matt): start work on Gratia + KAPEL (learn how to install KAPEL, find GRACC ITB locations) - AI (Mat): migrate repo to EL9","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240416/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240416/#support-update","text":"AI (Matt): Support Saad in adding new SE for HPC4L","title":"Support Update"},{"location":"meetings/2024/TechArea20240416/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240416/#osg-release-team","text":"Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240416/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240430/","text":"OSG Technology Area Meeting, 30 April 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Matt, TimT Announcements \u00b6 BrianL OOO today Triage Duty \u00b6 Triage duty shifts Tue-Mon Note: deltas are between the 4/16 meeting and this one This week: MatS Next week: MattW 9 (+0) open FreshDesk tickets 1 (+1) open GGUS ticket Jira (as of Monday morning) \u00b6 Note: deltas are between the 4/16 meeting and this one # of tickets \u0394 State 233 +0 Open 22 -0 Selected for Dev 21 -1 In Progress 15 +0 Dev Complete 2 +2 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes Hackathon: - AI (Matt): Various XDD tasks - AI (Matt): Continue KAPEL + gratia-probe work (accounting for pilot containers via Kubernetes) - AI (Mat): Move KAPEL test EP from uwdf-dev namespace so William can use it for UWDF work - AI (Mat): Investigate ways to address Topology timeouts Miscellaneous: - AI (Mat): Investigate xcache failures in the nightlies with the latest Pelican version - AI (Mat): Migrate repo to EL9 Discussion \u00b6 HTCondor 23.7 is on CHTC; no problems found so far HTCondor 23.8 will include the \"AP product\" MattW: Investigate discrepency between KAPEL-based reporting and Gratia probe Ryan Taylor (KAPEL maintainer) is interested in collaborating with us more closely. Mat/BrianL will respond to his email. Support Update \u00b6 AI (Mat, BrianL): Support Saad in adding new CE for HPC4L; running into a variety of issues; Jaime from the HTCSS dev team is providing lots of assistance. Identified places where the documentation and the various software components could be improved. DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet Discussion \u00b6 None this week","title":"April 30, 2024"},{"location":"meetings/2024/TechArea20240430/#osg-technology-area-meeting-30-april-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Matt, TimT","title":"OSG Technology Area Meeting, 30 April 2024"},{"location":"meetings/2024/TechArea20240430/#announcements","text":"BrianL OOO today","title":"Announcements"},{"location":"meetings/2024/TechArea20240430/#triage-duty","text":"Triage duty shifts Tue-Mon Note: deltas are between the 4/16 meeting and this one This week: MatS Next week: MattW 9 (+0) open FreshDesk tickets 1 (+1) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240430/#jira-as-of-monday-morning","text":"Note: deltas are between the 4/16 meeting and this one # of tickets \u0394 State 233 +0 Open 22 -0 Selected for Dev 21 -1 In Progress 15 +0 Dev Complete 2 +2 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240430/#osg-software-team","text":"Kubernetes Hackathon: - AI (Matt): Various XDD tasks - AI (Matt): Continue KAPEL + gratia-probe work (accounting for pilot containers via Kubernetes) - AI (Mat): Move KAPEL test EP from uwdf-dev namespace so William can use it for UWDF work - AI (Mat): Investigate ways to address Topology timeouts Miscellaneous: - AI (Mat): Investigate xcache failures in the nightlies with the latest Pelican version - AI (Mat): Migrate repo to EL9","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240430/#discussion","text":"HTCondor 23.7 is on CHTC; no problems found so far HTCondor 23.8 will include the \"AP product\" MattW: Investigate discrepency between KAPEL-based reporting and Gratia probe Ryan Taylor (KAPEL maintainer) is interested in collaborating with us more closely. Mat/BrianL will respond to his email.","title":"Discussion"},{"location":"meetings/2024/TechArea20240430/#support-update","text":"AI (Mat, BrianL): Support Saad in adding new CE for HPC4L; running into a variety of issues; Jaime from the HTCSS dev team is providing lots of assistance. Identified places where the documentation and the various software components could be improved.","title":"Support Update"},{"location":"meetings/2024/TechArea20240430/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240430/#osg-release-team","text":"Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240430/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240507/","text":"OSG Technology Area Meeting, 7 May 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, MatS, MattW, TimT Announcements \u00b6 BrianL at CHTC planning retreat all day Thursday Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: MattW Next week: BrianL 5 (-4) open FreshDesk tickets 1 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 Note: deltas are between the 4/16 meeting and this one # of tickets \u0394 State 230 -3 Open 21 -1 Selected for Dev 22 +1 In Progress 15 +0 Dev Complete 2 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Doc focus this Friday afternoon - Matt: Provide README for Kubernetes Gratia Probe - Mat: Document OSPool testing procedure for new versions of Pelican Yum Repo (Mat): - Status update? - Pungi has been chosen as the replacement; Mat is familiarizing himself with it - Separately, if we retire vdt.cs.wisc.edu, what all needs updating? - osg-build and Koji will need to reference the new URLs for upstream sources and SVN Kubernetes Gratia Probe (Matt): - Where are we on data visualization? - BrianL has an ATLAS meeting today so may be able to find more about NET2 Prometheus authZ - If Ryan is on board, let's throw the helm chart bits into our helm chart repo under supported/iris-hep - For the k8s PromQL container, we can - Point Ryan at SOTERIA to get him Harbor access Discussion \u00b6 HTCondor: Rolling out a new 23.7.x release to fix an ssh-to-job issue on EL9 New release expected Thursday Depending on OSPool testing of Pelican 7.8.0, the HTCondor tarballs will either contain 7.8.0 or 7.7 with a backport of an error message patch GlideinWMS: Patch release candidate in progress; team has started using the new linting tool \"Ruff\" Includes setting Apptainer variables so its image cache is not written to home directories Improves provisioning of Glideins by letting Frontend admins configure thresholds for determining whether EPs are full Support Update \u00b6 HPC4L (Mat, BrianL, Jaime Frey): Various CE issues have been resolved and jobs are running DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet Discussion \u00b6 None this week","title":"May 7, 2024"},{"location":"meetings/2024/TechArea20240507/#osg-technology-area-meeting-7-may-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, MatS, MattW, TimT","title":"OSG Technology Area Meeting, 7 May 2024"},{"location":"meetings/2024/TechArea20240507/#announcements","text":"BrianL at CHTC planning retreat all day Thursday","title":"Announcements"},{"location":"meetings/2024/TechArea20240507/#triage-duty","text":"Triage duty shifts Tue-Mon This week: MattW Next week: BrianL 5 (-4) open FreshDesk tickets 1 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240507/#jira-as-of-monday-morning","text":"Note: deltas are between the 4/16 meeting and this one # of tickets \u0394 State 230 -3 Open 21 -1 Selected for Dev 22 +1 In Progress 15 +0 Dev Complete 2 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240507/#osg-software-team","text":"Doc focus this Friday afternoon - Matt: Provide README for Kubernetes Gratia Probe - Mat: Document OSPool testing procedure for new versions of Pelican Yum Repo (Mat): - Status update? - Pungi has been chosen as the replacement; Mat is familiarizing himself with it - Separately, if we retire vdt.cs.wisc.edu, what all needs updating? - osg-build and Koji will need to reference the new URLs for upstream sources and SVN Kubernetes Gratia Probe (Matt): - Where are we on data visualization? - BrianL has an ATLAS meeting today so may be able to find more about NET2 Prometheus authZ - If Ryan is on board, let's throw the helm chart bits into our helm chart repo under supported/iris-hep - For the k8s PromQL container, we can - Point Ryan at SOTERIA to get him Harbor access","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240507/#discussion","text":"HTCondor: Rolling out a new 23.7.x release to fix an ssh-to-job issue on EL9 New release expected Thursday Depending on OSPool testing of Pelican 7.8.0, the HTCondor tarballs will either contain 7.8.0 or 7.7 with a backport of an error message patch GlideinWMS: Patch release candidate in progress; team has started using the new linting tool \"Ruff\" Includes setting Apptainer variables so its image cache is not written to home directories Improves provisioning of Glideins by letting Frontend admins configure thresholds for determining whether EPs are full","title":"Discussion"},{"location":"meetings/2024/TechArea20240507/#support-update","text":"HPC4L (Mat, BrianL, Jaime Frey): Various CE issues have been resolved and jobs are running","title":"Support Update"},{"location":"meetings/2024/TechArea20240507/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240507/#osg-release-team","text":"Ready for Testing XRootD 5.6.9 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240507/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240514/","text":"OSG Technology Area Meeting, 14 May 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: TimT 7 (+2) open FreshDesk tickets 0 (-1) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 225 -5 Open 22 +1 Selected for Dev 22 +0 In Progress 14 -1 Dev Complete 0 -2 Ready for Testing 3 +3 Ready for Release OSG Software Team \u00b6 Kubernetes Hackathon this afternoon - AI (Matt): set up Traefik in OSDF and UWDF namespaces - AI (Mat): upgrade OSDF central services based on William's document - AI (BrianL): clean up services on River - AI (BrianL?): GlideIn Factory namespace stuff Yum Repo (Mat): - Built pungi container - Next step: Write pungi config for koji integration Kubernetes Gratia Probe (Matt): - Working with Ryan (kapel author) to clean up PR - Running into issues getting Gratia to run in non-privileged mode - Build pipeline: Issues with where to put the code/build pipeline, might just defer to manual build - Desired state: We want to be able to build from the Kapel repo via some config in the osg helm charts repo - Look at potential updates that Eduardo may have made (github: edubach) - May deadline OSG Notify (Mat/Matt): - Facilitators need to send out weekly reminders for campus meetup - OSG Notify is not an ideal tool to do this given need to configure certificates - We can dump an email list via OSG Notify and put it in a more user friendly service Discussion \u00b6 None this week Support Update \u00b6 AI (Matt W): Need to redeploy OSG Repo with the new mirrorlist after River DNS failure last week - Redeploy to tempest instead of river AI (Mat): Nick at NERSC having issues with 3rd party copy - Was able to resolve DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing Ready for Release Nothing yet Plan to release Condor 23.7.2 this week Issues with Apptainer in GlideinWMS Other miscellaneous issues: AI (Matt): Issues with syncing condor testing repo to osg-upcoming-testing repos Builds are failing for software-base in Alma 8/9 Discussion \u00b6 OSG-24: Need a Yubikey multiplexing solution AI (Matt): Confirm that gpg-agent can handle two yubikeys on Linux AI (Mat): Confirm that two yubikeys work on Windows","title":"May 14, 2024"},{"location":"meetings/2024/TechArea20240514/#osg-technology-area-meeting-14-may-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 14 May 2024"},{"location":"meetings/2024/TechArea20240514/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240514/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: TimT 7 (+2) open FreshDesk tickets 0 (-1) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240514/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 225 -5 Open 22 +1 Selected for Dev 22 +0 In Progress 14 -1 Dev Complete 0 -2 Ready for Testing 3 +3 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240514/#osg-software-team","text":"Kubernetes Hackathon this afternoon - AI (Matt): set up Traefik in OSDF and UWDF namespaces - AI (Mat): upgrade OSDF central services based on William's document - AI (BrianL): clean up services on River - AI (BrianL?): GlideIn Factory namespace stuff Yum Repo (Mat): - Built pungi container - Next step: Write pungi config for koji integration Kubernetes Gratia Probe (Matt): - Working with Ryan (kapel author) to clean up PR - Running into issues getting Gratia to run in non-privileged mode - Build pipeline: Issues with where to put the code/build pipeline, might just defer to manual build - Desired state: We want to be able to build from the Kapel repo via some config in the osg helm charts repo - Look at potential updates that Eduardo may have made (github: edubach) - May deadline OSG Notify (Mat/Matt): - Facilitators need to send out weekly reminders for campus meetup - OSG Notify is not an ideal tool to do this given need to configure certificates - We can dump an email list via OSG Notify and put it in a more user friendly service","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240514/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240514/#support-update","text":"AI (Matt W): Need to redeploy OSG Repo with the new mirrorlist after River DNS failure last week - Redeploy to tempest instead of river AI (Mat): Nick at NERSC having issues with 3rd party copy - Was able to resolve","title":"Support Update"},{"location":"meetings/2024/TechArea20240514/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240514/#osg-release-team","text":"Ready for Testing Nothing Ready for Release Nothing yet Plan to release Condor 23.7.2 this week Issues with Apptainer in GlideinWMS Other miscellaneous issues: AI (Matt): Issues with syncing condor testing repo to osg-upcoming-testing repos Builds are failing for software-base in Alma 8/9","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240514/#discussion_1","text":"OSG-24: Need a Yubikey multiplexing solution AI (Matt): Confirm that gpg-agent can handle two yubikeys on Linux AI (Mat): Confirm that two yubikeys work on Windows","title":"Discussion"},{"location":"meetings/2024/TechArea20240521/","text":"OSG Technology Area Meeting, 21 May 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, Mat, Matt, TimT Announcements \u00b6 Memorial day next Monday Matt OOO May 24 afternoon, full day May 31 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: TimT Next week: Mat 5 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 225 -5 Open 22 +1 Selected for Dev 22 +0 In Progress 14 -1 Dev Complete 0 -2 Ready for Testing 3 +3 Ready for Release OSG Software Team \u00b6 Release: - XRootD fixes for Pelican Yum Repo (Mat): - Next step: Write pungi config for koji integration Kubernetes Gratia Probe (Matt): - Let's get this ready in some installable form for SWT2 this week Discussion \u00b6 Code freeze for HTCondor 23.8 is this week; TimT to discuss tarball update policy with factory ops team. A new version of Pelican is not necessary since there have been no client/plugin changes released since Pelican 7.8.2 HTCondor-CE 23.8 will be put into osg-23-upcoming soon; this version disables support for legacy job route syntax, which is a breaking change. AI (TimT): Send a heads up with information about this, as well as how to use the provided migration script GlideinWMS 3.10.7rc1 is being tested; no problems seen yet, so the final release is expected shortly. Support Update \u00b6 University of Chicago (BrianL): AP22 origin did not work due to mismatched signing key on disk vs HTCondor configuration. Nothing in XRootD origin logs, look key IDs Topology (BrianL): Unknown projects in CE dashboard. Combination of unregistered projects in Topology (hence needing a quick turnaround time on them), misconfigured APs, and missing ProjectName attribute in Glideins DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing Ready for Release Nothing yet Plan to release Condor 23.7.2 this week Issues with Apptainer in GlideinWMS Other miscellaneous issues: AI (Matt): Issues with syncing condor testing repo to osg-upcoming-testing repos Builds are failing for software-base in Alma 8/9 Discussion \u00b6 OSG-24: Need a Yubikey multiplexing solution AI (Matt): Confirm that gpg-agent can handle two yubikeys on Linux AI (Mat): Confirm that two yubikeys work on Windows","title":"May 21, 2024"},{"location":"meetings/2024/TechArea20240521/#osg-technology-area-meeting-21-may-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, Mat, Matt, TimT","title":"OSG Technology Area Meeting, 21 May 2024"},{"location":"meetings/2024/TechArea20240521/#announcements","text":"Memorial day next Monday Matt OOO May 24 afternoon, full day May 31","title":"Announcements"},{"location":"meetings/2024/TechArea20240521/#triage-duty","text":"Triage duty shifts Tue-Mon This week: TimT Next week: Mat 5 (-2) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240521/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 225 -5 Open 22 +1 Selected for Dev 22 +0 In Progress 14 -1 Dev Complete 0 -2 Ready for Testing 3 +3 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240521/#osg-software-team","text":"Release: - XRootD fixes for Pelican Yum Repo (Mat): - Next step: Write pungi config for koji integration Kubernetes Gratia Probe (Matt): - Let's get this ready in some installable form for SWT2 this week","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240521/#discussion","text":"Code freeze for HTCondor 23.8 is this week; TimT to discuss tarball update policy with factory ops team. A new version of Pelican is not necessary since there have been no client/plugin changes released since Pelican 7.8.2 HTCondor-CE 23.8 will be put into osg-23-upcoming soon; this version disables support for legacy job route syntax, which is a breaking change. AI (TimT): Send a heads up with information about this, as well as how to use the provided migration script GlideinWMS 3.10.7rc1 is being tested; no problems seen yet, so the final release is expected shortly.","title":"Discussion"},{"location":"meetings/2024/TechArea20240521/#support-update","text":"University of Chicago (BrianL): AP22 origin did not work due to mismatched signing key on disk vs HTCondor configuration. Nothing in XRootD origin logs, look key IDs Topology (BrianL): Unknown projects in CE dashboard. Combination of unregistered projects in Topology (hence needing a quick turnaround time on them), misconfigured APs, and missing ProjectName attribute in Glideins","title":"Support Update"},{"location":"meetings/2024/TechArea20240521/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240521/#osg-release-team","text":"Ready for Testing Nothing Ready for Release Nothing yet Plan to release Condor 23.7.2 this week Issues with Apptainer in GlideinWMS Other miscellaneous issues: AI (Matt): Issues with syncing condor testing repo to osg-upcoming-testing repos Builds are failing for software-base in Alma 8/9","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240521/#discussion_1","text":"OSG-24: Need a Yubikey multiplexing solution AI (Matt): Confirm that gpg-agent can handle two yubikeys on Linux AI (Mat): Confirm that two yubikeys work on Windows","title":"Discussion"},{"location":"meetings/2024/TechArea20240528/","text":"OSG Technology Area Meeting, 21 May 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 BrianL at ATLAS S&C next week Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Matt 7 (+2) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 212 -1 Open 20 -2 Selected for Dev 28 +7 In Progress 15 +1 Dev Complete 0 +0 Ready for Testing 0 -1 Ready for Release OSG Software Team \u00b6 Kubernetes Hackathon: - AI (Mat): repo EL9 replacement container work - AI (Mat): Hermit Crab Glidein work - tokens.tgz is empty in the job - AI (Matt): any remaining kuantifier work - Changes to helm chart are merged in - Some concerns from Ryan wrt how the new prometheus metrics are being utilized - Lincoln might also be able to get a test environment going to test the chart in - Find services that could be ingresses - Services that only serve http(s) are eating too many ipv4 addresses Release: - AI (Matt): voms-server systemd regression Discussion \u00b6 None this week Support Update \u00b6 AGLT2 (BrianL): unpriv users could not query CM from AP. Dev team suggested that they add ANONYMOUS to SEC_READ_AUTHENTICATION_METHODS on the CM and SEC_CLIENT_AUTHENTICATION_METHODS on the AP. FNAL (BrianL): voms-server systemd regression -- need to work with them to transition to IAM or similar solution Would prefer dropping voms-server from osg-24 software stack, rely on epel Project is not sufficiently supported upstream Need to figure out what to do with OSG patches DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing Ready for Release Nothing yet Plan to release Condor 23.8 2024-06-20 Other miscellaneous issues: AI (TimT): Investigate ARM minicondor file transfer issues For next week (Mat): xrootd-5.6.9-1.6+ Discussion \u00b6 None this week","title":"May 28, 2024"},{"location":"meetings/2024/TechArea20240528/#osg-technology-area-meeting-21-may-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 21 May 2024"},{"location":"meetings/2024/TechArea20240528/#announcements","text":"BrianL at ATLAS S&C next week","title":"Announcements"},{"location":"meetings/2024/TechArea20240528/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Matt 7 (+2) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240528/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 212 -1 Open 20 -2 Selected for Dev 28 +7 In Progress 15 +1 Dev Complete 0 +0 Ready for Testing 0 -1 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240528/#osg-software-team","text":"Kubernetes Hackathon: - AI (Mat): repo EL9 replacement container work - AI (Mat): Hermit Crab Glidein work - tokens.tgz is empty in the job - AI (Matt): any remaining kuantifier work - Changes to helm chart are merged in - Some concerns from Ryan wrt how the new prometheus metrics are being utilized - Lincoln might also be able to get a test environment going to test the chart in - Find services that could be ingresses - Services that only serve http(s) are eating too many ipv4 addresses Release: - AI (Matt): voms-server systemd regression","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240528/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240528/#support-update","text":"AGLT2 (BrianL): unpriv users could not query CM from AP. Dev team suggested that they add ANONYMOUS to SEC_READ_AUTHENTICATION_METHODS on the CM and SEC_CLIENT_AUTHENTICATION_METHODS on the AP. FNAL (BrianL): voms-server systemd regression -- need to work with them to transition to IAM or similar solution Would prefer dropping voms-server from osg-24 software stack, rely on epel Project is not sufficiently supported upstream Need to figure out what to do with OSG patches","title":"Support Update"},{"location":"meetings/2024/TechArea20240528/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240528/#osg-release-team","text":"Ready for Testing Nothing Ready for Release Nothing yet Plan to release Condor 23.8 2024-06-20 Other miscellaneous issues: AI (TimT): Investigate ARM minicondor file transfer issues For next week (Mat): xrootd-5.6.9-1.6+","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240528/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240604/","text":"OSG Technology Area Meeting, 4 June 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Matt, Tim Announcements \u00b6 BrianL at ATLAS S&C this week Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: TimT 10 (+3) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 217 +5 Open 20 +0 Selected for Dev 23 -5 In Progress 15 +0 Dev Complete 5 +5 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 AI (Mat): repo EL9 replacement container work AI (Mat): Hermit Crab Glidein work AI (Mat): update OSG-Build documentation AI (Matt): new kuantifier requests from Armen: Some config values should be split out and more expliticly documented Support for NodeSelector in the Helm chart AI (Matt): investigate tag2distrepo Discussion \u00b6 None this week Support Update \u00b6 JLab (Mat): user having strange permission issue with Vault support; Mat asking Jason from the HTCondor dev team for help SDSC (Mat): user having trouble with the OSDF director and resorting to pulling from caches directly. The issue seems to be with the origin; Mat will ask Fabio, who runs that origin, about its status JLab (Matt): their xrootd-multiuser issue is caused by xrootd-multiuser refusing to switch to a group with a GID less than 500 (probably for security reasons); Derek is discussing solutions with the admin DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing xrootd-5.6.9-1.6 Ready for Release Nothing yet Discussion \u00b6 None this week","title":"OSG Technology Area Meeting, 4 June 2024"},{"location":"meetings/2024/TechArea20240604/#osg-technology-area-meeting-4-june-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Matt, Tim","title":"OSG Technology Area Meeting, 4 June 2024"},{"location":"meetings/2024/TechArea20240604/#announcements","text":"BrianL at ATLAS S&C this week","title":"Announcements"},{"location":"meetings/2024/TechArea20240604/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: TimT 10 (+3) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240604/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 217 +5 Open 20 +0 Selected for Dev 23 -5 In Progress 15 +0 Dev Complete 5 +5 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240604/#osg-software-team","text":"AI (Mat): repo EL9 replacement container work AI (Mat): Hermit Crab Glidein work AI (Mat): update OSG-Build documentation AI (Matt): new kuantifier requests from Armen: Some config values should be split out and more expliticly documented Support for NodeSelector in the Helm chart AI (Matt): investigate tag2distrepo","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240604/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240604/#support-update","text":"JLab (Mat): user having strange permission issue with Vault support; Mat asking Jason from the HTCondor dev team for help SDSC (Mat): user having trouble with the OSDF director and resorting to pulling from caches directly. The issue seems to be with the origin; Mat will ask Fabio, who runs that origin, about its status JLab (Matt): their xrootd-multiuser issue is caused by xrootd-multiuser refusing to switch to a group with a GID less than 500 (probably for security reasons); Derek is discussing solutions with the admin","title":"Support Update"},{"location":"meetings/2024/TechArea20240604/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240604/#osg-release-team","text":"Ready for Testing xrootd-5.6.9-1.6 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240604/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240611/","text":"OSG Technology Area Meeting, 11 June 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Marco, Mat, Matt, Tim Announcements \u00b6 BrianL OOO Thursday and Friday Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: TimT Next week: BrianL 9 (-1) open FreshDesk tickets 1 (+1) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 211 -6 Open 17 -3 Selected for Dev 29 +6 In Progress 16 +1 Dev Complete 6 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes Hackathon ideas: - Add CVMFS port redirection to cache base - Add multi-export support to Pelican origins - Remove topology-scitokens-fetcher from base cache and origin bases - Add osg-htc.org CNAMEs to production Yum repos - Add nodeSelector to the k8s gratia probe - Finish moving UW OSDF cache to a data center with a faster network connection BrianL and Jason will be late to the hackathon due to a regularly conflicting Fellows meeting This week, let's focus on tackling our support tickets: - GGUS ticket about HP4CL where they're asking about Active: False resources - Many tickets in overdue yesterday, lots of tickets in \"Customer responded\" state: Discussion \u00b6 Tim: HTCondor LTS (23.0.12) release this week; feature release (23.8.0) next week. The tarballs will contain Pelican 7.8. Tim will debug broken file transfer on ARM minicondor containers; GlideinWMS: 3.10.7rc2 is ready. Marco successfully built for 3.6 but the build is hanging for OSG 23-main; Mat noticed the Koji builders are not responding and will debug. Support Update \u00b6 SWT2 (BrianL): had some back and forth with Armen around installing the k8s gratia probe. Passed along feedback to Ryan and Matt around config, started registering the resource in Topology. Should we create a separate service or is Execution Endpoint sufficient? BrianL leans towards the latter. Mat leans towards new service. Mat: User reported errors accessing his data through the director and switched to trying specific caches by hand. The origin turned out to be broken; those caches were working because they had already cached his data. Fabio fixed the origin and migrated it to Pelican. DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing xrootd-5.6.9-1.6 Ready for Release Nothing yet Discussion \u00b6 None this week","title":"June 11, 2024"},{"location":"meetings/2024/TechArea20240611/#osg-technology-area-meeting-11-june-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Marco, Mat, Matt, Tim","title":"OSG Technology Area Meeting, 11 June 2024"},{"location":"meetings/2024/TechArea20240611/#announcements","text":"BrianL OOO Thursday and Friday","title":"Announcements"},{"location":"meetings/2024/TechArea20240611/#triage-duty","text":"Triage duty shifts Tue-Mon This week: TimT Next week: BrianL 9 (-1) open FreshDesk tickets 1 (+1) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240611/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 211 -6 Open 17 -3 Selected for Dev 29 +6 In Progress 16 +1 Dev Complete 6 +1 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240611/#osg-software-team","text":"Kubernetes Hackathon ideas: - Add CVMFS port redirection to cache base - Add multi-export support to Pelican origins - Remove topology-scitokens-fetcher from base cache and origin bases - Add osg-htc.org CNAMEs to production Yum repos - Add nodeSelector to the k8s gratia probe - Finish moving UW OSDF cache to a data center with a faster network connection BrianL and Jason will be late to the hackathon due to a regularly conflicting Fellows meeting This week, let's focus on tackling our support tickets: - GGUS ticket about HP4CL where they're asking about Active: False resources - Many tickets in overdue yesterday, lots of tickets in \"Customer responded\" state:","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240611/#discussion","text":"Tim: HTCondor LTS (23.0.12) release this week; feature release (23.8.0) next week. The tarballs will contain Pelican 7.8. Tim will debug broken file transfer on ARM minicondor containers; GlideinWMS: 3.10.7rc2 is ready. Marco successfully built for 3.6 but the build is hanging for OSG 23-main; Mat noticed the Koji builders are not responding and will debug.","title":"Discussion"},{"location":"meetings/2024/TechArea20240611/#support-update","text":"SWT2 (BrianL): had some back and forth with Armen around installing the k8s gratia probe. Passed along feedback to Ryan and Matt around config, started registering the resource in Topology. Should we create a separate service or is Execution Endpoint sufficient? BrianL leans towards the latter. Mat leans towards new service. Mat: User reported errors accessing his data through the director and switched to trying specific caches by hand. The origin turned out to be broken; those caches were working because they had already cached his data. Fabio fixed the origin and migrated it to Pelican.","title":"Support Update"},{"location":"meetings/2024/TechArea20240611/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240611/#osg-release-team","text":"Ready for Testing xrootd-5.6.9-1.6 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240611/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240618/","text":"OSG Technology Area Meeting, 11 June 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: ??? 7 (-2) open FreshDesk tickets 1 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 211 -6 Open 17 -3 Selected for Dev 29 +6 In Progress 16 +1 Dev Complete 6 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Neha (Jason's fellow) needs some help setting up an OSPool glidein script \"dev environment\" Run against ITB, land in CHTC Suggestion: Create new frontend group targetting ITB svn-to-git -> OSG 23 Still on EL7 currently Already being built in OSG 23, give it a try Check for any remaining OSG services on EL7 on Tiger AI (MattW): Run script that grabs OS for containers AI (Mat): verify Yubikey functionality on Windows Discussion \u00b6 KApel: Need to refactor apel- and gratia- specific env vars into separate values.yaml fields Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing yet Ready for Release Nothing yet Discussion \u00b6 None this week","title":"June 18, 2024"},{"location":"meetings/2024/TechArea20240618/#osg-technology-area-meeting-11-june-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 11 June 2024"},{"location":"meetings/2024/TechArea20240618/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240618/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: ??? 7 (-2) open FreshDesk tickets 1 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240618/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 211 -6 Open 17 -3 Selected for Dev 29 +6 In Progress 16 +1 Dev Complete 6 +1 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240618/#osg-software-team","text":"Neha (Jason's fellow) needs some help setting up an OSPool glidein script \"dev environment\" Run against ITB, land in CHTC Suggestion: Create new frontend group targetting ITB svn-to-git -> OSG 23 Still on EL7 currently Already being built in OSG 23, give it a try Check for any remaining OSG services on EL7 on Tiger AI (MattW): Run script that grabs OS for containers AI (Mat): verify Yubikey functionality on Windows","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240618/#discussion","text":"KApel: Need to refactor apel- and gratia- specific env vars into separate values.yaml fields","title":"Discussion"},{"location":"meetings/2024/TechArea20240618/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240618/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240618/#osg-release-team","text":"Ready for Testing Nothing yet Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240618/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240625/","text":"OSG Technology Area Meeting, 25 June 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian, Derek, Marco, Mat, Matt, Tim Announcements \u00b6 HTC24 in two weeks Derek out next week; will attend HTC24 Mon-Wed Marco will not be able to attend HTC24; Bruno will represent the GlideinWMS team, along with two students Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: TimT Next week: Mat 10 (+3) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 215 +3 Open 16 -2 Selected for Dev 30 +3 In Progress 16 +0 Dev Complete 4 +3 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes Hackathon - Remaining repo work - Convert prod services in the osg namespace to EL9 - Check if there are any EL7 containers on Tempest - Turn off osgdev services on EL7 Discussion \u00b6 The \"contact-repo\" app is on OSG 3.6; if it needs updating, contact Nebraska folks about it. \"lightweight-issuer\" is in the osgdev namespace but being used in production; it needs to be upgraded to EL9 and moved to the osg namespace. Derek will investigate. GlideinWMS 3.10.7 has been released and built into osg-development Support Update \u00b6 BrianL: PATh Facility at UNL went down to expired Kubernetes certs. Our CoreDNS app uses an image that is no longer available. BrianL: Assisting Huijun at UNL with HTCondor 23 upgrade; condor_status was failing due to the default security settings but a workaround was available. Derek: Ticket regarding xrootd assistance was assigned to Operations but should be rerouted to Collaboration Support. DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing yet Ready for Release Nothing yet Discussion \u00b6 None this week","title":"June 25, 2024"},{"location":"meetings/2024/TechArea20240625/#osg-technology-area-meeting-25-june-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian, Derek, Marco, Mat, Matt, Tim","title":"OSG Technology Area Meeting, 25 June 2024"},{"location":"meetings/2024/TechArea20240625/#announcements","text":"HTC24 in two weeks Derek out next week; will attend HTC24 Mon-Wed Marco will not be able to attend HTC24; Bruno will represent the GlideinWMS team, along with two students","title":"Announcements"},{"location":"meetings/2024/TechArea20240625/#triage-duty","text":"Triage duty shifts Tue-Mon This week: TimT Next week: Mat 10 (+3) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240625/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 215 +3 Open 16 -2 Selected for Dev 30 +3 In Progress 16 +0 Dev Complete 4 +3 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240625/#osg-software-team","text":"Kubernetes Hackathon - Remaining repo work - Convert prod services in the osg namespace to EL9 - Check if there are any EL7 containers on Tempest - Turn off osgdev services on EL7","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240625/#discussion","text":"The \"contact-repo\" app is on OSG 3.6; if it needs updating, contact Nebraska folks about it. \"lightweight-issuer\" is in the osgdev namespace but being used in production; it needs to be upgraded to EL9 and moved to the osg namespace. Derek will investigate. GlideinWMS 3.10.7 has been released and built into osg-development","title":"Discussion"},{"location":"meetings/2024/TechArea20240625/#support-update","text":"BrianL: PATh Facility at UNL went down to expired Kubernetes certs. Our CoreDNS app uses an image that is no longer available. BrianL: Assisting Huijun at UNL with HTCondor 23 upgrade; condor_status was failing due to the default security settings but a workaround was available. Derek: Ticket regarding xrootd assistance was assigned to Operations but should be rerouted to Collaboration Support.","title":"Support Update"},{"location":"meetings/2024/TechArea20240625/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240625/#osg-release-team","text":"Ready for Testing Nothing yet Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240625/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240702/","text":"OSG Technology Area Meeting, 2 July 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Matt, Tim Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Matt 11 (+1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 217 +2 Open 16 +0 Selected for Dev 29 -1 In Progress 16 +0 Dev Complete 1 -3 Ready for Testing 6 +6 Ready for Release OSG Software Team \u00b6 AI (Mat): Build xrootd 5.7.0 AI (Mat): IGTF CA certs release 130 AI (Matt): Disable broken EL7 image builds Discussion \u00b6 None this week Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing Nothing yet Ready for Release Nothing yet Discussion \u00b6 IGTF CA certs will be released next week due to the holiday","title":"July 2, 2024"},{"location":"meetings/2024/TechArea20240702/#osg-technology-area-meeting-2-july-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Matt, Tim","title":"OSG Technology Area Meeting, 2 July 2024"},{"location":"meetings/2024/TechArea20240702/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240702/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Matt 11 (+1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240702/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 217 +2 Open 16 +0 Selected for Dev 29 -1 In Progress 16 +0 Dev Complete 1 -3 Ready for Testing 6 +6 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240702/#osg-software-team","text":"AI (Mat): Build xrootd 5.7.0 AI (Mat): IGTF CA certs release 130 AI (Matt): Disable broken EL7 image builds","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240702/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240702/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240702/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240702/#osg-release-team","text":"Ready for Testing Nothing yet Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240702/#discussion_1","text":"IGTF CA certs will be released next week due to the holiday","title":"Discussion"},{"location":"meetings/2024/TechArea20240716/","text":"OSG Technology Area Meeting, 16 July 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: MattW, TimT, MatS, BrianL, Marco Mambelli Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: Mat 5 (-6) open FreshDesk tickets 1 (+1) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 215 +0 Open 15 +0 Selected for Dev 31 -1 In Progress 16 +0 Dev Complete 4 +2 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 AI (Mat): repo upgrade status? AI (Matt): k8s Gratia probe (helm chart GHA, nodeSelector/tolerations) GHA should live in osg-htc repo Add rptaylor/kapel to osg's repo backup script Backup script currently just lives on AFS Document process for discovering your cluster's Prometheus server AI (BrianL): mount AFS on osg-sw-submit for external collaborators USB A vs C preference for USB hub? - Mat: A, Matt: either, Tim: A GlideinWMS Software: - Will need continued access to osg 3.6 repos for now - Tests for release 3.11 run into issues when updated to OSG 23 Discussion \u00b6 None this week Support Update \u00b6 AI (Mat): Duncan from IGWN has issues running pilots Issues with base container, missing wget DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing IGTF 1.130 XRootD 5.7.0 osdf-server 7.9.3 Ready for Release Nothing yet Discussion \u00b6 IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"July 16, 2024"},{"location":"meetings/2024/TechArea20240716/#osg-technology-area-meeting-16-july-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: MattW, TimT, MatS, BrianL, Marco Mambelli","title":"OSG Technology Area Meeting, 16 July 2024"},{"location":"meetings/2024/TechArea20240716/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240716/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: Mat 5 (-6) open FreshDesk tickets 1 (+1) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240716/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 215 +0 Open 15 +0 Selected for Dev 31 -1 In Progress 16 +0 Dev Complete 4 +2 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240716/#osg-software-team","text":"AI (Mat): repo upgrade status? AI (Matt): k8s Gratia probe (helm chart GHA, nodeSelector/tolerations) GHA should live in osg-htc repo Add rptaylor/kapel to osg's repo backup script Backup script currently just lives on AFS Document process for discovering your cluster's Prometheus server AI (BrianL): mount AFS on osg-sw-submit for external collaborators USB A vs C preference for USB hub? - Mat: A, Matt: either, Tim: A GlideinWMS Software: - Will need continued access to osg 3.6 repos for now - Tests for release 3.11 run into issues when updated to OSG 23","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240716/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240716/#support-update","text":"AI (Mat): Duncan from IGWN has issues running pilots Issues with base container, missing wget","title":"Support Update"},{"location":"meetings/2024/TechArea20240716/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240716/#osg-release-team","text":"Ready for Testing IGTF 1.130 XRootD 5.7.0 osdf-server 7.9.3 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240716/#discussion_1","text":"IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"Discussion"},{"location":"meetings/2024/TechArea20240723/","text":"OSG Technology Area Meeting, 23 July 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Matt, TimT Announcements \u00b6 BrianL OOO Thu / Fri Mat / Matt / TimT out Fri Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 3 (-2) open FreshDesk tickets 0 (-1) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 215 +0 Open 16 +1 Selected for Dev 30 -1 In Progress 16 +0 Dev Complete 4 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 AI (TimT, Mat): Migrate various CHTC cron jobs (some of which are used for OSG Software) from moria to a new host AI (BrianL): Debug not being able to get a Let's Encrypt cert with a chtc.wisc.edu domain AI (Mat): Set up ARM Koji builder AI (BrianL): Purchase YubiKeys and USB hubs; USB hubs should have at least 4 ports Kubernetes Hackathon - AI (BrianL): deploy ResearchDrive UWDF prod origin - AI (Matt): macrostrat work, remaining kuantifier items - AI (Mat): repo rsync source IP logging Discussion \u00b6 OSG VMU integration tests using the Pelican version of stashcp have been failing since they relied on a Pelican backward compatibility feature that does not seem to be working in current versions. Mat to start a discussion with the Pelican team to determine the best way forward. Patch Tuesday: new versions of HTCondor and HTCondor-CE ready for OSPool deployment Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing AI (Matt): GlideinWMS 3.10.7 htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release Nothing yet Discussion \u00b6 IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"July 23, 2024"},{"location":"meetings/2024/TechArea20240723/#osg-technology-area-meeting-23-july-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Matt, TimT","title":"OSG Technology Area Meeting, 23 July 2024"},{"location":"meetings/2024/TechArea20240723/#announcements","text":"BrianL OOO Thu / Fri Mat / Matt / TimT out Fri","title":"Announcements"},{"location":"meetings/2024/TechArea20240723/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 3 (-2) open FreshDesk tickets 0 (-1) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240723/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 215 +0 Open 16 +1 Selected for Dev 30 -1 In Progress 16 +0 Dev Complete 4 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240723/#osg-software-team","text":"AI (TimT, Mat): Migrate various CHTC cron jobs (some of which are used for OSG Software) from moria to a new host AI (BrianL): Debug not being able to get a Let's Encrypt cert with a chtc.wisc.edu domain AI (Mat): Set up ARM Koji builder AI (BrianL): Purchase YubiKeys and USB hubs; USB hubs should have at least 4 ports Kubernetes Hackathon - AI (BrianL): deploy ResearchDrive UWDF prod origin - AI (Matt): macrostrat work, remaining kuantifier items - AI (Mat): repo rsync source IP logging","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240723/#discussion","text":"OSG VMU integration tests using the Pelican version of stashcp have been failing since they relied on a Pelican backward compatibility feature that does not seem to be working in current versions. Mat to start a discussion with the Pelican team to determine the best way forward. Patch Tuesday: new versions of HTCondor and HTCondor-CE ready for OSPool deployment","title":"Discussion"},{"location":"meetings/2024/TechArea20240723/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20240723/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240723/#osg-release-team","text":"Ready for Testing AI (Matt): GlideinWMS 3.10.7 htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240723/#discussion_1","text":"IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"Discussion"},{"location":"meetings/2024/TechArea20240730/","text":"OSG Technology Area Meeting, 23 July 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Tim out Thu/Fri Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: Matt 4 (+1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 218 +3 Open 16 +1 Selected for Dev 29 -1 In Progress 16 +0 Dev Complete 1 -3 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 What's the process for adding patching XRootD in Pelican containers? AI (Matt): Kuantifier Deliverable due at the end of August Status of NET2 and Prometheus auth? Derive ProbeName from SUBMIT_HOST nodeSelector + toleration support AI (Matt): Ping collaborators GHA from osg helm-charts repo AI (TimT, Mat): Migrate various CHTC cron jobs (some of which are used for OSG Software) from moria to a new host One job has been failing since 2022, reports failures to condor-util but the mailbox is full AI (Mat): Set up ARM Koji builder Need to updated systemd unit file, then create production builder VM AI (Mat): EL9 repo AI (BrianL): Purchase YubiKeys and USB hubs; USB hubs should have at least 4 ports Discussion \u00b6 None this week Support Update \u00b6 GlideinWMS: Newly occurring issues with trust domain config Receive config warning suggesting value be quoted RT: Question from AWS Developer about installing HTCondor via annex DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release GlideinWMS 3.10.7 Discussion \u00b6 IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"July 30, 2024"},{"location":"meetings/2024/TechArea20240730/#osg-technology-area-meeting-23-july-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 23 July 2024"},{"location":"meetings/2024/TechArea20240730/#announcements","text":"Tim out Thu/Fri","title":"Announcements"},{"location":"meetings/2024/TechArea20240730/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: Matt 4 (+1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240730/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 218 +3 Open 16 +1 Selected for Dev 29 -1 In Progress 16 +0 Dev Complete 1 -3 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240730/#osg-software-team","text":"What's the process for adding patching XRootD in Pelican containers? AI (Matt): Kuantifier Deliverable due at the end of August Status of NET2 and Prometheus auth? Derive ProbeName from SUBMIT_HOST nodeSelector + toleration support AI (Matt): Ping collaborators GHA from osg helm-charts repo AI (TimT, Mat): Migrate various CHTC cron jobs (some of which are used for OSG Software) from moria to a new host One job has been failing since 2022, reports failures to condor-util but the mailbox is full AI (Mat): Set up ARM Koji builder Need to updated systemd unit file, then create production builder VM AI (Mat): EL9 repo AI (BrianL): Purchase YubiKeys and USB hubs; USB hubs should have at least 4 ports","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240730/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240730/#support-update","text":"GlideinWMS: Newly occurring issues with trust domain config Receive config warning suggesting value be quoted RT: Question from AWS Developer about installing HTCondor via annex","title":"Support Update"},{"location":"meetings/2024/TechArea20240730/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240730/#osg-release-team","text":"Ready for Testing htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release GlideinWMS 3.10.7","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240730/#discussion_1","text":"IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"Discussion"},{"location":"meetings/2024/TechArea20240806/","text":"OSG Technology Area Meeting, 6 August 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: TimT 3 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 220 +2 Open 17 +1 Selected for Dev 32 +3 In Progress 16 +0 Dev Complete 1 -3 Ready for Testing 1 +1 Ready for Release OSG Software Team \u00b6 What's the process for adding patching XRootD in Pelican containers? AI (Matt): Kuantifier Status of NET2 and Prometheus auth? Follow up again re: prometheus API access Derive ProbeName from SUBMIT_HOST nodeSelector + toleration support GHA from osg helm-charts repo Require that pods set a CPU resource request We use this to derive the \"Processors\" field AI (Mat): Set up ARM Koji builder Need to updated systemd unit file, then create production builder VM AI (Mat): EL9 repo Contribute VOMS patches upstream? Want to drop from OSG software Topology \"StashCache\" GHA has been failing for some time Validates topology cache list against cvmfs New Yubikeys are in, BrianL working on purchasing hubs Discussion \u00b6 None this week Support Update \u00b6 University of Tennessee at Chattanooga (BrianL): NVIDIA MPS server (provides a CUDA interface) seems to be causing glideins to hang upon condor_gpu_discovery . NVIDIA recommends against using MPS across multiple jobs (but may be useful for a single user). Add check to see if it's enabled? Show up as a process under ps Probably not exposed to containers JLab (Mat): Support in setting up Pelican Origin, setting up weekly meeting DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release GlideinWMS 3.10.7 Discussion \u00b6 IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"August 6, 2024"},{"location":"meetings/2024/TechArea20240806/#osg-technology-area-meeting-6-august-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 6 August 2024"},{"location":"meetings/2024/TechArea20240806/#announcements","text":"","title":"Announcements"},{"location":"meetings/2024/TechArea20240806/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: TimT 3 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240806/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 220 +2 Open 17 +1 Selected for Dev 32 +3 In Progress 16 +0 Dev Complete 1 -3 Ready for Testing 1 +1 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240806/#osg-software-team","text":"What's the process for adding patching XRootD in Pelican containers? AI (Matt): Kuantifier Status of NET2 and Prometheus auth? Follow up again re: prometheus API access Derive ProbeName from SUBMIT_HOST nodeSelector + toleration support GHA from osg helm-charts repo Require that pods set a CPU resource request We use this to derive the \"Processors\" field AI (Mat): Set up ARM Koji builder Need to updated systemd unit file, then create production builder VM AI (Mat): EL9 repo Contribute VOMS patches upstream? Want to drop from OSG software Topology \"StashCache\" GHA has been failing for some time Validates topology cache list against cvmfs New Yubikeys are in, BrianL working on purchasing hubs","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240806/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240806/#support-update","text":"University of Tennessee at Chattanooga (BrianL): NVIDIA MPS server (provides a CUDA interface) seems to be causing glideins to hang upon condor_gpu_discovery . NVIDIA recommends against using MPS across multiple jobs (but may be useful for a single user). Add check to see if it's enabled? Show up as a process under ps Probably not exposed to containers JLab (Mat): Support in setting up Pelican Origin, setting up weekly meeting","title":"Support Update"},{"location":"meetings/2024/TechArea20240806/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240806/#osg-release-team","text":"Ready for Testing htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release GlideinWMS 3.10.7","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240806/#discussion_1","text":"IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"Discussion"},{"location":"meetings/2024/TechArea20240813/","text":"OSG Technology Area Meeting, 13 August 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Derek, Marco, Mat, Matt Announcements \u00b6 Doc focus this Friday PATh PI meeting for the next three days; Brian will be on call Tim T out part of this week and all of next week Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt (replacing TimT) Next week: ? 7 (+4) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 220 +0 Open 14 -3 Selected for Dev 35 +3 In Progress 16 +0 Dev Complete 4 +3 Ready for Testing 2 +1 Ready for Release OSG Software Team \u00b6 Doc focus this Friday AI (Matt): Kuantifier status: If Kubernetes Jobs don't have a resource request, then the processour count would show up as 0; failing loudly on such as misconfiguration is not really possible, but this can be added as a warning, along with notes in the Helm chart and an install guide. AI (Mat): ARM: Koji builders are ready; no mass rebuid because that would require bumping the package release numbers. Mat has been creating tickets to rebuild individual software as needed. Next step for ARM is to add integration testing -- we will need to make VM images for the VM Universe jobs. Nebraska has an ARM machine in a Kubernetes cluster; we may be able to make use of that. AI (Matt): EL9 repo AI (Mat): Contribute VOMS patches upstream AI (BrianL): Prepare tickets for this Friday's doc focus BrianL working on purchasing USB hubs for Yubikeys Discussion \u00b6 Marco continuing work on GlideinWMS development release; release canddiates are available. Fermilab is shutting down for the last week of August and first week of September. Derek reporting low GPU utilization for NRAO Glideins on NRP; Brian will show him how to log in to the PATh Facility AP so he can see the status of NRAO's jobs. Matt fixed the repo-rsync server, which was down due to a mismatch of VLANs between the Service and Pods. Support Update \u00b6 JLab (Mat): Support setting up their Pelican Origin JLab (Matt): Troubleshoot crashing OAuth credmon PATh Facility (Mat): missing GPUs PATH-UNL had nodes shut down due to overheating; PATH-Expanse had GPU pods (and CPU pods) failing to start -- at first we thought it was the return of a volume mounting issue we've seen before (see https://opensciencegrid.atlassian.net/issues/INF-1672); later we discovered that it was due to an outage at SDSC. DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release GlideinWMS 3.10.7 Discussion \u00b6 IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"August 13, 2024"},{"location":"meetings/2024/TechArea20240813/#osg-technology-area-meeting-13-august-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Derek, Marco, Mat, Matt","title":"OSG Technology Area Meeting, 13 August 2024"},{"location":"meetings/2024/TechArea20240813/#announcements","text":"Doc focus this Friday PATh PI meeting for the next three days; Brian will be on call Tim T out part of this week and all of next week","title":"Announcements"},{"location":"meetings/2024/TechArea20240813/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt (replacing TimT) Next week: ? 7 (+4) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240813/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 220 +0 Open 14 -3 Selected for Dev 35 +3 In Progress 16 +0 Dev Complete 4 +3 Ready for Testing 2 +1 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240813/#osg-software-team","text":"Doc focus this Friday AI (Matt): Kuantifier status: If Kubernetes Jobs don't have a resource request, then the processour count would show up as 0; failing loudly on such as misconfiguration is not really possible, but this can be added as a warning, along with notes in the Helm chart and an install guide. AI (Mat): ARM: Koji builders are ready; no mass rebuid because that would require bumping the package release numbers. Mat has been creating tickets to rebuild individual software as needed. Next step for ARM is to add integration testing -- we will need to make VM images for the VM Universe jobs. Nebraska has an ARM machine in a Kubernetes cluster; we may be able to make use of that. AI (Matt): EL9 repo AI (Mat): Contribute VOMS patches upstream AI (BrianL): Prepare tickets for this Friday's doc focus BrianL working on purchasing USB hubs for Yubikeys","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240813/#discussion","text":"Marco continuing work on GlideinWMS development release; release canddiates are available. Fermilab is shutting down for the last week of August and first week of September. Derek reporting low GPU utilization for NRAO Glideins on NRP; Brian will show him how to log in to the PATh Facility AP so he can see the status of NRAO's jobs. Matt fixed the repo-rsync server, which was down due to a mismatch of VLANs between the Service and Pods.","title":"Discussion"},{"location":"meetings/2024/TechArea20240813/#support-update","text":"JLab (Mat): Support setting up their Pelican Origin JLab (Matt): Troubleshoot crashing OAuth credmon PATh Facility (Mat): missing GPUs PATH-UNL had nodes shut down due to overheating; PATH-Expanse had GPU pods (and CPU pods) failing to start -- at first we thought it was the return of a volume mounting issue we've seen before (see https://opensciencegrid.atlassian.net/issues/INF-1672); later we discovered that it was due to an outage at SDSC.","title":"Support Update"},{"location":"meetings/2024/TechArea20240813/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240813/#osg-release-team","text":"Ready for Testing htcondor-ce osdf-server-7.9.3 xrootd-5.7.0 igtf-ca-certs Ready for Release GlideinWMS 3.10.7","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240813/#discussion_1","text":"IGTF CA certs will be released next week due to the holiday Full support periods for HTC23 and OSG23 are not fully aligned Support for OSG 23 lasts several months longer Marco has been using OSG noarch packages successfully in ARM","title":"Discussion"},{"location":"meetings/2024/TechArea20240820/","text":"OSG Technology Area Meeting, 20 August 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Tim T out this week Fermilab is closed next week Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: ? 7 (+4) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 220 +0 Open 14 -3 Selected for Dev 35 +3 In Progress 16 +0 Dev Complete 4 +3 Ready for Testing 2 +1 Ready for Release OSG Software Team \u00b6 AI (Matt): Kuantifier status Cut a release candidate, coordinate testing with UNL AI (Mat): Koji ARM work is complete K8s hackathon: NRP dev Flux integration Move OSG voms patches upstream, drop support for voms AI (Matt): EL9 repo K8s hackathon: Set up new IceCube namespace in the UNL PATh Facility OSG 24 yubikeys, usb and paperkey backups AI (Mat): Contribute VOMS patches upstream Marco: GlideinWMS ARM packaging Using noarch packages, works pretty well out of our x86_64 repos Should attempt to pull from aarch64 repos instead Discussion \u00b6 TimT currently running releases while on vacation, might need more robust back-up release process in place. Support Update \u00b6 AI (Matt): JLab CredMon issue Able to start CredMon daemon as root but not from the condor_master . Need to follow up with HTCSS dev team DevOps \u00b6 None this week OSG Release Team \u00b6 None this week Discussion \u00b6 None this week","title":"August 20, 2024"},{"location":"meetings/2024/TechArea20240820/#osg-technology-area-meeting-20-august-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 20 August 2024"},{"location":"meetings/2024/TechArea20240820/#announcements","text":"Tim T out this week Fermilab is closed next week","title":"Announcements"},{"location":"meetings/2024/TechArea20240820/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: ? 7 (+4) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240820/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 220 +0 Open 14 -3 Selected for Dev 35 +3 In Progress 16 +0 Dev Complete 4 +3 Ready for Testing 2 +1 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240820/#osg-software-team","text":"AI (Matt): Kuantifier status Cut a release candidate, coordinate testing with UNL AI (Mat): Koji ARM work is complete K8s hackathon: NRP dev Flux integration Move OSG voms patches upstream, drop support for voms AI (Matt): EL9 repo K8s hackathon: Set up new IceCube namespace in the UNL PATh Facility OSG 24 yubikeys, usb and paperkey backups AI (Mat): Contribute VOMS patches upstream Marco: GlideinWMS ARM packaging Using noarch packages, works pretty well out of our x86_64 repos Should attempt to pull from aarch64 repos instead","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240820/#discussion","text":"TimT currently running releases while on vacation, might need more robust back-up release process in place.","title":"Discussion"},{"location":"meetings/2024/TechArea20240820/#support-update","text":"AI (Matt): JLab CredMon issue Able to start CredMon daemon as root but not from the condor_master . Need to follow up with HTCSS dev team","title":"Support Update"},{"location":"meetings/2024/TechArea20240820/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240820/#osg-release-team","text":"None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240820/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240827/","text":"OSG Technology Area Meeting, 27 August 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Tim Announcements \u00b6 Fermilab is closed this week IRIS-HEP Retreat next week Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Matt 9 (+2) open FreshDesk tickets (as of Tuesday morning) 0 (+0) open GGUS tickets (as of Tuesday morning) Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 217 -3 Open 14 +0 Selected for Dev 31 -4 In Progress 17 +1 Dev Complete 5 +1 Ready for Testing 0 -2 Ready for Release OSG Software Team \u00b6 AI (Mat): Move OSG voms patches upstream, drop support for voms Transition EL9 repo work to Matt Assist HTCondor dev team on release run-through AI (Mat): Check output transfer transforms on the OSPool AI (Mat): Add VMU tests for ARM (build images) Verify test infrastructure works for multiple architectures Discussion \u00b6 Matt finished OSG 24 YubiKeys, USB and Paperkey backups HTCondor 24 scheduled for October; HTCondor 23.9.7 released last week with truncated hold messages and wrong hold reason codes fixed Support Update \u00b6 Oklahoma University (Mat): osg-xrootd-standalone does not start up on a new EL9 installation; not the usual SHA1 problem, so further debugging is needed DevOps \u00b6 None this week OSG Release Team \u00b6 None this week Discussion \u00b6 None this week","title":"August 27, 2024"},{"location":"meetings/2024/TechArea20240827/#osg-technology-area-meeting-27-august-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Mat, Tim","title":"OSG Technology Area Meeting, 27 August 2024"},{"location":"meetings/2024/TechArea20240827/#announcements","text":"Fermilab is closed this week IRIS-HEP Retreat next week","title":"Announcements"},{"location":"meetings/2024/TechArea20240827/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Matt 9 (+2) open FreshDesk tickets (as of Tuesday morning) 0 (+0) open GGUS tickets (as of Tuesday morning)","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240827/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 217 -3 Open 14 +0 Selected for Dev 31 -4 In Progress 17 +1 Dev Complete 5 +1 Ready for Testing 0 -2 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20240827/#osg-software-team","text":"AI (Mat): Move OSG voms patches upstream, drop support for voms Transition EL9 repo work to Matt Assist HTCondor dev team on release run-through AI (Mat): Check output transfer transforms on the OSPool AI (Mat): Add VMU tests for ARM (build images) Verify test infrastructure works for multiple architectures","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240827/#discussion","text":"Matt finished OSG 24 YubiKeys, USB and Paperkey backups HTCondor 24 scheduled for October; HTCondor 23.9.7 released last week with truncated hold messages and wrong hold reason codes fixed","title":"Discussion"},{"location":"meetings/2024/TechArea20240827/#support-update","text":"Oklahoma University (Mat): osg-xrootd-standalone does not start up on a new EL9 installation; not the usual SHA1 problem, so further debugging is needed","title":"Support Update"},{"location":"meetings/2024/TechArea20240827/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240827/#osg-release-team","text":"None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240827/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240903/","text":"OSG Technology Area Meeting, 3 September 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, Mat, Matt, TimT Announcements \u00b6 BrianB, BrianL, Derek, and Mat at IRIS-HEP Retreat this week Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: TimT 7 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) (TODO) \u00b6 # of tickets \u0394 State 214 -3 Open 14 +0 Selected for Dev 34 +3 In Progress 17 +0 Dev Complete 7 +2 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 AI (BrianL): review Kuantifier docs AI (Mat): Contribute VOMS patches upstream AI (Matt) EL9 repo K8s hackathon: Set up new IceCube namespace in the UNL PATh Facility Continuing OSG 24 setup AI (TimT): verify that release scripts can handle multiple architectures; some changes will be necessary to enable aarch64 Discussion \u00b6 \"opensciencegrid\" project on Harbor has almost filled its quota; tag immutability rules and a critical bug in Harbor garbage collection make it difficult to clean up A student may be starting later this month, to be supervised by Matt; writing scripts to assist Harbor image cleanup by using its API would be a good project for her Support Update \u00b6 LBL / ORNL (BrianL): assissting them set up their HTCondor-CE for ALICE JLAB (Matt): credmon started working when run as root without their wrapper script LIGO (Matt): stash-cache service now fails to start after SystemD overrides; the actual error is coming from the authfile-update script OU (Mat): new xrootd-standalone setup failed to start xrootd due to 'CA errors'; the actual issue was that XRootD was configured to require CRLs and they were missing DevOps \u00b6 None this week OSG Release Team \u00b6 None this week Discussion \u00b6 None this week","title":"September 3, 2024"},{"location":"meetings/2024/TechArea20240903/#osg-technology-area-meeting-3-september-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Marco, Mat, Matt, TimT","title":"OSG Technology Area Meeting, 3 September 2024"},{"location":"meetings/2024/TechArea20240903/#announcements","text":"BrianB, BrianL, Derek, and Mat at IRIS-HEP Retreat this week","title":"Announcements"},{"location":"meetings/2024/TechArea20240903/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: TimT 7 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240903/#jira-as-of-monday-morning-todo","text":"# of tickets \u0394 State 214 -3 Open 14 +0 Selected for Dev 34 +3 In Progress 17 +0 Dev Complete 7 +2 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning) (TODO)"},{"location":"meetings/2024/TechArea20240903/#osg-software-team","text":"AI (BrianL): review Kuantifier docs AI (Mat): Contribute VOMS patches upstream AI (Matt) EL9 repo K8s hackathon: Set up new IceCube namespace in the UNL PATh Facility Continuing OSG 24 setup AI (TimT): verify that release scripts can handle multiple architectures; some changes will be necessary to enable aarch64","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240903/#discussion","text":"\"opensciencegrid\" project on Harbor has almost filled its quota; tag immutability rules and a critical bug in Harbor garbage collection make it difficult to clean up A student may be starting later this month, to be supervised by Matt; writing scripts to assist Harbor image cleanup by using its API would be a good project for her","title":"Discussion"},{"location":"meetings/2024/TechArea20240903/#support-update","text":"LBL / ORNL (BrianL): assissting them set up their HTCondor-CE for ALICE JLAB (Matt): credmon started working when run as root without their wrapper script LIGO (Matt): stash-cache service now fails to start after SystemD overrides; the actual error is coming from the authfile-update script OU (Mat): new xrootd-standalone setup failed to start xrootd due to 'CA errors'; the actual issue was that XRootD was configured to require CRLs and they were missing","title":"Support Update"},{"location":"meetings/2024/TechArea20240903/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240903/#osg-release-team","text":"None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240903/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20240924/","text":"OSG Technology Area Meeting, 24 September 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 TimT Wed-Fri Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Matt 6 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) (TODO) \u00b6 # of tickets \u0394 State 215 -1 Open 15 +0 Selected for Dev 31 -4 In Progress 17 +0 Dev Complete 4 +4 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 GGUS is going away Link to open Topology CO petitions has been updated. Looks like the old link caused us to miss some registrations OSG 24: AI (BrianL): create spreadsheet of packages to release AI (Matt): update GHA to build ARM OSPool EP containers First pass at ARM build has many failures - no ARM cvmfsexec for el9 Should upgrade CUDA images from el8 to el9 AI (Mat): update osg-release Discussion \u00b6 mem4000 can't run VMUniverse jobs due to libvirtd issues Support Update \u00b6 AI (Mat): Begin decomissioning colorado.xsede.org DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: condor 23.10 condor 23.0.15 Ready for Testing: xrootd-5.7.1 xrd-pelican-0.9.4 To test: install on ap40 Discussion \u00b6 HTCondor 24 not expected until mid-October","title":"September 24, 2024"},{"location":"meetings/2024/TechArea20240924/#osg-technology-area-meeting-24-september-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 24 September 2024"},{"location":"meetings/2024/TechArea20240924/#announcements","text":"TimT Wed-Fri Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22","title":"Announcements"},{"location":"meetings/2024/TechArea20240924/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Matt 6 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20240924/#jira-as-of-monday-morning-todo","text":"# of tickets \u0394 State 215 -1 Open 15 +0 Selected for Dev 31 -4 In Progress 17 +0 Dev Complete 4 +4 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning) (TODO)"},{"location":"meetings/2024/TechArea20240924/#osg-software-team","text":"GGUS is going away Link to open Topology CO petitions has been updated. Looks like the old link caused us to miss some registrations OSG 24: AI (BrianL): create spreadsheet of packages to release AI (Matt): update GHA to build ARM OSPool EP containers First pass at ARM build has many failures - no ARM cvmfsexec for el9 Should upgrade CUDA images from el8 to el9 AI (Mat): update osg-release","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20240924/#discussion","text":"mem4000 can't run VMUniverse jobs due to libvirtd issues","title":"Discussion"},{"location":"meetings/2024/TechArea20240924/#support-update","text":"AI (Mat): Begin decomissioning colorado.xsede.org","title":"Support Update"},{"location":"meetings/2024/TechArea20240924/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20240924/#osg-release-team","text":"Ready for Release: condor 23.10 condor 23.0.15 Ready for Testing: xrootd-5.7.1 xrd-pelican-0.9.4 To test: install on ap40","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20240924/#discussion_1","text":"HTCondor 24 not expected until mid-October","title":"Discussion"},{"location":"meetings/2024/TechArea20241001/","text":"OSG Technology Area Meeting, 1 October 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Matt, Tim Announcements \u00b6 Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: Tim 8 (+2) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) (TODO) \u00b6 # of tickets \u0394 State 219 +4 Open 20 +5 Selected for Dev 37 +6 In Progress 17 +0 Dev Complete 4 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 Kubernetes hackathon AI (Brian): upgrade Prometheus AI (Mat): EL9 repo? AI (Mat): GLBRC origins AI (Mat): one of sealed-secrets or cert-manager upgrades AI (Matt): IGWN CVMFS on the PATh Facility AI (Matt): PATh FIU IPv6 OSG 24: Package builds assigned! AI (Matt): ARM OSPool EP containers? Need to debug x86_64 Singularity test failures causes by adding arm Add CPU/memory limiting Custom GitHub action for building Docker images needs to be fixed for multiarch builds; AI (Matt): Merge Mat's repo-scripts PRs for updating the repo server; need to put osg-release for OSG 24 on repo to start testing Discussion \u00b6 OSPool ARM Singularity images (for payload jobs) do not exist -- Mat, Mats need to design a scheme for distinguishing them from x86_64 Kuantifier (Matt): Suggested Prometheus metrics for Kuantifier are easy to access, but the required config changes are not well documented which suggests a low usage rate across the community. The current Kuantifier user base is well versed in Kubernetes so this should be fine for now. Related: https://github.com/rptaylor/kapel/issues/71#issuecomment-2384268967 Support Update \u00b6 Many support requests are coming in from IGWN because they want to use the PATh Facility Mat (ComputeCanada-Cedar for IGWN): Singularity support wasn't being enabled because GlideinWMS startup scripts were requiring the default images to be from CVMFS. Fixed in frontend config. DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: condor 23.10 condor 23.0.15 Ready for Testing: xrootd-5.7.1 xrd-pelican-0.9.4 To test: install on ap40 Discussion \u00b6 HTCondor 24 not expected until mid-October","title":"October 1, 2024"},{"location":"meetings/2024/TechArea20241001/#osg-technology-area-meeting-1-october-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Matt, Tim","title":"OSG Technology Area Meeting, 1 October 2024"},{"location":"meetings/2024/TechArea20241001/#announcements","text":"Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22","title":"Announcements"},{"location":"meetings/2024/TechArea20241001/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: Tim 8 (+2) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241001/#jira-as-of-monday-morning-todo","text":"# of tickets \u0394 State 219 +4 Open 20 +5 Selected for Dev 37 +6 In Progress 17 +0 Dev Complete 4 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning) (TODO)"},{"location":"meetings/2024/TechArea20241001/#osg-software-team","text":"Kubernetes hackathon AI (Brian): upgrade Prometheus AI (Mat): EL9 repo? AI (Mat): GLBRC origins AI (Mat): one of sealed-secrets or cert-manager upgrades AI (Matt): IGWN CVMFS on the PATh Facility AI (Matt): PATh FIU IPv6 OSG 24: Package builds assigned! AI (Matt): ARM OSPool EP containers? Need to debug x86_64 Singularity test failures causes by adding arm Add CPU/memory limiting Custom GitHub action for building Docker images needs to be fixed for multiarch builds; AI (Matt): Merge Mat's repo-scripts PRs for updating the repo server; need to put osg-release for OSG 24 on repo to start testing","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241001/#discussion","text":"OSPool ARM Singularity images (for payload jobs) do not exist -- Mat, Mats need to design a scheme for distinguishing them from x86_64 Kuantifier (Matt): Suggested Prometheus metrics for Kuantifier are easy to access, but the required config changes are not well documented which suggests a low usage rate across the community. The current Kuantifier user base is well versed in Kubernetes so this should be fine for now. Related: https://github.com/rptaylor/kapel/issues/71#issuecomment-2384268967","title":"Discussion"},{"location":"meetings/2024/TechArea20241001/#support-update","text":"Many support requests are coming in from IGWN because they want to use the PATh Facility Mat (ComputeCanada-Cedar for IGWN): Singularity support wasn't being enabled because GlideinWMS startup scripts were requiring the default images to be from CVMFS. Fixed in frontend config.","title":"Support Update"},{"location":"meetings/2024/TechArea20241001/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241001/#osg-release-team","text":"Ready for Release: condor 23.10 condor 23.0.15 Ready for Testing: xrootd-5.7.1 xrd-pelican-0.9.4 To test: install on ap40","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241001/#discussion_1","text":"HTCondor 24 not expected until mid-October","title":"Discussion"},{"location":"meetings/2024/TechArea20241008/","text":"OSG Technology Area Meeting, 8 October 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Tim Next week: Matt? 8 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) (TODO) \u00b6 # of tickets \u0394 State 220 +1 Open 20 +0 Selected for Dev 38 +1 In Progress 17 +0 Dev Complete 1 -3 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 OSG 24: Start building packages push osg/igtf-ca-certs into 24-release so osg-24 software base can be built AI (BrianL): review Matt's GHA ARM PRs el9 repo is not a blocker for osg-24 Once osg-ca-certs is ready, we can add OSG 24 to the software-base builds Discussion \u00b6 None this week Support Update \u00b6 osg-notify: unable to send outgoing emails Currently send emails via MTA in tiger which relays to DoIT MTA Larger issues around spam filtering software.igwn.org: not yet seeing the new CVMFS namespace need to complete daemonset rollouts in other PATh clusters DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: osdf-server-7.10.7 To test: install on ap40 BrianL kicked the tires during the meeting Ready for Testing: None this week Discussion \u00b6 TimT setting up HTCondor-24 RC repo Need to start moving build pipeline from dumbo to nest User group/filesystem config needed to support this, will create INF tickets","title":"October 8, 2024"},{"location":"meetings/2024/TechArea20241008/#osg-technology-area-meeting-8-october-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 8 October 2024"},{"location":"meetings/2024/TechArea20241008/#announcements","text":"Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22","title":"Announcements"},{"location":"meetings/2024/TechArea20241008/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Tim Next week: Matt? 8 (+0) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241008/#jira-as-of-monday-morning-todo","text":"# of tickets \u0394 State 220 +1 Open 20 +0 Selected for Dev 38 +1 In Progress 17 +0 Dev Complete 1 -3 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning) (TODO)"},{"location":"meetings/2024/TechArea20241008/#osg-software-team","text":"OSG 24: Start building packages push osg/igtf-ca-certs into 24-release so osg-24 software base can be built AI (BrianL): review Matt's GHA ARM PRs el9 repo is not a blocker for osg-24 Once osg-ca-certs is ready, we can add OSG 24 to the software-base builds","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241008/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20241008/#support-update","text":"osg-notify: unable to send outgoing emails Currently send emails via MTA in tiger which relays to DoIT MTA Larger issues around spam filtering software.igwn.org: not yet seeing the new CVMFS namespace need to complete daemonset rollouts in other PATh clusters","title":"Support Update"},{"location":"meetings/2024/TechArea20241008/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241008/#osg-release-team","text":"Ready for Release: osdf-server-7.10.7 To test: install on ap40 BrianL kicked the tires during the meeting Ready for Testing: None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241008/#discussion_1","text":"TimT setting up HTCondor-24 RC repo Need to start moving build pipeline from dumbo to nest User group/filesystem config needed to support this, will create INF tickets","title":"Discussion"},{"location":"meetings/2024/TechArea20241015/","text":"OSG Technology Area Meeting, 15 October 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Matt Next week: TimT 10 (+2) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) (TODO) \u00b6 # of tickets \u0394 State 224 +4 Open 19 -1 Selected for Dev 39 +1 In Progress 17 +0 Dev Complete 1 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 OSG 24: HTCSS 24 repos ready yet? AI (Matt): ospool-ep upgrade instructions. How hard will it be to push osg-htc tags? AI (Matt): Some additional builds AI (BrianL): a few builds, Pelican / OSDF packaging -> use config.d AI (BrianL): review Matt's doc PR, start working on initial OSG 24 release doc Kubernetes hackathon AI (Matt, BrianL): Traefik upgrades Glidein Manager work PATh Expanse IGWN CVMFS support Prometheus upgrades Discussion \u00b6 None this week Support Update \u00b6 IGWN (BrianL, Jason): glideins still aren't seeing CVMFS / Apptainer in the PF but the underlying EP containers look ok except for Expanse AGLT2 (BrianL): jobs in their HTCondor batch got a SIGTERM after restarting the CE DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: osdf-server-7.10.7 To test: install on ap40 BrianL kicked the tires during the meeting Ready for Testing: None this week Discussion \u00b6 TimT setting up HTCondor-24 RC repo Need to start moving build pipeline from dumbo to nest User group/filesystem config needed to support this, will create INF tickets","title":"October 15, 2024"},{"location":"meetings/2024/TechArea20241015/#osg-technology-area-meeting-15-october-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 15 October 2024"},{"location":"meetings/2024/TechArea20241015/#announcements","text":"Mat OOO Oct 14 - 29 BrianL OOO Oct 21 and 22","title":"Announcements"},{"location":"meetings/2024/TechArea20241015/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Matt Next week: TimT 10 (+2) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241015/#jira-as-of-monday-morning-todo","text":"# of tickets \u0394 State 224 +4 Open 19 -1 Selected for Dev 39 +1 In Progress 17 +0 Dev Complete 1 +0 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning) (TODO)"},{"location":"meetings/2024/TechArea20241015/#osg-software-team","text":"OSG 24: HTCSS 24 repos ready yet? AI (Matt): ospool-ep upgrade instructions. How hard will it be to push osg-htc tags? AI (Matt): Some additional builds AI (BrianL): a few builds, Pelican / OSDF packaging -> use config.d AI (BrianL): review Matt's doc PR, start working on initial OSG 24 release doc Kubernetes hackathon AI (Matt, BrianL): Traefik upgrades Glidein Manager work PATh Expanse IGWN CVMFS support Prometheus upgrades","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241015/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20241015/#support-update","text":"IGWN (BrianL, Jason): glideins still aren't seeing CVMFS / Apptainer in the PF but the underlying EP containers look ok except for Expanse AGLT2 (BrianL): jobs in their HTCondor batch got a SIGTERM after restarting the CE","title":"Support Update"},{"location":"meetings/2024/TechArea20241015/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241015/#osg-release-team","text":"Ready for Release: osdf-server-7.10.7 To test: install on ap40 BrianL kicked the tires during the meeting Ready for Testing: None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241015/#discussion_1","text":"TimT setting up HTCondor-24 RC repo Need to start moving build pipeline from dumbo to nest User group/filesystem config needed to support this, will create INF tickets","title":"Discussion"},{"location":"meetings/2024/TechArea20241029/","text":"OSG Technology Area Meeting, 29 October 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Announcements \u00b6 Mat OOO Oct 14 - 29 BrianL OOO Nov 5 PM Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: BrianL Next week: Mat 9 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) (TODO) \u00b6 # of tickets \u0394 State 221 -1 Open 21 -1 Selected for Dev 44 +4 In Progress 17 +0 Dev Complete 1 -2 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 OSG 24: aiming for release this Thursday! VMU tests look pretty bad with testing + release as expected. Do we have a run against development? HTCondor not syncing into 24-testing/release - we might need to update el7 repo again Development tests https://osg-sw-submit.chtc.wisc.edu/tests/20241021-1629/results.html Today: Start pushing packages through -main testing and -upcoming testing We are not currently testing 24-upcoming-development, but should have been while we were building packages Pelican 7.11.0 won't be ready by the initial release so we'll release with 7.10.11 and tell folks to hold off on installation / upgrade Need to make a decision on hosted-ce-tools when Mat gets back Contains some cvmfsexec utilities that we might not be able to get rid of AI (BrianL): write up OSG 24 initial release, review Matt's OSG 24 changes, OSDF origin/cache Container images: AI (Matt): Fix ospool-ep GHA + upgrade instructions GHAs fixed, still need instructions Following repos need OSG 24 added: docker-compute-entrypoint docker-xcache gratia-probe htcondor-autoscale-manager open-science-pool-registry osg-repo-scripts (?) Kubernetes hackathon Troubleshoot PATh Facility Syracuse horiziontal pod autoscaling Investigate Expanse cvmfsexec setup Glidein Manager work Discussion \u00b6 None this week Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: pelican-7.10.11 Ready for Testing: None this week Discussion \u00b6 AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"October 29, 2024"},{"location":"meetings/2024/TechArea20241029/#osg-technology-area-meeting-29-october-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending:","title":"OSG Technology Area Meeting, 29 October 2024"},{"location":"meetings/2024/TechArea20241029/#announcements","text":"Mat OOO Oct 14 - 29 BrianL OOO Nov 5 PM","title":"Announcements"},{"location":"meetings/2024/TechArea20241029/#triage-duty","text":"Triage duty shifts Tue-Mon This week: BrianL Next week: Mat 9 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241029/#jira-as-of-monday-morning-todo","text":"# of tickets \u0394 State 221 -1 Open 21 -1 Selected for Dev 44 +4 In Progress 17 +0 Dev Complete 1 -2 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning) (TODO)"},{"location":"meetings/2024/TechArea20241029/#osg-software-team","text":"OSG 24: aiming for release this Thursday! VMU tests look pretty bad with testing + release as expected. Do we have a run against development? HTCondor not syncing into 24-testing/release - we might need to update el7 repo again Development tests https://osg-sw-submit.chtc.wisc.edu/tests/20241021-1629/results.html Today: Start pushing packages through -main testing and -upcoming testing We are not currently testing 24-upcoming-development, but should have been while we were building packages Pelican 7.11.0 won't be ready by the initial release so we'll release with 7.10.11 and tell folks to hold off on installation / upgrade Need to make a decision on hosted-ce-tools when Mat gets back Contains some cvmfsexec utilities that we might not be able to get rid of AI (BrianL): write up OSG 24 initial release, review Matt's OSG 24 changes, OSDF origin/cache Container images: AI (Matt): Fix ospool-ep GHA + upgrade instructions GHAs fixed, still need instructions Following repos need OSG 24 added: docker-compute-entrypoint docker-xcache gratia-probe htcondor-autoscale-manager open-science-pool-registry osg-repo-scripts (?) Kubernetes hackathon Troubleshoot PATh Facility Syracuse horiziontal pod autoscaling Investigate Expanse cvmfsexec setup Glidein Manager work","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241029/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20241029/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20241029/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241029/#osg-release-team","text":"Ready for Release: pelican-7.10.11 Ready for Testing: None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241029/#discussion_1","text":"AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"Discussion"},{"location":"meetings/2024/TechArea20241105/","text":"OSG Technology Area Meeting, 5 November 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Tim Announcements \u00b6 BrianL OOO Nov 5 PM BrianL OOO week of Thanksgiving NDC-C hackathon next week Tue-Fri; BrianL, Mat will be occupied. 11/12 meeting is cancelled. Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Matt 8 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket Jira (as of Monday morning) (TODO) \u00b6 # of tickets \u0394 State 220 -1 Open 17 -4 Selected for Dev 45 +1 In Progress 17 +0 Dev Complete 2 +1 Ready for Testing 1 +1 Ready for Release OSG Software Team \u00b6 OSG 24 clean-up: OSDF caches / origins AI (Mat): prepare osdf-server for release with Pelican 7.11 AI (Mat): write cache doc, these upstream changes will be relevant: https://github.com/PelicanPlatform/pelican/pull/1709/files AI (BrianL) write origin docs Container images: AI (Matt): upgrade instructions for the OSPool EP Following repos need OSG 24 added: docker-compute-entrypoint docker-xcache gratia-probe htcondor-autoscale-manager open-science-pool-registry osg-repo-scripts (?) AI (Matt): Sync 24-main tarballs onto repo EL9 repo: - AI (Mat): Write repo migration instructions Discussion \u00b6 None this week Support Update \u00b6 BNL (BrianL): Belle II SSL-based glideins aren't getting reported. Not a supported method in OSG so we have a hacky workaround where they set orig_AuthTokenIssuer and GRACC will map the value to Belle II. ATLAS / BNL / AGLT2 (BrianL, MattW): questions about scheduling prioritizing cores and memory efficiency. Greg is taking point on the ATLAS communication thread, seems relevant for us. Notably UChicago doesn't have these problems: they allow Harvester to specify the memory that they want and allow pilots to consume 3x their memory requests DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: pelican-7.10.11 Ready for Testing: None this week Discussion \u00b6 AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"November 5, 2024"},{"location":"meetings/2024/TechArea20241105/#osg-technology-area-meeting-5-november-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: BrianL, Mat, Tim","title":"OSG Technology Area Meeting, 5 November 2024"},{"location":"meetings/2024/TechArea20241105/#announcements","text":"BrianL OOO Nov 5 PM BrianL OOO week of Thanksgiving NDC-C hackathon next week Tue-Fri; BrianL, Mat will be occupied. 11/12 meeting is cancelled.","title":"Announcements"},{"location":"meetings/2024/TechArea20241105/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Matt 8 (-1) open FreshDesk tickets 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241105/#jira-as-of-monday-morning-todo","text":"# of tickets \u0394 State 220 -1 Open 17 -4 Selected for Dev 45 +1 In Progress 17 +0 Dev Complete 2 +1 Ready for Testing 1 +1 Ready for Release","title":"Jira (as of Monday morning) (TODO)"},{"location":"meetings/2024/TechArea20241105/#osg-software-team","text":"OSG 24 clean-up: OSDF caches / origins AI (Mat): prepare osdf-server for release with Pelican 7.11 AI (Mat): write cache doc, these upstream changes will be relevant: https://github.com/PelicanPlatform/pelican/pull/1709/files AI (BrianL) write origin docs Container images: AI (Matt): upgrade instructions for the OSPool EP Following repos need OSG 24 added: docker-compute-entrypoint docker-xcache gratia-probe htcondor-autoscale-manager open-science-pool-registry osg-repo-scripts (?) AI (Matt): Sync 24-main tarballs onto repo EL9 repo: - AI (Mat): Write repo migration instructions","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241105/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20241105/#support-update","text":"BNL (BrianL): Belle II SSL-based glideins aren't getting reported. Not a supported method in OSG so we have a hacky workaround where they set orig_AuthTokenIssuer and GRACC will map the value to Belle II. ATLAS / BNL / AGLT2 (BrianL, MattW): questions about scheduling prioritizing cores and memory efficiency. Greg is taking point on the ATLAS communication thread, seems relevant for us. Notably UChicago doesn't have these problems: they allow Harvester to specify the memory that they want and allow pilots to consume 3x their memory requests","title":"Support Update"},{"location":"meetings/2024/TechArea20241105/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241105/#osg-release-team","text":"Ready for Release: pelican-7.10.11 Ready for Testing: None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241105/#discussion_1","text":"AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"Discussion"},{"location":"meetings/2024/TechArea20241119/","text":"OSG Technology Area Meeting, 19 November 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Marco, Mat, Matt, Tim Announcements \u00b6 BrianL OOO week of Thanksgiving Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: TimT Next week: Mat 8 (+0) open FreshDesk tickets (since 11/05) 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 (delta since 11/05 meeting) # of tickets \u0394 State 221 +0 Open 17 +0 Selected for Dev 45 +0 In Progress 17 +0 Dev Complete 1 -1 Ready for Testing 0 -1 Ready for Release OSG Software Team \u00b6 OSG 24 clean-up: OSDF caches / origins AI (Mat): prepare osdf-server for release with Pelican 7.11 AI (Matt): Sync 24-main tarballs onto repo Ask UW CSL to set up rsync server AI (Matt): Fix various OSG 24 image build issues AI (BrianL): Review PRs for: OSG 23 to 24 upgrade docs HTCondor-CE 23 to 24 upgrade docs EL9 repo: - AI (Mat): Write repo migration instructions Discussion \u00b6 GlideinWMS: 3.10.8 in progress LIGO uses startd disk enforcement, which is causing Glideins to request insufficient disk and exit without running any jobs. Marco will talk to James Clark to debug. Support Update \u00b6 Matt (Harrisburg): ospool-ep container not cleaning up scratch directories. Mats Rynge is working on a cleanup script for factory-sent pilots; Matt will investigate making use of that script for container pilots as well. DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Testing: Nothing yet Ready for Release: Nothing yet Discussion \u00b6 None this week","title":"OSG Technology Area Meeting, 19 November 2024"},{"location":"meetings/2024/TechArea20241119/#osg-technology-area-meeting-19-november-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Marco, Mat, Matt, Tim","title":"OSG Technology Area Meeting, 19 November 2024"},{"location":"meetings/2024/TechArea20241119/#announcements","text":"BrianL OOO week of Thanksgiving","title":"Announcements"},{"location":"meetings/2024/TechArea20241119/#triage-duty","text":"Triage duty shifts Tue-Mon This week: TimT Next week: Mat 8 (+0) open FreshDesk tickets (since 11/05) 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241119/#jira-as-of-monday-morning","text":"(delta since 11/05 meeting) # of tickets \u0394 State 221 +0 Open 17 +0 Selected for Dev 45 +0 In Progress 17 +0 Dev Complete 1 -1 Ready for Testing 0 -1 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20241119/#osg-software-team","text":"OSG 24 clean-up: OSDF caches / origins AI (Mat): prepare osdf-server for release with Pelican 7.11 AI (Matt): Sync 24-main tarballs onto repo Ask UW CSL to set up rsync server AI (Matt): Fix various OSG 24 image build issues AI (BrianL): Review PRs for: OSG 23 to 24 upgrade docs HTCondor-CE 23 to 24 upgrade docs EL9 repo: - AI (Mat): Write repo migration instructions","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241119/#discussion","text":"GlideinWMS: 3.10.8 in progress LIGO uses startd disk enforcement, which is causing Glideins to request insufficient disk and exit without running any jobs. Marco will talk to James Clark to debug.","title":"Discussion"},{"location":"meetings/2024/TechArea20241119/#support-update","text":"Matt (Harrisburg): ospool-ep container not cleaning up scratch directories. Mats Rynge is working on a cleanup script for factory-sent pilots; Matt will investigate making use of that script for container pilots as well.","title":"Support Update"},{"location":"meetings/2024/TechArea20241119/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241119/#osg-release-team","text":"Ready for Testing: Nothing yet Ready for Release: Nothing yet","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241119/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2024/TechArea20241126/","text":"OSG Technology Area Meeting, 26 November 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Derek, Marco, Mat, Matt, Tim Announcements \u00b6 BrianL OOO this week Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: Mat? 10 (+2) open FreshDesk tickets (since 11/19) 0 (+0) open GGUS ticket Jira (as of Monday morning) \u00b6 # of tickets \u0394 State 223 +2 Open 17 +0 Selected for Dev 37 -8 In Progress 17 +0 Dev Complete 2 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team \u00b6 AI (Mat): fix VMU test images lost in data outage AI (Matt): investigate slow rsync of Condor RPMs into repo OSG 24 clean-up: - OSDF caches / origins - AI (Mat): prepare osdf-server for release with Pelican 7.11 EL9 repo: - AI (Mat): Write repo migration instructions Discussion \u00b6 3.10.8 is out and has been smoke tested; promotion request will be incoming 3.10.7-3 has not been promoted yet: CHTC data outage was preventing VMU tests from running; the only change in this version vs. 3.10.7-2 is disabling path mangling which was causing problems for CMS EL6 containers Support Update \u00b6 None this week DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: pelican-7.10.11 Ready for Testing: None this week Discussion \u00b6 AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"OSG Technology Area Meeting, 26 November 2024"},{"location":"meetings/2024/TechArea20241126/#osg-technology-area-meeting-26-november-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Derek, Marco, Mat, Matt, Tim","title":"OSG Technology Area Meeting, 26 November 2024"},{"location":"meetings/2024/TechArea20241126/#announcements","text":"BrianL OOO this week","title":"Announcements"},{"location":"meetings/2024/TechArea20241126/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: Mat? 10 (+2) open FreshDesk tickets (since 11/19) 0 (+0) open GGUS ticket","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241126/#jira-as-of-monday-morning","text":"# of tickets \u0394 State 223 +2 Open 17 +0 Selected for Dev 37 -8 In Progress 17 +0 Dev Complete 2 +1 Ready for Testing 0 +0 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20241126/#osg-software-team","text":"AI (Mat): fix VMU test images lost in data outage AI (Matt): investigate slow rsync of Condor RPMs into repo OSG 24 clean-up: - OSDF caches / origins - AI (Mat): prepare osdf-server for release with Pelican 7.11 EL9 repo: - AI (Mat): Write repo migration instructions","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241126/#discussion","text":"3.10.8 is out and has been smoke tested; promotion request will be incoming 3.10.7-3 has not been promoted yet: CHTC data outage was preventing VMU tests from running; the only change in this version vs. 3.10.7-2 is disabling path mangling which was causing problems for CMS EL6 containers","title":"Discussion"},{"location":"meetings/2024/TechArea20241126/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2024/TechArea20241126/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241126/#osg-release-team","text":"Ready for Release: pelican-7.10.11 Ready for Testing: None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241126/#discussion_1","text":"AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"Discussion"},{"location":"meetings/2024/TechArea20241203/","text":"OSG Technology Area Meeting, 3 December 2024 \u00b6 Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian, Tim, Mat, Matt Announcements \u00b6 BrianL OOO Dec 23 - Jan 3 Matt OOO Dec 23 - Jan 1 Triage Duty \u00b6 Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 10 (+0) open FreshDesk tickets (since 11/19) Jira (as of Monday morning) \u00b6 (delta since 11/05 meeting) # of tickets \u0394 State 221 +0 Open 17 +0 Selected for Dev 45 +0 In Progress 17 +0 Dev Complete 1 -1 Ready for Testing 0 -1 Ready for Release OSG Software Team \u00b6 EL9 repo status AI (Mat): Finish announcement email AI (Matt): Review PR for migration script Glidein dir cleanup status: temporarily disabled due to holding up Glidein startup at a site with a large number of dirs to clean up on a shared filesystem. Todd T, Matt, and Mat had a brainstorming discussion, for which Todd will write up a design. AI (Mat): Create new VMU disk images so we can run VMU tests again. AI (Mat): Build XRootD 5.7.2 AI (Mat): Review OSG 24 upgrade instructions PRs AI (Matt): Remaining OSG 24 image builds: docker-xcache: Mat to review PR docker-compute-entrypoint: requires a new osg-ce build, though hosted CEs should already be using the version of osg-ce from main-testing or upcoming-testing. Matt should ask hosted CE operations if they are running the new version AI (Matt): VMU tests on ARM; CHTC has an ARM machine that is restricted to running jobs from BatLab and CHTC staff; CHTC operations will review the policy. Discussion \u00b6 Thoughts on how we can reduce Hosted CE IPv4 usage? Need to discuss with the HTCondor development team if unique IPv4 addresses are necessary; Tim thinks yes, for reverse DNS, but Mat thinks reverse DNS isn't used anymore Tim suggests running each Hosted CE on a different port but Brian says that would be verfy painful to manage and would essentially lock CEs to specific nodes. Brian will investigate IngressRouteTCP. Support Update \u00b6 SURF (Mat): LIGO site using an xcache-based cache; they got a new cert and need a LIGO test file to test with because their cache does not support the OSG VO. IceCube (Mat): condor_gpu_discovery not discovering Intel GPUs. Mat will discuss with the HTCondor dev team. DevOps \u00b6 None this week OSG Release Team \u00b6 Ready for Release: pelican-7.10.11 Ready for Testing: None this week Discussion \u00b6 AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"December 3, 2024"},{"location":"meetings/2024/TechArea20241203/#osg-technology-area-meeting-3-december-2024","text":"Coordinates: Conference: +1-415-655-0002, PIN: 146 266 9392, https://morgridge-org.zoom.us/j/91987518094 (password sent separately) Attending: Brian, Tim, Mat, Matt","title":"OSG Technology Area Meeting, 3 December 2024"},{"location":"meetings/2024/TechArea20241203/#announcements","text":"BrianL OOO Dec 23 - Jan 3 Matt OOO Dec 23 - Jan 1","title":"Announcements"},{"location":"meetings/2024/TechArea20241203/#triage-duty","text":"Triage duty shifts Tue-Mon This week: Mat Next week: BrianL 10 (+0) open FreshDesk tickets (since 11/19)","title":"Triage Duty"},{"location":"meetings/2024/TechArea20241203/#jira-as-of-monday-morning","text":"(delta since 11/05 meeting) # of tickets \u0394 State 221 +0 Open 17 +0 Selected for Dev 45 +0 In Progress 17 +0 Dev Complete 1 -1 Ready for Testing 0 -1 Ready for Release","title":"Jira (as of Monday morning)"},{"location":"meetings/2024/TechArea20241203/#osg-software-team","text":"EL9 repo status AI (Mat): Finish announcement email AI (Matt): Review PR for migration script Glidein dir cleanup status: temporarily disabled due to holding up Glidein startup at a site with a large number of dirs to clean up on a shared filesystem. Todd T, Matt, and Mat had a brainstorming discussion, for which Todd will write up a design. AI (Mat): Create new VMU disk images so we can run VMU tests again. AI (Mat): Build XRootD 5.7.2 AI (Mat): Review OSG 24 upgrade instructions PRs AI (Matt): Remaining OSG 24 image builds: docker-xcache: Mat to review PR docker-compute-entrypoint: requires a new osg-ce build, though hosted CEs should already be using the version of osg-ce from main-testing or upcoming-testing. Matt should ask hosted CE operations if they are running the new version AI (Matt): VMU tests on ARM; CHTC has an ARM machine that is restricted to running jobs from BatLab and CHTC staff; CHTC operations will review the policy.","title":"OSG Software Team"},{"location":"meetings/2024/TechArea20241203/#discussion","text":"Thoughts on how we can reduce Hosted CE IPv4 usage? Need to discuss with the HTCondor development team if unique IPv4 addresses are necessary; Tim thinks yes, for reverse DNS, but Mat thinks reverse DNS isn't used anymore Tim suggests running each Hosted CE on a different port but Brian says that would be verfy painful to manage and would essentially lock CEs to specific nodes. Brian will investigate IngressRouteTCP.","title":"Discussion"},{"location":"meetings/2024/TechArea20241203/#support-update","text":"SURF (Mat): LIGO site using an xcache-based cache; they got a new cert and need a LIGO test file to test with because their cache does not support the OSG VO. IceCube (Mat): condor_gpu_discovery not discovering Intel GPUs. Mat will discuss with the HTCondor dev team.","title":"Support Update"},{"location":"meetings/2024/TechArea20241203/#devops","text":"None this week","title":"DevOps"},{"location":"meetings/2024/TechArea20241203/#osg-release-team","text":"Ready for Release: pelican-7.10.11 Ready for Testing: None this week","title":"OSG Release Team"},{"location":"meetings/2024/TechArea20241203/#discussion_1","text":"AI (Matt): Additional XRootD patches for a patch release: #2300 and #2363","title":"Discussion"},{"location":"operations/comanage-recipes/","text":"COmanage Recipes \u00b6 A collection of step-by-step instructions for OSG COmanage administrators. Provisioning a CO Group in COManage \u00b6 In order for a CO Group from COManage to show up in LDAP (and thus be made available for reference on hosts), it must first be provisioned. Follow these steps to provision a CO Group into LDAP: Create CO Group in COManage and add members (or use existing CO Group) \u00b6 If using an already existing group, skip to the next step. Navigate to the All Groups page in COManage and click the + Add Group button near the top-right. Give the group a name then click ADD , which will bring you to the Edit page for the new group Click on MEMBERS , then type in the name or identifier for a user you want to give membership to, then select the user from the drop-down and click the ADD button. Repeat as necessary for each group member. As the creator of the group you will already have both Membership in, and Ownership over, the new group. Remove yourself as appropriate. A Note on Groups used for OIDC Authorization If a CO Group is only being used for Authorization via OIDC clients (i.e. doesn't need to be accessed by LDAP or any special scripts), then it only needs to be created and have members added. Identifier assignment and Provisioning are not nessisary. Find lowest unclaimed non-user OSG GID \u00b6 Each group needs a unique OSG group ID number or OSG GID , assigned from the non-user range starting at 200000 . Run the following command on a host with ldapsearch capability (like ap40) to find the highest / most recently assigned OSG GID . sudo ldapsearch -H ldaps://ldap.cilogon.org -D uid=readonly_user,ou=system,o=OSG,o=CO,dc=cilogon,dc=org \\ -w $(sudo awk '/ldap_default_authtok/ {print $3}' /etc/sssd/conf.d/0060_domain_CILOGON.ORG.conf) \\ -b ou=groups,o=OSG,o=CO,dc=cilogon,dc=org -s one '(cn=*)' | grep \"gidNumber\" | sort | tail Set OSG GID and OSG Group Name Identifiers \u00b6 Navigate back to the PROPERTIES tab of Edit page for the group you are trying to provision, then click the + Add Identifier button. Add an Identifier of type OSG GID with a value one greater than the highest one assigned so far (found in the last step). Add an Identifier of type OSG Group Name with the group's name as it should appear in LDAP. Create Unix Cluster Group \u00b6 Each COManage Group needs a Unix Cluster Group in order to be provisioned. On COManage, navigate to Configuration -> Clusters -> Configure -> Manage Unix Cluster Groups -> + Add Unix Cluster Group Select the name of the Group you are trying to provision from the drop-down menu, then click ADD Provision group \u00b6 In the PROVISIONED SERVICES tab of the Edit page for the Group, click the \u2699 Provision button, then on Provision . If all prior steps have been completed, you should get a message that the Group was successfully provisioned.","title":"COManage Recipes"},{"location":"operations/comanage-recipes/#comanage-recipes","text":"A collection of step-by-step instructions for OSG COmanage administrators.","title":"COmanage Recipes"},{"location":"operations/comanage-recipes/#provisioning-a-co-group-in-comanage","text":"In order for a CO Group from COManage to show up in LDAP (and thus be made available for reference on hosts), it must first be provisioned. Follow these steps to provision a CO Group into LDAP:","title":"Provisioning a CO Group in COManage"},{"location":"operations/comanage-recipes/#create-co-group-in-comanage-and-add-members-or-use-existing-co-group","text":"If using an already existing group, skip to the next step. Navigate to the All Groups page in COManage and click the + Add Group button near the top-right. Give the group a name then click ADD , which will bring you to the Edit page for the new group Click on MEMBERS , then type in the name or identifier for a user you want to give membership to, then select the user from the drop-down and click the ADD button. Repeat as necessary for each group member. As the creator of the group you will already have both Membership in, and Ownership over, the new group. Remove yourself as appropriate. A Note on Groups used for OIDC Authorization If a CO Group is only being used for Authorization via OIDC clients (i.e. doesn't need to be accessed by LDAP or any special scripts), then it only needs to be created and have members added. Identifier assignment and Provisioning are not nessisary.","title":"Create CO Group in COManage and add members (or use existing CO Group)"},{"location":"operations/comanage-recipes/#find-lowest-unclaimed-non-user-osg-gid","text":"Each group needs a unique OSG group ID number or OSG GID , assigned from the non-user range starting at 200000 . Run the following command on a host with ldapsearch capability (like ap40) to find the highest / most recently assigned OSG GID . sudo ldapsearch -H ldaps://ldap.cilogon.org -D uid=readonly_user,ou=system,o=OSG,o=CO,dc=cilogon,dc=org \\ -w $(sudo awk '/ldap_default_authtok/ {print $3}' /etc/sssd/conf.d/0060_domain_CILOGON.ORG.conf) \\ -b ou=groups,o=OSG,o=CO,dc=cilogon,dc=org -s one '(cn=*)' | grep \"gidNumber\" | sort | tail","title":"Find lowest unclaimed non-user OSG GID"},{"location":"operations/comanage-recipes/#set-osg-gid-and-osg-group-name-identifiers","text":"Navigate back to the PROPERTIES tab of Edit page for the group you are trying to provision, then click the + Add Identifier button. Add an Identifier of type OSG GID with a value one greater than the highest one assigned so far (found in the last step). Add an Identifier of type OSG Group Name with the group's name as it should appear in LDAP.","title":"Set OSG GID and OSG Group Name Identifiers"},{"location":"operations/comanage-recipes/#create-unix-cluster-group","text":"Each COManage Group needs a Unix Cluster Group in order to be provisioned. On COManage, navigate to Configuration -> Clusters -> Configure -> Manage Unix Cluster Groups -> + Add Unix Cluster Group Select the name of the Group you are trying to provision from the drop-down menu, then click ADD","title":"Create Unix Cluster Group"},{"location":"operations/comanage-recipes/#provision-group","text":"In the PROVISIONED SERVICES tab of the Edit page for the Group, click the \u2699 Provision button, then on Provision . If all prior steps have been completed, you should get a message that the Group was successfully provisioned.","title":"Provision group"},{"location":"operations/comanage-sop/","text":"COManage Operations \u00b6 OSG is using a new identity management system called COManage. This system is used for managing contact information for OSPool and PATh Facility users, Topology site contacts, and OSG/PATh staff. Contact Registration \u00b6 Contact registrations must be manually approved by a COManage admin. Follow the instructions below to approve a contact registration. Note This page is for COManage Admins who want to approve contact registrations. If you are a user who wants to register with COManage, go to the Registering for the OSG COManage page instead. Check for contact registration requests: If you are a COManage sponsor for a given group of registrants, you will receive email notifications when there are new registration requests. Check for an email from registry@cilogon.org saying \"Petition for changed status from Confirmed to Pending Approval\" and visit the first link in the body. Alternatively, you can view all requests pending approval here . Click on the registrant's name to view their request. Topology registrations You can view the list of Topology registrations here . Note Many groups share our COManage instance so make sure that you're only approving registration requests for the appropriate group, e.g. site contacts. If prompted, log in with your institutional credentials. Review the request: Verify that the request is legitimate by asking someone affiliated with the site, collaboration, or the sponsor of a project to verify the registrant's affiliation. In the case of the OSPool, this should be the OSG Campus Coordinator (try searching Freshdesk for the requester's email address and look for a private note with approval). Verify that the registrant has submitted their request using the correct form, e.g. OSPool users should not have submitted a request to register as a Topology contact. In the top-right corner, click the \"Add comment\" link and add a note indicating how you verified the request 'Approver Comment' is public The registrant will see notes added to the \"Approver Comment\" field Click the \"Approve\" button. You should see \"Petition Approved\" and \"Petition Finalized\" on top. The Status should now be \"Finalized\". Click on their name next to CO Person to verify that the registrant is Active and that they are in the expected groups. The user will get an email saying \"Petition for changed status from Pending Approval to Approved\". Revoking AP login access \u00b6 Login access to AP1 (PATh Facility) and AP40 (OSPool) is controlled by membership to COManage groups. To revoke a user's login access to either of these APs, perform the following steps: Find the corresponding user in COManage and revoke access to all OSG services or just the relevant AP: If you are revoking access to all OSG services, set the user's CO Person status to Suspended If you only need to revoke access to AP1 or AP40, remove the user from the ap1-login or ap40-login group, respectively Note the OSG Username identifier of the user On the AP host(s) where you are revoking access, clear the SSSD cache as root: root@ap-host # sss_cache -u <OSG Username> Replacing <OSG Username> with the OSG Username identifier that you noted in step (2)","title":"COManage SOP"},{"location":"operations/comanage-sop/#comanage-operations","text":"OSG is using a new identity management system called COManage. This system is used for managing contact information for OSPool and PATh Facility users, Topology site contacts, and OSG/PATh staff.","title":"COManage Operations"},{"location":"operations/comanage-sop/#contact-registration","text":"Contact registrations must be manually approved by a COManage admin. Follow the instructions below to approve a contact registration. Note This page is for COManage Admins who want to approve contact registrations. If you are a user who wants to register with COManage, go to the Registering for the OSG COManage page instead. Check for contact registration requests: If you are a COManage sponsor for a given group of registrants, you will receive email notifications when there are new registration requests. Check for an email from registry@cilogon.org saying \"Petition for changed status from Confirmed to Pending Approval\" and visit the first link in the body. Alternatively, you can view all requests pending approval here . Click on the registrant's name to view their request. Topology registrations You can view the list of Topology registrations here . Note Many groups share our COManage instance so make sure that you're only approving registration requests for the appropriate group, e.g. site contacts. If prompted, log in with your institutional credentials. Review the request: Verify that the request is legitimate by asking someone affiliated with the site, collaboration, or the sponsor of a project to verify the registrant's affiliation. In the case of the OSPool, this should be the OSG Campus Coordinator (try searching Freshdesk for the requester's email address and look for a private note with approval). Verify that the registrant has submitted their request using the correct form, e.g. OSPool users should not have submitted a request to register as a Topology contact. In the top-right corner, click the \"Add comment\" link and add a note indicating how you verified the request 'Approver Comment' is public The registrant will see notes added to the \"Approver Comment\" field Click the \"Approve\" button. You should see \"Petition Approved\" and \"Petition Finalized\" on top. The Status should now be \"Finalized\". Click on their name next to CO Person to verify that the registrant is Active and that they are in the expected groups. The user will get an email saying \"Petition for changed status from Pending Approval to Approved\".","title":"Contact Registration"},{"location":"operations/comanage-sop/#revoking-ap-login-access","text":"Login access to AP1 (PATh Facility) and AP40 (OSPool) is controlled by membership to COManage groups. To revoke a user's login access to either of these APs, perform the following steps: Find the corresponding user in COManage and revoke access to all OSG services or just the relevant AP: If you are revoking access to all OSG services, set the user's CO Person status to Suspended If you only need to revoke access to AP1 or AP40, remove the user from the ap1-login or ap40-login group, respectively Note the OSG Username identifier of the user On the AP host(s) where you are revoking access, clear the SSSD cache as root: root@ap-host # sss_cache -u <OSG Username> Replacing <OSG Username> with the OSG Username identifier that you noted in step (2)","title":"Revoking AP login access"},{"location":"operations/comanage-troubleshooting-guide/","text":"COManage Troubleshooting Guide \u00b6 A resource for COManage administrators to find solutions to commonly encountered problems. COManage Troubleshooting Items \u00b6 This section contains some of the issues you may encounter when interacting with the OSG COManage. The COManage petition is stuck in the \"confirmed\" state \u00b6 This may happen if there are issues when confirming the user's email address. We have seen this occur if a user clicks the confirmation link then closes the tab too quickly. Under the People drop-down on the left, click on My Population Browse to the CO Person record. Scroll down to the Role Attributes , click the gear icon, select Edit , and set the status for the Role to Active . Verify that the overall status of the CO Person record is Active . If not, change it to Active as well. Click on Autogenerate Identifiers on the right, so that the necessary identifiers are created. Now that the necessary identifiers exist for the CO Person record, the LDAP DN can be computed and the record provisioned in LDAP. To make sure, click on Provisioned Services and then Provision . The COManage petition is stuck in the \"Approved\" state \u00b6 This may happen when the approver has internet issues while attempting to approve a CO Petition. Navigate to the Petitioner's CO Person page (this can be done directly from the CO Petition under the Attched Identities section). If the CO Person is missing identifiers of types UID and OSG ID , click Autogenerate Identifiers on the right side of the page. If, on the CO Person page, the CO Person's status and Role status are both Approved : In the Status dropdown in the Person Attributes section, change the CO Person's status to Active from Approved . Click on the gear icon for the CO Person's Role attribute, then click \"Edit\", this will bring you to the CO Person Role page On the CO Person Role page, set the Role Attribute Status to Active from Approved , then click save. To verify that the fix worked, navigate back to the related CO Person page and click Provisioned Services near the Autogenerate Identifiers . Confirm that the Status column for every row says \"Provisioned\" instead of \"Not Provisioned\" (you may need to wait a few minutes and refresh the page). Valid Tiger User is unauthorized with Dex credentials \u00b6 Under certain circumstances Dex may not be able to match a user to their COManage identity, possibly due to incorrect and/or missing organization identities/identifiers. Symptoms \u00b6 Tiger-Dex credentials missing some or all of a users group permissions. User unable to connect to the Tiger cluster. Error message from Tiger involving an unauthorized user. Example error message: kubectl get pods -n osg-dev Error from server (Forbidden): pods is forbidden: User \"user@email.edu\" cannot list resource \"pods\" in API group \"\" in the namespace \"osg-dev\"\" Empty groups in the Dex response after logging in with SSO Next actions \u00b6 Ensure that the user's LDAP record contains the attribute uid , which shares the same value as the COManage identifier used as a source for the LDAP provisioner target. LDAP attributes and their COManage sources can be found here Have the user log in Have the user fetch a new Dex token Explanation of SAML -> CILogon OIDC sub claim -> LDAP matching used in Dex. \u00b6 How the OAuth2 server does the following: Receives appropriate SAML attributes from the campus Identity provider. Uses those attributes to find the user in the CILogon user database, from which the CILogon OIDC sub claim is obtained. Performs an LDAP search using a filter of (uid=<CILogon OIDC sub claim value>) . Retrieves configured LDAP attributes from the record it has found, and sends those values out as claims with names as configured.","title":"COmanage Troubleshooting Guide"},{"location":"operations/comanage-troubleshooting-guide/#comanage-troubleshooting-guide","text":"A resource for COManage administrators to find solutions to commonly encountered problems.","title":"COManage Troubleshooting Guide"},{"location":"operations/comanage-troubleshooting-guide/#comanage-troubleshooting-items","text":"This section contains some of the issues you may encounter when interacting with the OSG COManage.","title":"COManage Troubleshooting Items"},{"location":"operations/comanage-troubleshooting-guide/#the-comanage-petition-is-stuck-in-the-confirmed-state","text":"This may happen if there are issues when confirming the user's email address. We have seen this occur if a user clicks the confirmation link then closes the tab too quickly. Under the People drop-down on the left, click on My Population Browse to the CO Person record. Scroll down to the Role Attributes , click the gear icon, select Edit , and set the status for the Role to Active . Verify that the overall status of the CO Person record is Active . If not, change it to Active as well. Click on Autogenerate Identifiers on the right, so that the necessary identifiers are created. Now that the necessary identifiers exist for the CO Person record, the LDAP DN can be computed and the record provisioned in LDAP. To make sure, click on Provisioned Services and then Provision .","title":"The COManage petition is stuck in the \"confirmed\" state"},{"location":"operations/comanage-troubleshooting-guide/#the-comanage-petition-is-stuck-in-the-approved-state","text":"This may happen when the approver has internet issues while attempting to approve a CO Petition. Navigate to the Petitioner's CO Person page (this can be done directly from the CO Petition under the Attched Identities section). If the CO Person is missing identifiers of types UID and OSG ID , click Autogenerate Identifiers on the right side of the page. If, on the CO Person page, the CO Person's status and Role status are both Approved : In the Status dropdown in the Person Attributes section, change the CO Person's status to Active from Approved . Click on the gear icon for the CO Person's Role attribute, then click \"Edit\", this will bring you to the CO Person Role page On the CO Person Role page, set the Role Attribute Status to Active from Approved , then click save. To verify that the fix worked, navigate back to the related CO Person page and click Provisioned Services near the Autogenerate Identifiers . Confirm that the Status column for every row says \"Provisioned\" instead of \"Not Provisioned\" (you may need to wait a few minutes and refresh the page).","title":"The COManage petition is stuck in the \"Approved\" state"},{"location":"operations/comanage-troubleshooting-guide/#valid-tiger-user-is-unauthorized-with-dex-credentials","text":"Under certain circumstances Dex may not be able to match a user to their COManage identity, possibly due to incorrect and/or missing organization identities/identifiers.","title":"Valid Tiger User is unauthorized with Dex credentials"},{"location":"operations/comanage-troubleshooting-guide/#symptoms","text":"Tiger-Dex credentials missing some or all of a users group permissions. User unable to connect to the Tiger cluster. Error message from Tiger involving an unauthorized user. Example error message: kubectl get pods -n osg-dev Error from server (Forbidden): pods is forbidden: User \"user@email.edu\" cannot list resource \"pods\" in API group \"\" in the namespace \"osg-dev\"\" Empty groups in the Dex response after logging in with SSO","title":"Symptoms"},{"location":"operations/comanage-troubleshooting-guide/#next-actions","text":"Ensure that the user's LDAP record contains the attribute uid , which shares the same value as the COManage identifier used as a source for the LDAP provisioner target. LDAP attributes and their COManage sources can be found here Have the user log in Have the user fetch a new Dex token","title":"Next actions"},{"location":"operations/comanage-troubleshooting-guide/#explanation-of-saml-cilogon-oidc-sub-claim-ldap-matching-used-in-dex","text":"How the OAuth2 server does the following: Receives appropriate SAML attributes from the campus Identity provider. Uses those attributes to find the user in the CILogon user database, from which the CILogon OIDC sub claim is obtained. Performs an LDAP search using a filter of (uid=<CILogon OIDC sub claim value>) . Retrieves configured LDAP attributes from the record it has found, and sends those values out as claims with names as configured.","title":"Explanation of SAML -&gt; CILogon OIDC sub claim -&gt; LDAP matching used in Dex."},{"location":"policy/bestman2-retire/","text":"BeStMan2 Retirement \u00b6 This document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack. Introduction \u00b6 BeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol. SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\" Additionally, SRM can be used to mediate transfer protocol selection. OSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers. This functionality is redundant to what can be accomplished with GridFTP alone. BeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize. OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents. We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan. Site Transition Plans \u00b6 We have released documentation for a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple GridFTP endpoints. Sites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2. Timeline \u00b6 March 2017 (completed): Release load balanced GridFTP documentation June 2017 (completed): OSG 3.4.0 is released without BeStMan December 2018 (completed): Security-only support for OSG 3.3 series and BeStMan is provided May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.","title":"BeStMan2 Retirement"},{"location":"policy/bestman2-retire/#bestman2-retirement","text":"This document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack.","title":"BeStMan2 Retirement"},{"location":"policy/bestman2-retire/#introduction","text":"BeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol. SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\" Additionally, SRM can be used to mediate transfer protocol selection. OSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers. This functionality is redundant to what can be accomplished with GridFTP alone. BeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize. OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents. We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan.","title":"Introduction"},{"location":"policy/bestman2-retire/#site-transition-plans","text":"We have released documentation for a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple GridFTP endpoints. Sites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2.","title":"Site Transition Plans"},{"location":"policy/bestman2-retire/#timeline","text":"March 2017 (completed): Release load balanced GridFTP documentation June 2017 (completed): OSG 3.4.0 is released without BeStMan December 2018 (completed): Security-only support for OSG 3.3 series and BeStMan is provided May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.","title":"Timeline"},{"location":"policy/campus-cyberinfrastructure/","text":"The OSG and NSF Campus Cyberinfrastructure \u00b6 The NSF Campus Cyberinfrastructure (CC*) program invests in coordinated campus-level cyberinfrastructure improvements, innovation, integration, and engineering for science applications and distributed research projects, including enhancements to campus networking and computing resources. The Open Science Grid (as part of the Partnership to Advance Throughput Computing (PATh) ), is here to help you with your Campus Cyberinfrastructure (CC*) proposal! Please contact us at cc-star-proposals@osg-htc.org We have significant experience working with CC* applicants and awardees, offering letters of collaboration and consulting for: bringing the power of the OSG to YOUR researchers gathering science drivers and planning local computing resources CC*-required resource sharing for the Campus Compute category*, and other options for integrating local resources into OSG *In the most recent call for proposals ( NSF 21-528 ), joining the OSG is mentioned as a potential path to sharing resources with the wider research community: Proposals are required to commit to a minimum of 20% shared time on the cluster and describe their approach to making the cluster available as a shared resource external to the campus, [...] One possible approach to implementing such a federated distributed computing solution is joining a multi-campus or national federated system such as the Open Science Grid. Sharing Resources via the OSG \u00b6 The OSG consortium provides standard services and support for computational resource providers (i.e., \"sites\") using a distributed fabric of high throughput computating (HTC) technologies. These distributed-HTC (dHTC) services communicate with the site's local resource management (e.g. \"queueing\") systems to provision resources for OSG users. The OSG itself does not own resources, but provides software and services that enable the sharing of resources by many sites, and enable users to take advantage of these from submission points (whether via an OSG-operated submission point, like OSG Connect , or a locally-managed one). To contribute computational resources to the OSG, the following will be needed: An existing compute cluster running on a supported operating system with a supported resource management system: Grid Engine , HTCondor , LSF , PBS Pro / Torque , Slurm , and some local cloud provisioners. Outbound network connectivity from the cluster's worker nodes SSH access to your local cluster's submit node from a known IP address Temporary scratch space on each worker node and shared home directories on each cluster node Installation of some additional packages on the local cluster, IF the site would like to maximize its ability to support users, including those with large per-job data, containerized software, and/or GPU jobs. (There ARE some exceptions to the above. Contact us to discuss them!) Next steps If you are interested in OSG-offered services, please contact us for a consultation, even if your site does not meet all the conditions as outlined above! Additional Materials \u00b6 If you are interested in learning more about the dHTC, OSG, and what it means to share resources via OSG services, consider reviewing the following presentations from our October 2020 workshop on dHTC and OSG services for campuses ( YouTube Playlist ): Intro to dHTC and PATh Services for Campuses ( YouTube ) How OSG Works ( YouTube ) Intro to OSG Resource Sharing ( YouTube ) Resource Sharing Technology, Security, System Requirements, Setup Process ( YouTube )","title":"Campus Cyberinfrastructure"},{"location":"policy/campus-cyberinfrastructure/#the-osg-and-nsf-campus-cyberinfrastructure","text":"The NSF Campus Cyberinfrastructure (CC*) program invests in coordinated campus-level cyberinfrastructure improvements, innovation, integration, and engineering for science applications and distributed research projects, including enhancements to campus networking and computing resources. The Open Science Grid (as part of the Partnership to Advance Throughput Computing (PATh) ), is here to help you with your Campus Cyberinfrastructure (CC*) proposal! Please contact us at cc-star-proposals@osg-htc.org We have significant experience working with CC* applicants and awardees, offering letters of collaboration and consulting for: bringing the power of the OSG to YOUR researchers gathering science drivers and planning local computing resources CC*-required resource sharing for the Campus Compute category*, and other options for integrating local resources into OSG *In the most recent call for proposals ( NSF 21-528 ), joining the OSG is mentioned as a potential path to sharing resources with the wider research community: Proposals are required to commit to a minimum of 20% shared time on the cluster and describe their approach to making the cluster available as a shared resource external to the campus, [...] One possible approach to implementing such a federated distributed computing solution is joining a multi-campus or national federated system such as the Open Science Grid.","title":"The OSG and NSF Campus Cyberinfrastructure"},{"location":"policy/campus-cyberinfrastructure/#sharing-resources-via-the-osg","text":"The OSG consortium provides standard services and support for computational resource providers (i.e., \"sites\") using a distributed fabric of high throughput computating (HTC) technologies. These distributed-HTC (dHTC) services communicate with the site's local resource management (e.g. \"queueing\") systems to provision resources for OSG users. The OSG itself does not own resources, but provides software and services that enable the sharing of resources by many sites, and enable users to take advantage of these from submission points (whether via an OSG-operated submission point, like OSG Connect , or a locally-managed one). To contribute computational resources to the OSG, the following will be needed: An existing compute cluster running on a supported operating system with a supported resource management system: Grid Engine , HTCondor , LSF , PBS Pro / Torque , Slurm , and some local cloud provisioners. Outbound network connectivity from the cluster's worker nodes SSH access to your local cluster's submit node from a known IP address Temporary scratch space on each worker node and shared home directories on each cluster node Installation of some additional packages on the local cluster, IF the site would like to maximize its ability to support users, including those with large per-job data, containerized software, and/or GPU jobs. (There ARE some exceptions to the above. Contact us to discuss them!) Next steps If you are interested in OSG-offered services, please contact us for a consultation, even if your site does not meet all the conditions as outlined above!","title":"Sharing Resources via the OSG"},{"location":"policy/campus-cyberinfrastructure/#additional-materials","text":"If you are interested in learning more about the dHTC, OSG, and what it means to share resources via OSG services, consider reviewing the following presentations from our October 2020 workshop on dHTC and OSG services for campuses ( YouTube Playlist ): Intro to dHTC and PATh Services for Campuses ( YouTube ) How OSG Works ( YouTube ) Intro to OSG Resource Sharing ( YouTube ) Resource Sharing Technology, Security, System Requirements, Setup Process ( YouTube )","title":"Additional Materials"},{"location":"policy/collab-bearer-tokens/","text":"Collaborations and Bearer Tokens \u00b6 Sites in the OSG grant access to their grid services based on client's association with a specific collaboration (i.e. VO) instead of granting access on a per-user basis. In the past, this type of access was provided through X.509 proxies with VOMS attributes to demonstrate assocation with a collaboration: Users (both human and robots) would request that their collaboration sign their X.509 proxies (usually through a VOMS server), Or in the case of automated services (i.e. pilot job submission) a collaboration could directly create and sign proxies themselves Now, sites can authenticate and authorize clients presenting bearer tokens, such as SciTokens or WLCG tokens. This document describes how collaborations can issue bearer tokens for the aforementioned use cases in ways that are compatible with OSG sites. Issuers \u00b6 To generate bearer tokens, a collaboration must adminster at least one \"token issuer\" to issue tokens to their users. In addition to generating and signing tokens, token issuers provide a public endpoint that can be used to validate an issued token, e.g. an OSG Compute Entrypoint (CE) will contact the token issuer to authorize a bearer token used for pilot job submission. Token issuer uptime Due to the centralized nature of bearer token validation, token issuers should be treated as critical, highly available services. Otherwise, a token issuer outage will result in OSG sites being unable to authenticate a collaboration's tokens, meaning an interruption in pilot job submission and authenticated data transfers. Choose one of the token issuer types below, depending on the needs of your collaboration. Simple issuer \u00b6 If your collaboration centrally administers all services requiring bearer tokens and your users do not need to directly manage bearer tokens, consider running a simple token issuer. A simple token issuer consists of a public/private certificate keypair where the private key is used to issue tokens directly and the public certificate is made available through a web server. For example, the OSPool (n\u00e9e OSG VO) serves its public certificate through GitHub pages and uses the private key to sign tokens used for pilot job submission as well as automatically generating tokens to accompany user jobs so that they can access their private storage areas. OAuth2/OpenID Connect \u00b6 If your collaboration distributes administrative responsibility or your users need to request and manage their own tokens, you should administer an OAuth2/OpenID Connect (OIDC) service (e.g., INDIGO IAM ) or work with an existing OAuth2/OIDC provider (e.g., CILogon ). For example, Fermilab uses CILogon as their OIDC provider combined with an HTVault server used to streamline the OIDC process for users and integrate with their local Kerberos. Claims \u00b6 Bearer tokens are self-describing credentials that enumerate their capabilities as \"claims\" and different token \"profiles\" enumerate common sets of claims. OSG sites support the following bearer token profiles: SciTokens WLCG tokens Claims are further described below with recommendations to ensure the greatest compatibility with OSG sites. Scope \u00b6 The scope claim is a space-separated list of authorizations that should be granted to the bearer. Scopes utilized by OSG services include the following: Capability SciTokens scope WLCG scope HTCondor READ condor:/READ compute.read HTCondor WRITE condor:/WRITE compute.modify compute.cancel compute.create XRootD read read:<PATH> storage.read:<PATH> XRootD write write:<PATH> storage.modify:<PATH> Replacing <PATH> with a path to the storage location that the bearer should be authorized to access. Issuer \u00b6 The issuer URL, or the iss claim, indicates the endpoint to use for authenticating a given token. A collaboration may have more than one token issuer but a single token issuer should never serve more than one collaboration. In other words, given the token issuer, the site can determine the collaboration that issued the token. The following collaborations have registered token issuers with the OSG: Collaboration Issuers ATLAS https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/ATLAS.yaml CLAS12 https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/CLAS12.yaml CMS https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/CMS.yaml DES https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/DES.yaml DUNE https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/DUNE.yaml EIC https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/EIC.yaml Fermilab https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/Fermilab.yaml CHTC (n\u00e9e GLOW) https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/GLOW.yaml Gluex https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/Gluex.yaml IceCube https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/IceCube.yaml LIGO https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/LIGO.yaml Mu2e https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/Mu2e.yaml OSPool https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/OSG.yaml SBND https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/SBND.yaml gm2 https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/gm2.yaml The OSG distributes the osg-scitokens-mapfile RPM package that includes issuer and subject to default user mappings for use by OSG CEs. Subject \u00b6 Subjects (i.e., the sub claim) should be unique, stable identifiers that correspond to a user or service (e.g. pilot job submission). In other words, subjects combined with a token issuer can be used for suspending access for a given collaboration user, user-level accounting, monitoring, auditing, or tracing. In tandem with a token issuer URL (i.e., the iss claim), subjects can be used by site HTCondor-CE or XRootD services to map to a local identity. Privacy considerations Depending on your collaboration's userbase and contributing sites, you may have to take privacy concerns, such as the GDPR into account when assigning subjects to users. Thus, it may be preferable to assign users a randomly-generated string as their sub . Audience \u00b6 To take advantage of the improved security posture of bearer tokens, we recommend that the aud claim be set to the intended host. For example, tokens used for submission to an HTCondor-CE should set the following: aud = <CE FQDN>:<CE PORT> WLCG groups \u00b6 WLCG tokens may have the wlcg.groups claim consisting of a comma and space separated list of collaboration groups. The format of these groups are similar to VOMS FQANs: /<collaboration>[/<group>][/Role=<role>] , replacing <collaboration> , <group> , and <role> with the collaboration, group, and role, respectively, where the group and role are optional. For example, the following groups and roles have been used by the ATLAS and CMS collaborations: /atlas/ /atlas/usatlas /cms/Role=pilot /cms/local/Role=pilot Traditionally, sites have made local accounting and scheduling decisions based on the first VOMS FQAN so collaborations should set the first group/role in wlcg.groups to the most specific group or role. For example: wlcg.groups = /cms/Role=pilot, /cms Instead of: wlcg.groups = /cms, /cms/Role=pilot Help \u00b6 To get assistance, please use the this page .","title":"Collaborations and Bearer Tokens"},{"location":"policy/collab-bearer-tokens/#collaborations-and-bearer-tokens","text":"Sites in the OSG grant access to their grid services based on client's association with a specific collaboration (i.e. VO) instead of granting access on a per-user basis. In the past, this type of access was provided through X.509 proxies with VOMS attributes to demonstrate assocation with a collaboration: Users (both human and robots) would request that their collaboration sign their X.509 proxies (usually through a VOMS server), Or in the case of automated services (i.e. pilot job submission) a collaboration could directly create and sign proxies themselves Now, sites can authenticate and authorize clients presenting bearer tokens, such as SciTokens or WLCG tokens. This document describes how collaborations can issue bearer tokens for the aforementioned use cases in ways that are compatible with OSG sites.","title":"Collaborations and Bearer Tokens"},{"location":"policy/collab-bearer-tokens/#issuers","text":"To generate bearer tokens, a collaboration must adminster at least one \"token issuer\" to issue tokens to their users. In addition to generating and signing tokens, token issuers provide a public endpoint that can be used to validate an issued token, e.g. an OSG Compute Entrypoint (CE) will contact the token issuer to authorize a bearer token used for pilot job submission. Token issuer uptime Due to the centralized nature of bearer token validation, token issuers should be treated as critical, highly available services. Otherwise, a token issuer outage will result in OSG sites being unable to authenticate a collaboration's tokens, meaning an interruption in pilot job submission and authenticated data transfers. Choose one of the token issuer types below, depending on the needs of your collaboration.","title":"Issuers"},{"location":"policy/collab-bearer-tokens/#simple-issuer","text":"If your collaboration centrally administers all services requiring bearer tokens and your users do not need to directly manage bearer tokens, consider running a simple token issuer. A simple token issuer consists of a public/private certificate keypair where the private key is used to issue tokens directly and the public certificate is made available through a web server. For example, the OSPool (n\u00e9e OSG VO) serves its public certificate through GitHub pages and uses the private key to sign tokens used for pilot job submission as well as automatically generating tokens to accompany user jobs so that they can access their private storage areas.","title":"Simple issuer"},{"location":"policy/collab-bearer-tokens/#oauth2openid-connect","text":"If your collaboration distributes administrative responsibility or your users need to request and manage their own tokens, you should administer an OAuth2/OpenID Connect (OIDC) service (e.g., INDIGO IAM ) or work with an existing OAuth2/OIDC provider (e.g., CILogon ). For example, Fermilab uses CILogon as their OIDC provider combined with an HTVault server used to streamline the OIDC process for users and integrate with their local Kerberos.","title":"OAuth2/OpenID Connect"},{"location":"policy/collab-bearer-tokens/#claims","text":"Bearer tokens are self-describing credentials that enumerate their capabilities as \"claims\" and different token \"profiles\" enumerate common sets of claims. OSG sites support the following bearer token profiles: SciTokens WLCG tokens Claims are further described below with recommendations to ensure the greatest compatibility with OSG sites.","title":"Claims"},{"location":"policy/collab-bearer-tokens/#scope","text":"The scope claim is a space-separated list of authorizations that should be granted to the bearer. Scopes utilized by OSG services include the following: Capability SciTokens scope WLCG scope HTCondor READ condor:/READ compute.read HTCondor WRITE condor:/WRITE compute.modify compute.cancel compute.create XRootD read read:<PATH> storage.read:<PATH> XRootD write write:<PATH> storage.modify:<PATH> Replacing <PATH> with a path to the storage location that the bearer should be authorized to access.","title":"Scope"},{"location":"policy/collab-bearer-tokens/#issuer","text":"The issuer URL, or the iss claim, indicates the endpoint to use for authenticating a given token. A collaboration may have more than one token issuer but a single token issuer should never serve more than one collaboration. In other words, given the token issuer, the site can determine the collaboration that issued the token. The following collaborations have registered token issuers with the OSG: Collaboration Issuers ATLAS https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/ATLAS.yaml CLAS12 https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/CLAS12.yaml CMS https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/CMS.yaml DES https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/DES.yaml DUNE https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/DUNE.yaml EIC https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/EIC.yaml Fermilab https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/Fermilab.yaml CHTC (n\u00e9e GLOW) https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/GLOW.yaml Gluex https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/Gluex.yaml IceCube https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/IceCube.yaml LIGO https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/LIGO.yaml Mu2e https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/Mu2e.yaml OSPool https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/OSG.yaml SBND https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/SBND.yaml gm2 https://github.com/opensciencegrid/topology/blob/master/virtual-organizations/gm2.yaml The OSG distributes the osg-scitokens-mapfile RPM package that includes issuer and subject to default user mappings for use by OSG CEs.","title":"Issuer"},{"location":"policy/collab-bearer-tokens/#subject","text":"Subjects (i.e., the sub claim) should be unique, stable identifiers that correspond to a user or service (e.g. pilot job submission). In other words, subjects combined with a token issuer can be used for suspending access for a given collaboration user, user-level accounting, monitoring, auditing, or tracing. In tandem with a token issuer URL (i.e., the iss claim), subjects can be used by site HTCondor-CE or XRootD services to map to a local identity. Privacy considerations Depending on your collaboration's userbase and contributing sites, you may have to take privacy concerns, such as the GDPR into account when assigning subjects to users. Thus, it may be preferable to assign users a randomly-generated string as their sub .","title":"Subject"},{"location":"policy/collab-bearer-tokens/#audience","text":"To take advantage of the improved security posture of bearer tokens, we recommend that the aud claim be set to the intended host. For example, tokens used for submission to an HTCondor-CE should set the following: aud = <CE FQDN>:<CE PORT>","title":"Audience"},{"location":"policy/collab-bearer-tokens/#wlcg-groups","text":"WLCG tokens may have the wlcg.groups claim consisting of a comma and space separated list of collaboration groups. The format of these groups are similar to VOMS FQANs: /<collaboration>[/<group>][/Role=<role>] , replacing <collaboration> , <group> , and <role> with the collaboration, group, and role, respectively, where the group and role are optional. For example, the following groups and roles have been used by the ATLAS and CMS collaborations: /atlas/ /atlas/usatlas /cms/Role=pilot /cms/local/Role=pilot Traditionally, sites have made local accounting and scheduling decisions based on the first VOMS FQAN so collaborations should set the first group/role in wlcg.groups to the most specific group or role. For example: wlcg.groups = /cms/Role=pilot, /cms Instead of: wlcg.groups = /cms, /cms/Role=pilot","title":"WLCG groups"},{"location":"policy/collab-bearer-tokens/#help","text":"To get assistance, please use the this page .","title":"Help"},{"location":"policy/community-testing/","text":"OSG Community Software Testing \u00b6 8 October 2019 The community of OSG resource providers has a vested interest in the quality and stability of the OSG software stack. We would like to notify our stakeholders of software updates as soon as they are designated as \"Ready for Testing\" by the Software Team. Direct engagement with the entire community would allow for feedback from a broader array of interested parties. Combined with our flexible release model , we hope to further improve the turnaround time of new features and bug fixes. Implementation \u00b6 After the OSG Software Team builds and tests a package successfully, it is marked \"Ready for Testing\" and is added to the appropriate Yum testing repository: osg-testing and osg-upcoming-testing for packages targeted for the release and the upcoming release, respectively. Upon addition to the relevant testing repository, we intend to notify OSG site administrators that the package, or a logically connected group of packages, is available for testing with a description of changes compared to previously released versions and provide a forum by which interested users can provide feedback. Additionally, any packages that are considered release candidates by their upstream authors will be noted as such. The Software and Release team will classify packages as either \"major\" or \"minor\"; where major packages are deemed critical to the functionality of the production grid and all other packages are minor. For major packages, we will notify site administrators as soon as they are eligible for testing; minor packages eligible for testing will be collected and announced in a weekly digest. After users have been notified of changes, minor packages will be marked eligible for release if they have not received negative feedback after 7 calendar days. In addition to the above requirements, major packages must also receive positive feedback and be approved by the Release Manager. If a major package has not received feedback after four weeks and it has been sufficiently tested by OSG integration tests, the Release Manager may approve the package for release. If a package receives negative feedback, the offending package will be removed from the relevant testing repository. Major Packages \u00b6 The following packages are considered critical to the production Open Science Grid: CVMFS Frontier Squid GlideinWMS Gratia Probes HTCondor HTCondor-CE stashcp XCache XRootD This list is maintained by the Release Manager with input from OSG stakeholders, the Software Manager, and the Operations Manager. Exceptions \u00b6 If an expedient release is required, the OSG Software Team may forego the community testing policy outlined above. Common exceptions to the policy include releases that contain one or more of the following: Security updates CA or VO data updates Updates that address installation or upgrade issues Version History \u00b6 2022-10-07 : All promoting of lanquishing critical packages when adequately covered by automated tests 2022-05-06 : Drop unsupported software from the critical package list 2019-10-08 : Add policy exceptions 2019-08-12 : Add notification frequency details 2019-02-20 : Initial policy","title":"Community Testing"},{"location":"policy/community-testing/#osg-community-software-testing","text":"8 October 2019 The community of OSG resource providers has a vested interest in the quality and stability of the OSG software stack. We would like to notify our stakeholders of software updates as soon as they are designated as \"Ready for Testing\" by the Software Team. Direct engagement with the entire community would allow for feedback from a broader array of interested parties. Combined with our flexible release model , we hope to further improve the turnaround time of new features and bug fixes.","title":"OSG Community Software Testing"},{"location":"policy/community-testing/#implementation","text":"After the OSG Software Team builds and tests a package successfully, it is marked \"Ready for Testing\" and is added to the appropriate Yum testing repository: osg-testing and osg-upcoming-testing for packages targeted for the release and the upcoming release, respectively. Upon addition to the relevant testing repository, we intend to notify OSG site administrators that the package, or a logically connected group of packages, is available for testing with a description of changes compared to previously released versions and provide a forum by which interested users can provide feedback. Additionally, any packages that are considered release candidates by their upstream authors will be noted as such. The Software and Release team will classify packages as either \"major\" or \"minor\"; where major packages are deemed critical to the functionality of the production grid and all other packages are minor. For major packages, we will notify site administrators as soon as they are eligible for testing; minor packages eligible for testing will be collected and announced in a weekly digest. After users have been notified of changes, minor packages will be marked eligible for release if they have not received negative feedback after 7 calendar days. In addition to the above requirements, major packages must also receive positive feedback and be approved by the Release Manager. If a major package has not received feedback after four weeks and it has been sufficiently tested by OSG integration tests, the Release Manager may approve the package for release. If a package receives negative feedback, the offending package will be removed from the relevant testing repository.","title":"Implementation"},{"location":"policy/community-testing/#major-packages","text":"The following packages are considered critical to the production Open Science Grid: CVMFS Frontier Squid GlideinWMS Gratia Probes HTCondor HTCondor-CE stashcp XCache XRootD This list is maintained by the Release Manager with input from OSG stakeholders, the Software Manager, and the Operations Manager.","title":"Major Packages"},{"location":"policy/community-testing/#exceptions","text":"If an expedient release is required, the OSG Software Team may forego the community testing policy outlined above. Common exceptions to the policy include releases that contain one or more of the following: Security updates CA or VO data updates Updates that address installation or upgrade issues","title":"Exceptions"},{"location":"policy/community-testing/#version-history","text":"2022-10-07 : All promoting of lanquishing critical packages when adequately covered by automated tests 2022-05-06 : Drop unsupported software from the critical package list 2019-10-08 : Add policy exceptions 2019-08-12 : Add notification frequency details 2019-02-20 : Initial policy","title":"Version History"},{"location":"policy/container-release/","text":"Container Release Policy \u00b6 16 February 2022 Container images are an increasingly popular tool for shortening the software development life cycle, allowing for speedy deployment of new software versions or additional instances of a service. Select services in the OSG Software Stack will be distributed as container images to support VOs and sites that are interested in this model. This document contains policy information for container images distributed by the OSG Software Team. Contents and Sources \u00b6 Similar to our existing RPM infrastructure, container image sources, build logs, and artifacts will be stored in publicly available repositories (e.g. GitHub, Docker Hub) for collaboration and traceability. Additionally, container images distributed by the OSG Software team will be based off of the latest version of a supported platform with software installed from OS, EPEL, and OSG Yum repositories. Tags \u00b6 OSG Software container images will be built at least weekly and tagged with the following format: <SERIES>-<REPO>[-<TIME>] Field Description <SERIES> The OSG release series used for software installation. Possible values: 3.6 and 3.5 . <REPO> OSG Yum repositories used for software installation, including the corresponding upcoming repository. Possible values: release and testing . <TIME> The time that the image was built, in the format YYYYMMDD-HHMM; see below for an example. OSG Software Release Series life cycle Container images based on unsupported OSG release series will stop receiving regular updates. Immutable vs mutable tags Image tags without a build time are treated as mutable, i.e. these tags are regularly updated with the latest available software in their respective Yum repositories. Image tags with a build time are treated as immutable and do not change. For example, to deploy an Open Science Data Federation cache with the latest production software versions from OSG 3.6, use the following image tag: opensciencegrid/stash-cache:3.6-release However, to deploy a cache with software that was available in the osg-testing and osg-upcoming-testing repositories at 3:17 PM on December 17, 2021, use the following image tag: opensciencegrid/stash-cache:3.6-testing-20211217-1517 Deprecated \u00b6 Images based off of OSG 3.5 originally did not have the release series prefix. The following tags will no longer be supported after the retirement of OSG 3.5 on May 1, 2022: release-<TIME> release testing-<TIME> testing Where <TIME> is the time that the tag was built. See this page for more details on release series support. Retention \u00b6 Image tags older than 6 months will be automatically removed. Additionally, the Software Team may remove images with detected security flaws. Validation \u00b6 OSG Software container images consist of RPMs for OSG services that are tested through existing release processes as well as scripts and configuration specific to the container implementation of the service. New container images limited to RPM updates undergo additional automated testing before being published. In order to test changes to container-specific scripts or configuration, OSG Software performs automated tests and coordinates testing of release candidate images before applying these changes to the production tags . Change Log \u00b6 21 April 2022: Deprecate tags without the OSG release series 16 February 2022: Remove Docker Hub dependency from the retention policy. 22 January 2021: Modify the tagging policy to more closely track OSG Yum repositories 14 August 2020: Updated cleanup policy to match Docker Hub image retention policy. 17 April 2019: Initial policy","title":"Container Release Policy"},{"location":"policy/container-release/#container-release-policy","text":"16 February 2022 Container images are an increasingly popular tool for shortening the software development life cycle, allowing for speedy deployment of new software versions or additional instances of a service. Select services in the OSG Software Stack will be distributed as container images to support VOs and sites that are interested in this model. This document contains policy information for container images distributed by the OSG Software Team.","title":"Container Release Policy"},{"location":"policy/container-release/#contents-and-sources","text":"Similar to our existing RPM infrastructure, container image sources, build logs, and artifacts will be stored in publicly available repositories (e.g. GitHub, Docker Hub) for collaboration and traceability. Additionally, container images distributed by the OSG Software team will be based off of the latest version of a supported platform with software installed from OS, EPEL, and OSG Yum repositories.","title":"Contents and Sources"},{"location":"policy/container-release/#tags","text":"OSG Software container images will be built at least weekly and tagged with the following format: <SERIES>-<REPO>[-<TIME>] Field Description <SERIES> The OSG release series used for software installation. Possible values: 3.6 and 3.5 . <REPO> OSG Yum repositories used for software installation, including the corresponding upcoming repository. Possible values: release and testing . <TIME> The time that the image was built, in the format YYYYMMDD-HHMM; see below for an example. OSG Software Release Series life cycle Container images based on unsupported OSG release series will stop receiving regular updates. Immutable vs mutable tags Image tags without a build time are treated as mutable, i.e. these tags are regularly updated with the latest available software in their respective Yum repositories. Image tags with a build time are treated as immutable and do not change. For example, to deploy an Open Science Data Federation cache with the latest production software versions from OSG 3.6, use the following image tag: opensciencegrid/stash-cache:3.6-release However, to deploy a cache with software that was available in the osg-testing and osg-upcoming-testing repositories at 3:17 PM on December 17, 2021, use the following image tag: opensciencegrid/stash-cache:3.6-testing-20211217-1517","title":"Tags"},{"location":"policy/container-release/#deprecated","text":"Images based off of OSG 3.5 originally did not have the release series prefix. The following tags will no longer be supported after the retirement of OSG 3.5 on May 1, 2022: release-<TIME> release testing-<TIME> testing Where <TIME> is the time that the tag was built. See this page for more details on release series support.","title":"Deprecated"},{"location":"policy/container-release/#retention","text":"Image tags older than 6 months will be automatically removed. Additionally, the Software Team may remove images with detected security flaws.","title":"Retention"},{"location":"policy/container-release/#validation","text":"OSG Software container images consist of RPMs for OSG services that are tested through existing release processes as well as scripts and configuration specific to the container implementation of the service. New container images limited to RPM updates undergo additional automated testing before being published. In order to test changes to container-specific scripts or configuration, OSG Software performs automated tests and coordinates testing of release candidate images before applying these changes to the production tags .","title":"Validation"},{"location":"policy/container-release/#change-log","text":"21 April 2022: Deprecate tags without the OSG release series 16 February 2022: Remove Docker Hub dependency from the retention policy. 22 January 2021: Modify the tagging policy to more closely track OSG Yum repositories 14 August 2020: Updated cleanup policy to match Docker Hub image retention policy. 17 April 2019: Initial policy","title":"Change Log"},{"location":"policy/cream-support/","text":"OSG/HTCondor CREAM-CE Support \u00b6 The CREAM working group has recently announced official support for the CREAM-CE will cease in December 2020. With this email, we are soliciting feedback on OSG and HTCondor\u2019s transition plan. OSG and HTCondor remain committed to supporting VOs who need to access to CREAM-CE based resources throughout the transition period; we will continue to support submission to CREAM-CE endpoints and offer assistance to VOs to manage the transition. OSG runs a glidein submission service that submits to grid infrastructures worldwide on behalf of dozens of science projects. This service currently submits to approximately 100 CREAM-CE endpoints; we will continue to maintain the capability to access these endpoints while we assist sites in testing and enabling ARC-CE and/or HTCondor-CE replacement services. The HTCondor team plans to support the CREAM-CE on EL6/7 in the 8.8.x stable release series and will maintain support for 8.8.x through December 2020. CREAM-CE support will remain enabled at the start of the 8.9.x developer series; in early 2020, the HTCondor team will re-evaluate, based on community need, whether CREAM-CE support will be available in the next stable series. We realize that software retirements can be very disruptive; the OSG and HTCondor teams are committed to assisting user communities through the process. If you believe our plan does not fit the needs of your user community, please contact help@osg-htc.org . Regards, Brian Bockelman, OSG Technology Area Coordinator Todd Tannenbaum, HTCondor Technical Lead Reference \u00b6 EGI broadcast about CREAM retirement ; the EGI link requires authentication; statement has been reproduced below: Dear Users The CREAM working group has announced that official support for the CREAM-CE component will cease at the end of the EOSC-hub project, i.e. in Dec 2020. To prepare for this, EGI Foundation and CERN are actively working to help to minimise disruption. This will include helping users migrate to alternative solutions, i.e. ARC-CE or HTCondor-CE. The CREAM working group will be providing full support until the end of 2019, including one minor release already scheduled. During 2020 only security updates will be released. If you have any concerns or queries, please open a support ticket at https://ggus.eu/ Best regards EGI Foundation and CERN Operations Teams","title":"CREAM-CE Support"},{"location":"policy/cream-support/#osghtcondor-cream-ce-support","text":"The CREAM working group has recently announced official support for the CREAM-CE will cease in December 2020. With this email, we are soliciting feedback on OSG and HTCondor\u2019s transition plan. OSG and HTCondor remain committed to supporting VOs who need to access to CREAM-CE based resources throughout the transition period; we will continue to support submission to CREAM-CE endpoints and offer assistance to VOs to manage the transition. OSG runs a glidein submission service that submits to grid infrastructures worldwide on behalf of dozens of science projects. This service currently submits to approximately 100 CREAM-CE endpoints; we will continue to maintain the capability to access these endpoints while we assist sites in testing and enabling ARC-CE and/or HTCondor-CE replacement services. The HTCondor team plans to support the CREAM-CE on EL6/7 in the 8.8.x stable release series and will maintain support for 8.8.x through December 2020. CREAM-CE support will remain enabled at the start of the 8.9.x developer series; in early 2020, the HTCondor team will re-evaluate, based on community need, whether CREAM-CE support will be available in the next stable series. We realize that software retirements can be very disruptive; the OSG and HTCondor teams are committed to assisting user communities through the process. If you believe our plan does not fit the needs of your user community, please contact help@osg-htc.org . Regards, Brian Bockelman, OSG Technology Area Coordinator Todd Tannenbaum, HTCondor Technical Lead","title":"OSG/HTCondor CREAM-CE Support"},{"location":"policy/cream-support/#reference","text":"EGI broadcast about CREAM retirement ; the EGI link requires authentication; statement has been reproduced below: Dear Users The CREAM working group has announced that official support for the CREAM-CE component will cease at the end of the EOSC-hub project, i.e. in Dec 2020. To prepare for this, EGI Foundation and CERN are actively working to help to minimise disruption. This will include helping users migrate to alternative solutions, i.e. ARC-CE or HTCondor-CE. The CREAM working group will be providing full support until the end of 2019, including one minor release already scheduled. During 2020 only security updates will be released. If you have any concerns or queries, please open a support ticket at https://ggus.eu/ Best regards EGI Foundation and CERN Operations Teams","title":"Reference"},{"location":"policy/external-oasis-repos/","text":"Policy for OSG Mirroring of External CVMFS repositories \u00b6 12 October 2017 This document provides an overview of the policies and security understanding with regards to OSG mirroring of CVMFS repositories of external organizations. It aims to help external repositories and OSG VOs understand what OSG is attempting to achieve with the mirroring service. This is not a service-level agreement but rather a statement of responsibilities. Note To actually understand the technical procedure for mirroring a repository, see the following page . This document solely covers the policy aspects. Introduction \u00b6 The OSG provides a network of CVMFS Stratum-1 servers for mirroring content of externally-managed repositories. These repositories are often hosted by large HEP or physics VOs for the purpose of distributing software for high-throughput computing jobs. Additionally, OSG provides a repository ( ) for smaller VOs; this is not covered here. OSG will include additional repositories into the content distribution network (CDN) at the request of an OSG-affiliated VO. These repositories are meant to help the OSG-affiliated VO accomplish their domain science. The goal of this mirroring provides an improved quality-of-service for the VO end-users running at OSG sites. OSG does not provide support for use of the software in external repositories, but will help end-users contact the VO for help as necessary. OSG Responsibilities \u00b6 OSG will provide the Stratum-1 server network according to the OASIS SLA OSG will provide a best-effort mirror of the full contents of the external repo. We will attempt to provide best-effort integrity of the object contents, but assume users of the Stratum-1 will do further integrity checking. No SLA is provided covering potential data corruptions. OSG will provide best-effort notification to the mirrored repository in case OSG detects a service outage of the external repo. In the event of a security incident, the operations group will replace the compromised repository with an empty directory, signed by the key managed by them. This will be done in consultation with the security team or, in the unlikely event they cannot be reached, at the discretion of the Operations Coordinator. Once the external repository is approved, OSG will distribute the corresponding repository signing keys in a valid whitelist. The whitelist will be signed by the OSG Stratum-0. This whitelist attests to the authenticity of the key, but not a statement about repository contents. VO Responsibilities \u00b6 The individual responsible on behalf of the VO will be registered with the OASIS Manager role in OIM. The requesting VO should only include targeted repositories they need to support their science. The VO should understand that in the event of a reported security incident, the contents of this repository may be replaced with an empty directory and signed by the OSG repository key. Depending on the OSG Security team's evaluation of the severity and urgency of the incident, the blanking may be done immediately without VO notification or after some notification period. In the case of a security incident, the VO and OSG Security team will need to mutually agree that the incident is resolved before the repository is unblanked. The VO is ultimately responsible for the contents of the repository. OSG provides a mirror. If the external repository is not operated by the VO, OSG may work directly with the external repository maintainers. This is done for ease of operations and may be limited to day-to-day, non-security-related support. Operational Policies \u00b6 To help us provide the best operational setup possible, we have a few additional replication policies: OSG Operations only hosts the shared oasis.opensciencegrid.org repository; VO-dedicated software respositories (such as nova.opensciencegrid.org for the NoVA VO) should be operated by the VO. VOs are asked to either run their own repository or utilize the shared repository, but not both. There is a finite amount of high-performance storage on the CDN. A minimum of 100 GB per repository is guaranteed. Larger limits may be requested. VOs may ask the OSG to replicate their repositories to the European Grid Infrastructure (EGI); however, this can only be done if the repository name ends in .opensciencegrid.org .","title":"OASIS Repository Mirroring"},{"location":"policy/external-oasis-repos/#policy-for-osg-mirroring-of-external-cvmfs-repositories","text":"12 October 2017 This document provides an overview of the policies and security understanding with regards to OSG mirroring of CVMFS repositories of external organizations. It aims to help external repositories and OSG VOs understand what OSG is attempting to achieve with the mirroring service. This is not a service-level agreement but rather a statement of responsibilities. Note To actually understand the technical procedure for mirroring a repository, see the following page . This document solely covers the policy aspects.","title":"Policy for OSG Mirroring of External CVMFS repositories"},{"location":"policy/external-oasis-repos/#introduction","text":"The OSG provides a network of CVMFS Stratum-1 servers for mirroring content of externally-managed repositories. These repositories are often hosted by large HEP or physics VOs for the purpose of distributing software for high-throughput computing jobs. Additionally, OSG provides a repository ( ) for smaller VOs; this is not covered here. OSG will include additional repositories into the content distribution network (CDN) at the request of an OSG-affiliated VO. These repositories are meant to help the OSG-affiliated VO accomplish their domain science. The goal of this mirroring provides an improved quality-of-service for the VO end-users running at OSG sites. OSG does not provide support for use of the software in external repositories, but will help end-users contact the VO for help as necessary.","title":"Introduction"},{"location":"policy/external-oasis-repos/#osg-responsibilities","text":"OSG will provide the Stratum-1 server network according to the OASIS SLA OSG will provide a best-effort mirror of the full contents of the external repo. We will attempt to provide best-effort integrity of the object contents, but assume users of the Stratum-1 will do further integrity checking. No SLA is provided covering potential data corruptions. OSG will provide best-effort notification to the mirrored repository in case OSG detects a service outage of the external repo. In the event of a security incident, the operations group will replace the compromised repository with an empty directory, signed by the key managed by them. This will be done in consultation with the security team or, in the unlikely event they cannot be reached, at the discretion of the Operations Coordinator. Once the external repository is approved, OSG will distribute the corresponding repository signing keys in a valid whitelist. The whitelist will be signed by the OSG Stratum-0. This whitelist attests to the authenticity of the key, but not a statement about repository contents.","title":"OSG Responsibilities"},{"location":"policy/external-oasis-repos/#vo-responsibilities","text":"The individual responsible on behalf of the VO will be registered with the OASIS Manager role in OIM. The requesting VO should only include targeted repositories they need to support their science. The VO should understand that in the event of a reported security incident, the contents of this repository may be replaced with an empty directory and signed by the OSG repository key. Depending on the OSG Security team's evaluation of the severity and urgency of the incident, the blanking may be done immediately without VO notification or after some notification period. In the case of a security incident, the VO and OSG Security team will need to mutually agree that the incident is resolved before the repository is unblanked. The VO is ultimately responsible for the contents of the repository. OSG provides a mirror. If the external repository is not operated by the VO, OSG may work directly with the external repository maintainers. This is done for ease of operations and may be limited to day-to-day, non-security-related support.","title":"VO Responsibilities"},{"location":"policy/external-oasis-repos/#operational-policies","text":"To help us provide the best operational setup possible, we have a few additional replication policies: OSG Operations only hosts the shared oasis.opensciencegrid.org repository; VO-dedicated software respositories (such as nova.opensciencegrid.org for the NoVA VO) should be operated by the VO. VOs are asked to either run their own repository or utilize the shared repository, but not both. There is a finite amount of high-performance storage on the CDN. A minimum of 100 GB per repository is guaranteed. Larger limits may be requested. VOs may ask the OSG to replicate their repositories to the European Grid Infrastructure (EGI); however, this can only be done if the repository name ends in .opensciencegrid.org .","title":"Operational Policies"},{"location":"policy/flexible-release-model/","text":"OSG Software Flexible Release Model \u00b6 Introduction \u00b6 Before November 2017, the OSG software stack was released on the second Tuesday of each month, except in the case of urgent releases, which were infrequent. This schedule had been in place since early 2013. Since then, conditions within and outside of the Software team have changed, and we have adjusted the release schedule and associated processes. The previous release model had the recurring problem of a \"release crunch,\" where it was difficult to find the effort required to test large changes before their deadline had passed. Sometimes the lack of timely effort led to software being pushed to the next release (a month later), because there was insufficient testing time. Based on software support tickets, we noticed that many sites follow a local update schedule that is independent of the OSG Release team schedule; some sites upgrade every few months, skipping interim releases, other sites upgrade individual packages as needed. In addition, upstream software developers do not follow our release schedule either, releasing software on their own development timelines. As a result, some site administrators would prefer to have OSG software updates more often, closer to when they become available, rather than tied to a monthly cycle. For these reasons, we created a new release model. Release Model \u00b6 The OSG Release team releases batches of integrated, tested software on an ad hoc basis, with the process outlined below (changes from the old process are highlighted): Software and Release Team members develop packages and mark them for testing Software and Release Team members test the packages, possibly with help from the community Once adequate testing is complete and successful, the Release Manager approves packages for release Weekly, the Release Manager evaluates packages that are ready for release; when a sufficient number of important packages are ready [1] , the Release Manager schedules a release date and announces it. For urgent changes, the Release Manager evaluates the packages as soon as they are tested The Software and Release Team performs pre-release testing, releases the packages, and announces the release Note: The release dates of parallel release series (e.g., 3.3 and 3.4) do not have to coincide, as they have in the past. [1] The threshold for \u201csufficient number of important packages\u201d is determined by the Release Manager, with input from the other Technology Area leaders.","title":"Flexible Release Model"},{"location":"policy/flexible-release-model/#osg-software-flexible-release-model","text":"","title":"OSG Software Flexible Release Model"},{"location":"policy/flexible-release-model/#introduction","text":"Before November 2017, the OSG software stack was released on the second Tuesday of each month, except in the case of urgent releases, which were infrequent. This schedule had been in place since early 2013. Since then, conditions within and outside of the Software team have changed, and we have adjusted the release schedule and associated processes. The previous release model had the recurring problem of a \"release crunch,\" where it was difficult to find the effort required to test large changes before their deadline had passed. Sometimes the lack of timely effort led to software being pushed to the next release (a month later), because there was insufficient testing time. Based on software support tickets, we noticed that many sites follow a local update schedule that is independent of the OSG Release team schedule; some sites upgrade every few months, skipping interim releases, other sites upgrade individual packages as needed. In addition, upstream software developers do not follow our release schedule either, releasing software on their own development timelines. As a result, some site administrators would prefer to have OSG software updates more often, closer to when they become available, rather than tied to a monthly cycle. For these reasons, we created a new release model.","title":"Introduction"},{"location":"policy/flexible-release-model/#release-model","text":"The OSG Release team releases batches of integrated, tested software on an ad hoc basis, with the process outlined below (changes from the old process are highlighted): Software and Release Team members develop packages and mark them for testing Software and Release Team members test the packages, possibly with help from the community Once adequate testing is complete and successful, the Release Manager approves packages for release Weekly, the Release Manager evaluates packages that are ready for release; when a sufficient number of important packages are ready [1] , the Release Manager schedules a release date and announces it. For urgent changes, the Release Manager evaluates the packages as soon as they are tested The Software and Release Team performs pre-release testing, releases the packages, and announces the release Note: The release dates of parallel release series (e.g., 3.3 and 3.4) do not have to coincide, as they have in the past. [1] The threshold for \u201csufficient number of important packages\u201d is determined by the Release Manager, with input from the other Technology Area leaders.","title":"Release Model"},{"location":"policy/globus-toolkit/","text":"OSG Support of the Globus Toolkit \u00b6 Gridftp and GSI Migration Plan In December 2019, the OSG developed a plan for migrating the OSG Software stack away from GridFTP and GSI that can be found here . 6 June 2017 Many in the OSG community have heard the news about the end of support for the open-source Globus Toolkit (formerly available from https://github.com/globus/globus-toolkit/blob/globus_6_branch/support-changes). What does this imply for the OSG Software stack? Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period. Note the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology. During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software. The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software. This model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception. Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus. Over the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI. Please reach out to goc@opensciencegrid.org with your questions, comments, and concerns. Change Log \u00b6 8 October 2020 Add note linking to the GridFTP and GSI migration plan","title":"Globus Toolkit Support"},{"location":"policy/globus-toolkit/#osg-support-of-the-globus-toolkit","text":"Gridftp and GSI Migration Plan In December 2019, the OSG developed a plan for migrating the OSG Software stack away from GridFTP and GSI that can be found here . 6 June 2017 Many in the OSG community have heard the news about the end of support for the open-source Globus Toolkit (formerly available from https://github.com/globus/globus-toolkit/blob/globus_6_branch/support-changes). What does this imply for the OSG Software stack? Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period. Note the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology. During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software. The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software. This model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception. Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus. Over the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI. Please reach out to goc@opensciencegrid.org with your questions, comments, and concerns.","title":"OSG Support of the Globus Toolkit"},{"location":"policy/globus-toolkit/#change-log","text":"8 October 2020 Add note linking to the GridFTP and GSI migration plan","title":"Change Log"},{"location":"policy/gridftp-gsi-migration/","text":"GridFTP and GSI Migration \u00b6 6 December 2019 Introduction \u00b6 The GridFTP protocol (for data transfer) and GSI (as an Authentication and Authorization Infrastructure, AAI) were selected for the OSG ecosystem nearly 15 years ago. In both cases, approaches are becoming increasingly niche; as they have not become widely adopted - indeed, as the communities dramatically shrink while the Internet ecosystem grows - the support costs are increasingly directly shouldered by the OSG. For example, we currently use the GridFTP and GSI implementations in the Grid Community Toolkit (GCT). While the OSG contributes to the GCT (a fork of the abandoned Globus Toolkit) to sustain operations, the long-term plan is to migrate our community off these approaches. The end of the Globus Toolkit is a stark reminder of how niche the current ecosystem is: even the original reference implementation was abandoned. Thus, OSG has the opportunity and motivation to evolve toward a data transfer protocol and security techniques that better fit our needs and allow us to connect to more vibrant software communities. For the data transfer, we are proposing HTTP; for the AAI, we are proposing the use of bearer tokens, HTTPS, and OAuth2. The production-oriented nature of the OSG \u2014 and the embedding of OSG-LHC in the WLCG community \u2014 means that careful coordination, communication, and planning are needed whenever we migrate away from production services. OSG has executed several such technology transitions before and managing the full software lifecycle is part of our value to stakeholders. This document proposes affected services, replacement technologies, and rough timelines for a transition. Timeline \u00b6 The following table contains the major milestones and deliverables for the entire transition. Detailed migration plans can be found in this document . Date Milestone or Deliverable Completed Aug 2019 Beginning of OSG 3.5 release series (last release series depending on GCT) \u2705 Aug 2019 Including HTCondor 8.9.2 in the \u2018upcoming\u2019 repository (first HTCondor version with SciTokens support). \u2705 Oct 2019 OSG no longer carries OSG-specific patches for the GCT. All patches are upstreamed or retired. \u2705 Mar 2020 \"GSI free\" site demo. Show, at proof-of-concept / prototype level, all components without use of GCT. \u2705 Sep 2020 All GCT-free components are in OSG-Upcoming. \u2705 Feb 2021 OSG series 3.6, without GCT dependencies, is released. \u2705 1 May 2022 End of support for OSG 3.5. \u2705 Frequently Asked Questions \u00b6 How does SciTokens interoperate with other token technologies in the WLCG? \u00b6 The scitokens-cpp library used by OSG can transparently use both WLCG JSON Web Tokens (JWTs) and SciTokens. Will a US-LHC migration from GridFTP to XRootD require the same migration for WLCG? \u00b6 No but we have been working to ensure that we coordinate our activities through the WLCG DOMA group. What role does LCMAPS play with SciTokens? \u00b6 LCMAPS only works with GSI. The model for SciTokens is sufficiently more simple for sites that the full complexity of LCMAPS is not needed. What does a SciTokens transition for GlideinWMS mean for European sites? \u00b6 The only piece that involves European sites is the factory to CE relationship: given HTCondor-CE 4.0 already supports SciTokens, we have begun to engage with the ARC-CE team. What are the T1s going to do? No SRM? How does tape work with XRootD and HTTPS? \u00b6 Note that CTA, which CERN is planning to transition to this year, only has XRootD support. We don't think there's a clear HTTPS picture here (or a clear dCache picture for SRM-free) so there will need to be coordination with other groups (e.g. the QoS working group). Can WLCG-FTS handle both SciTokens and x509 certificates at the same time? \u00b6 Yes. Can PhEDEx handle SciTokens? \u00b6 Yes. Version History \u00b6 2021-03-05 : Updated dates for the initial OSG 3.6 release and the targeted OSG 3.5 retirement 2020-03-30 : Completed GSI-free site demonstration 2020-03-17 : Highlighted delay in Jan 2020 milestone and new expected completion date of Mar 2020 2020-03-16 : Updated completed items on the overall timeline; added link for detailed document","title":"GridFTP and GSI Migration"},{"location":"policy/gridftp-gsi-migration/#gridftp-and-gsi-migration","text":"6 December 2019","title":"GridFTP and GSI Migration"},{"location":"policy/gridftp-gsi-migration/#introduction","text":"The GridFTP protocol (for data transfer) and GSI (as an Authentication and Authorization Infrastructure, AAI) were selected for the OSG ecosystem nearly 15 years ago. In both cases, approaches are becoming increasingly niche; as they have not become widely adopted - indeed, as the communities dramatically shrink while the Internet ecosystem grows - the support costs are increasingly directly shouldered by the OSG. For example, we currently use the GridFTP and GSI implementations in the Grid Community Toolkit (GCT). While the OSG contributes to the GCT (a fork of the abandoned Globus Toolkit) to sustain operations, the long-term plan is to migrate our community off these approaches. The end of the Globus Toolkit is a stark reminder of how niche the current ecosystem is: even the original reference implementation was abandoned. Thus, OSG has the opportunity and motivation to evolve toward a data transfer protocol and security techniques that better fit our needs and allow us to connect to more vibrant software communities. For the data transfer, we are proposing HTTP; for the AAI, we are proposing the use of bearer tokens, HTTPS, and OAuth2. The production-oriented nature of the OSG \u2014 and the embedding of OSG-LHC in the WLCG community \u2014 means that careful coordination, communication, and planning are needed whenever we migrate away from production services. OSG has executed several such technology transitions before and managing the full software lifecycle is part of our value to stakeholders. This document proposes affected services, replacement technologies, and rough timelines for a transition.","title":"Introduction"},{"location":"policy/gridftp-gsi-migration/#timeline","text":"The following table contains the major milestones and deliverables for the entire transition. Detailed migration plans can be found in this document . Date Milestone or Deliverable Completed Aug 2019 Beginning of OSG 3.5 release series (last release series depending on GCT) \u2705 Aug 2019 Including HTCondor 8.9.2 in the \u2018upcoming\u2019 repository (first HTCondor version with SciTokens support). \u2705 Oct 2019 OSG no longer carries OSG-specific patches for the GCT. All patches are upstreamed or retired. \u2705 Mar 2020 \"GSI free\" site demo. Show, at proof-of-concept / prototype level, all components without use of GCT. \u2705 Sep 2020 All GCT-free components are in OSG-Upcoming. \u2705 Feb 2021 OSG series 3.6, without GCT dependencies, is released. \u2705 1 May 2022 End of support for OSG 3.5. \u2705","title":"Timeline"},{"location":"policy/gridftp-gsi-migration/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"policy/gridftp-gsi-migration/#how-does-scitokens-interoperate-with-other-token-technologies-in-the-wlcg","text":"The scitokens-cpp library used by OSG can transparently use both WLCG JSON Web Tokens (JWTs) and SciTokens.","title":"How does SciTokens interoperate with other token technologies in the WLCG?"},{"location":"policy/gridftp-gsi-migration/#will-a-us-lhc-migration-from-gridftp-to-xrootd-require-the-same-migration-for-wlcg","text":"No but we have been working to ensure that we coordinate our activities through the WLCG DOMA group.","title":"Will a US-LHC migration from GridFTP to XRootD require the same migration for WLCG?"},{"location":"policy/gridftp-gsi-migration/#what-role-does-lcmaps-play-with-scitokens","text":"LCMAPS only works with GSI. The model for SciTokens is sufficiently more simple for sites that the full complexity of LCMAPS is not needed.","title":"What role does LCMAPS play with SciTokens?"},{"location":"policy/gridftp-gsi-migration/#what-does-a-scitokens-transition-for-glideinwms-mean-for-european-sites","text":"The only piece that involves European sites is the factory to CE relationship: given HTCondor-CE 4.0 already supports SciTokens, we have begun to engage with the ARC-CE team.","title":"What does a SciTokens transition for GlideinWMS mean for European sites?"},{"location":"policy/gridftp-gsi-migration/#what-are-the-t1s-going-to-do-no-srm-how-does-tape-work-with-xrootd-and-https","text":"Note that CTA, which CERN is planning to transition to this year, only has XRootD support. We don't think there's a clear HTTPS picture here (or a clear dCache picture for SRM-free) so there will need to be coordination with other groups (e.g. the QoS working group).","title":"What are the T1s going to do? No SRM? How does tape work with XRootD and HTTPS?"},{"location":"policy/gridftp-gsi-migration/#can-wlcg-fts-handle-both-scitokens-and-x509-certificates-at-the-same-time","text":"Yes.","title":"Can WLCG-FTS handle both SciTokens and x509 certificates at the same time?"},{"location":"policy/gridftp-gsi-migration/#can-phedex-handle-scitokens","text":"Yes.","title":"Can PhEDEx handle SciTokens?"},{"location":"policy/gridftp-gsi-migration/#version-history","text":"2021-03-05 : Updated dates for the initial OSG 3.6 release and the targeted OSG 3.5 retirement 2020-03-30 : Completed GSI-free site demonstration 2020-03-17 : Highlighted delay in Jan 2020 milestone and new expected completion date of Mar 2020 2020-03-16 : Updated completed items on the overall timeline; added link for detailed document","title":"Version History"},{"location":"policy/gums-retire/","text":"GUMS Retirement \u00b6 This document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack. Introduction \u00b6 GUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to local UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from multiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG community, we have found that sites use the following GUMS features: Mapping based on VOMS attributes Host-based mappings Banning users/VOs Supporting pool accounts GUMS is a large Java web application that is more complex than necessary for the subset of features used in the OSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG Software team. OSG's plans to retire GUMS has two major components: Find a suitable replacement for GUMS Provide documentation, tooling, and support to aid in the transition from GUMS to the intended solution Site Transition Plans \u00b6 We have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as user and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and the distributed verification of VOMS extensions makes this feature unnecessary. Pool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific document. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work with any site for which this solution does not work. LCMAPS VOMS plugin installation and configuration documentation can be found here (formerly: lcmaps-voms-authentication). Timeline \u00b6 April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. July 2017 (completed): OSG 3.4 CEs can be configured with 3.3 GUMS hosts March 2018: Complete transition for sites not using pool accounts May 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.","title":"GUMS Retirement"},{"location":"policy/gums-retire/#gums-retirement","text":"This document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack.","title":"GUMS Retirement"},{"location":"policy/gums-retire/#introduction","text":"GUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to local UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from multiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG community, we have found that sites use the following GUMS features: Mapping based on VOMS attributes Host-based mappings Banning users/VOs Supporting pool accounts GUMS is a large Java web application that is more complex than necessary for the subset of features used in the OSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG Software team. OSG's plans to retire GUMS has two major components: Find a suitable replacement for GUMS Provide documentation, tooling, and support to aid in the transition from GUMS to the intended solution","title":"Introduction"},{"location":"policy/gums-retire/#site-transition-plans","text":"We have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as user and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and the distributed verification of VOMS extensions makes this feature unnecessary. Pool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific document. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work with any site for which this solution does not work. LCMAPS VOMS plugin installation and configuration documentation can be found here (formerly: lcmaps-voms-authentication).","title":"Site Transition Plans"},{"location":"policy/gums-retire/#timeline","text":"April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. July 2017 (completed): OSG 3.4 CEs can be configured with 3.3 GUMS hosts March 2018: Complete transition for sites not using pool accounts May 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.","title":"Timeline"},{"location":"policy/new-ospool-user/","text":"Registering for a new Open Science Pool Account \u00b6 The OSG access points at UW-Madison us the COManage identity management system to register new users. COManage uses the InCommon federation , allowing users to register with their institutional identities; there is no \"OSPool password\" that users must memorize Starting the registration process \u00b6 You can start the application for a new account by following the registration process below: Visit the new OSPool user enrollment page . You will be presented with a CILogon Single-Sign On page. Select your insitution and sign in with your insitutional credentials: Please use your institution's credentials as this simplifies the verification process; only select the Google or GitHub identity providers if your institution is not an option. After you have signed in, you will be presented with the self-signup form. Click the \"BEGIN\" button: Enter your name and email address. In most cases, your institution will provide defaults for your name and email address. If you prefer, you may override these values. If you have a GitHub account, please fill in your username. If you have already been working with an member to get the OSPool account, please select their name from the Sponsor drop-down; otherwise, leave it blank. Click the \"SUBMIT\" button: Verifying Your Email Address \u00b6 After submitting your registration application, you will receive an email from registry@cilogon.org to verify your email address. Follow the link in the email and click the \"Accept\" button to complete the verification: Adding an SSH Key \u00b6 A SSH public key (see this overview page for more information) allows for easier SSH-based login to the access point. After verifying your email address, you will be given an option to upload a public key for your account. If you are not ready to do that at this time, simply click \"skip\". ) Meeting with a Facilitator \u00b6 After verifying your email address (and optionally uploading the SSH key), a new support ticket will be generated and we will contact you to arrange a meeting with a Facilitator for an introduction to the access point and High Throughput Computing. After this initial meeting, your account will be finalized and you will be able to login to the system. Getting Help \u00b6 For assistance or questions, please email the support team at support@opensciencegrid.org","title":"New OSPool User Registration"},{"location":"policy/new-ospool-user/#registering-for-a-new-open-science-pool-account","text":"The OSG access points at UW-Madison us the COManage identity management system to register new users. COManage uses the InCommon federation , allowing users to register with their institutional identities; there is no \"OSPool password\" that users must memorize","title":"Registering for a new Open Science Pool Account"},{"location":"policy/new-ospool-user/#starting-the-registration-process","text":"You can start the application for a new account by following the registration process below: Visit the new OSPool user enrollment page . You will be presented with a CILogon Single-Sign On page. Select your insitution and sign in with your insitutional credentials: Please use your institution's credentials as this simplifies the verification process; only select the Google or GitHub identity providers if your institution is not an option. After you have signed in, you will be presented with the self-signup form. Click the \"BEGIN\" button: Enter your name and email address. In most cases, your institution will provide defaults for your name and email address. If you prefer, you may override these values. If you have a GitHub account, please fill in your username. If you have already been working with an member to get the OSPool account, please select their name from the Sponsor drop-down; otherwise, leave it blank. Click the \"SUBMIT\" button:","title":"Starting the registration process"},{"location":"policy/new-ospool-user/#verifying-your-email-address","text":"After submitting your registration application, you will receive an email from registry@cilogon.org to verify your email address. Follow the link in the email and click the \"Accept\" button to complete the verification:","title":"Verifying Your Email Address"},{"location":"policy/new-ospool-user/#adding-an-ssh-key","text":"A SSH public key (see this overview page for more information) allows for easier SSH-based login to the access point. After verifying your email address, you will be given an option to upload a public key for your account. If you are not ready to do that at this time, simply click \"skip\". )","title":"Adding an SSH Key"},{"location":"policy/new-ospool-user/#meeting-with-a-facilitator","text":"After verifying your email address (and optionally uploading the SSH key), a new support ticket will be generated and we will contact you to arrange a meeting with a Facilitator for an introduction to the access point and High Throughput Computing. After this initial meeting, your account will be finalized and you will be able to login to the system.","title":"Meeting with a Facilitator"},{"location":"policy/new-ospool-user/#getting-help","text":"For assistance or questions, please email the support team at support@opensciencegrid.org","title":"Getting Help"},{"location":"policy/packaging-svn-to-git-migration/","text":"OSG Software Packaging SVN to Git Migration \u00b6 On Monday, February 16th, 2025, the OSG Software Team will be migrating the repository that hosts the source for OSG Software RPM packages from a Subversion repo at https://vdt.cs.wisc.edu/svn to GitHub at https://github.com/osg-htc/software-packaging . The new repo is available for preview and testing builds from but changes will not be preserved; you should keep using the SVN repo until the switchover. Requirements \u00b6 To use the new repo, you must have the latest OSG-Build, 2.2.1. Run osg-build --version to check which version you are using. If using it from Git, be sure you're on the V2-branch . Note the OSG Build Tools page has been updated with information about how to get the software via Pip or Apptainer. You must also have write permission to the osg-htc/software-packaging repo ; contact a software team member to request access. Workflow changes \u00b6 The layout of the GitHub repo is the same as the SVN repo -- each set of targets, such as 24-main , is a separate top-level directory, with package directories under it. Koji scratch builds are performed from the local file system as usual. To do a non-scratch build, you must push your changes to the main branch of the upstream repo. Feel free to push directly to the main branch -- no need to make a pull request, unless you want your changes reviewed. The development process page has been updated with these new instructions as well as some advice for keeping track of changes when building packages for multiple release series. If you have Python 3.9 or newer, consider installing pre-commit ; the hook in the repo should prevent you from accidentally checking in large files.","title":"Packaging SVN to Git Migration"},{"location":"policy/packaging-svn-to-git-migration/#osg-software-packaging-svn-to-git-migration","text":"On Monday, February 16th, 2025, the OSG Software Team will be migrating the repository that hosts the source for OSG Software RPM packages from a Subversion repo at https://vdt.cs.wisc.edu/svn to GitHub at https://github.com/osg-htc/software-packaging . The new repo is available for preview and testing builds from but changes will not be preserved; you should keep using the SVN repo until the switchover.","title":"OSG Software Packaging SVN to Git Migration"},{"location":"policy/packaging-svn-to-git-migration/#requirements","text":"To use the new repo, you must have the latest OSG-Build, 2.2.1. Run osg-build --version to check which version you are using. If using it from Git, be sure you're on the V2-branch . Note the OSG Build Tools page has been updated with information about how to get the software via Pip or Apptainer. You must also have write permission to the osg-htc/software-packaging repo ; contact a software team member to request access.","title":"Requirements"},{"location":"policy/packaging-svn-to-git-migration/#workflow-changes","text":"The layout of the GitHub repo is the same as the SVN repo -- each set of targets, such as 24-main , is a separate top-level directory, with package directories under it. Koji scratch builds are performed from the local file system as usual. To do a non-scratch build, you must push your changes to the main branch of the upstream repo. Feel free to push directly to the main branch -- no need to make a pull request, unless you want your changes reviewed. The development process page has been updated with these new instructions as well as some advice for keeping track of changes when building packages for multiple release series. If you have Python 3.9 or newer, consider installing pre-commit ; the hook in the repo should prevent you from accidentally checking in large files.","title":"Workflow changes"},{"location":"policy/repo-layout-migration-2025/","text":"OSG Software Repo Layout Migration \u00b6 11 December 2024 On Monday, January 6th, 2025, the OSG Software Team will be upgrading the server that hosts the OSG Software Yum repositories ( https://repo.osg-htc.org n\u00e9e https://repo.opensciencegrid.org ), which will result in changes to the directory layout for the OSG 23 and OSG 24 Yum repos. Users installing packages should not be affected since the URLs of the repositories themselves will not change. However, mirror administrators will have to take some action to avoid rsyncing those repos from scratch. Perform the following steps: Turn off rsync from repo-rsync.opensciencegrid.org or repo-rsync.osg-htc.org Download the migration script from https://github.com/osg-htc/osg-repo-scripts/blob/el9/migrate.py ( direct link ) Run the migration script on your OSG 23 and OSG 24 Yum repositories. For example, if the files on your mirror are under /mnt/mirror/osg , then run python3 migrate.py --all /mnt/mirror/osg/23-* /mnt/mirror/osg/24-* Switch the rsync source to repo-rsync-itb.osg-htc.org and add --delay-updates --delete-delay to the rsync command. For example, if your rsync command is typically rsync -av --delete \\ repo-rsync.opensciencegrid.org::osg/ \\ /mnt/mirror/osg then change it to rsync -av --delay-updates --delete-delay \\ repo-rsync-itb.osg-htc.org::osg/ \\ /mnt/mirror/osg This will switch your mirror to the new repo layout, minimizing disruption to users. On January 6th, we will upgrade https://repo.osg-htc.org , at which point you should change your rsync source from repo-rsync-itb.osg-htc.org to repo-rsync.osg-htc.org . We will send a reminder after the update.","title":"Repo Layout Migration - Jan 2025"},{"location":"policy/repo-layout-migration-2025/#osg-software-repo-layout-migration","text":"11 December 2024 On Monday, January 6th, 2025, the OSG Software Team will be upgrading the server that hosts the OSG Software Yum repositories ( https://repo.osg-htc.org n\u00e9e https://repo.opensciencegrid.org ), which will result in changes to the directory layout for the OSG 23 and OSG 24 Yum repos. Users installing packages should not be affected since the URLs of the repositories themselves will not change. However, mirror administrators will have to take some action to avoid rsyncing those repos from scratch. Perform the following steps: Turn off rsync from repo-rsync.opensciencegrid.org or repo-rsync.osg-htc.org Download the migration script from https://github.com/osg-htc/osg-repo-scripts/blob/el9/migrate.py ( direct link ) Run the migration script on your OSG 23 and OSG 24 Yum repositories. For example, if the files on your mirror are under /mnt/mirror/osg , then run python3 migrate.py --all /mnt/mirror/osg/23-* /mnt/mirror/osg/24-* Switch the rsync source to repo-rsync-itb.osg-htc.org and add --delay-updates --delete-delay to the rsync command. For example, if your rsync command is typically rsync -av --delete \\ repo-rsync.opensciencegrid.org::osg/ \\ /mnt/mirror/osg then change it to rsync -av --delay-updates --delete-delay \\ repo-rsync-itb.osg-htc.org::osg/ \\ /mnt/mirror/osg This will switch your mirror to the new repo layout, minimizing disruption to users. On January 6th, we will upgrade https://repo.osg-htc.org , at which point you should change your rsync source from repo-rsync-itb.osg-htc.org to repo-rsync.osg-htc.org . We will send a reminder after the update.","title":"OSG Software Repo Layout Migration"},{"location":"policy/service-migrations-spring-2018/","text":"Service Migrations - Spring 2018 \u00b6 The Open Science Grid (OSG) has transitioned effort from Indiana, requiring a redistribution of support and services. Some services were retired, most services were migrated to other locations (with minimal expected sites impact), and some services were migrated that resulted in significant impact on sites. This document was intended to guide OSG site administrators through these changes, highlighting where the site administrator action is required. If you have questions or concerns that are not addressed in this document, see the Getting Help section for details. Getting Help \u00b6 If you have questions or concerns that are not addressed in this document, please contact us at the usual locations: help@osg-htc.org software-discuss@opensciencegrid.org - General discussion of the OSG Software stack ( subscribe ) Slack channel - if you can't create an account, send an e-mail to help@osg-htc.org Support Changes \u00b6 The Footprints ticketing system at https://ticket.opensciencegrid.org was used to track support and security issues as well as certificate and membership requests. This service was retired in favor of two different ticketing systems, depending on the VOs you support at your site: If your site primarily supports... Submit new tickets to... LHC VOs GGUS Anyone else Freshdesk If you experience any problems with ticketing, please contact us at help@osg-htc.org . Service-specific details \u00b6 OSG CA \u00b6 The OSG CA service offered certificate request, renewal, and revocation through the OIM web interface, the OIM REST API, and the osg-pki-tools command-line tool. This service was retired on May 31 but the OSG CA certificate remains in the IGTF distribution, so any certificates issued by the OSG CA remain valid until they expire. The OSG recommends using the following CA certificate services: For... We plan to use the following Certificate Authorities... Host Certificates InCommon and Let\u2019s Encrypt User Certificates CILogon Basic for non-LHC users LHC users should continue to request their user certificates from CERN. Web-Based services Let's Encrypt Note The semantics of Let's Encrypt certificates are different from those of previous CAs. Please see the security team's position on Let's Encrypt for the security and setup implications of switching to a Let's Encrypt host or service certificate. If you experience any problems acquiring host or service certificates, please contact us at help@osg-htc.org . Software Repository \u00b6 The OSG Software repository includes the YUM repositories, client tarballs, and CA tarballs. The physical hosting location changed during the migration but was otherwise unchanged. If you experience any problems with the OSG Software repository, please contact us at help@osg-htc.org . MyOSG and OIM \u00b6 The MyOSG service used to provide web and REST interfaces to access information about OSG resource topology, projects, and VOs. The MyOSG web interface was retired but we continue to offer the same REST interface at https://my.opensciencegrid.org . OIM served as the database for the information used by MyOSG with a web interface for data updates. The OIM web interface was retired but its data was migrated to the topology repository . Updates to the aforementioned data can be requested via email or pull request. Note Please see the OSG CA section for information regarding the OIM certificate service. If you experience any problems with MyOSG or the topology repository, please contact us at help@osg-htc.org . GRACC Accounting and WLCG Accounting \u00b6 No changes were made to the GRACC accounting service during the service migration. If you experience any problems with GRACC accounting, please contact us at help@osg-htc.org . OASIS and CVMFS \u00b6 The OASIS (OSG Application and Software Installation Service) is a service used to distribute common applications and software to OSG sites via CVMFS. The OSG hosts a CVMFS Stratum-0 for keysigning, a repository server, and a CVMFS Stratum-1. The physical hosting location of these services were moved to Nebraska without any other changes. If you experience any problems with OASIS or CVMFS, please contact us at help@osg-htc.org . VOMS Admin Server \u00b6 The OSG VOMS service was used to sign VOMS attributes for members of the OSG VO and responded to queries for a list of VO members. VOMS Admin Server is deprecated in the OSG and the OSG VOMS servers were retired as planned. RSV \u00b6 The central RSV service was a monitoring tool that displayed every service status information about OSG sites that elected to provide it. It was retired since there was no longer a need to monitor OSG site status as a whole. If you would like to monitor your OSG services, you can access the status page of your local RSV instance. Collector \u00b6 The central Collector is a central database service that provides details about pilot jobs currently running in the OSG. The physical hosting location of the central Collector was moved but there were no other changes. If you experience any problems with the central Collector, please contact us at help@osg-htc.org . Homepage \u00b6 The OSG homepage was a Wordpress instance that has been moved to a static site. If you experience any problems with the homepage, please contact us at help@osg-htc.org .","title":"Service Migrations - Spring 2018"},{"location":"policy/service-migrations-spring-2018/#service-migrations-spring-2018","text":"The Open Science Grid (OSG) has transitioned effort from Indiana, requiring a redistribution of support and services. Some services were retired, most services were migrated to other locations (with minimal expected sites impact), and some services were migrated that resulted in significant impact on sites. This document was intended to guide OSG site administrators through these changes, highlighting where the site administrator action is required. If you have questions or concerns that are not addressed in this document, see the Getting Help section for details.","title":"Service Migrations - Spring 2018"},{"location":"policy/service-migrations-spring-2018/#getting-help","text":"If you have questions or concerns that are not addressed in this document, please contact us at the usual locations: help@osg-htc.org software-discuss@opensciencegrid.org - General discussion of the OSG Software stack ( subscribe ) Slack channel - if you can't create an account, send an e-mail to help@osg-htc.org","title":"Getting Help"},{"location":"policy/service-migrations-spring-2018/#support-changes","text":"The Footprints ticketing system at https://ticket.opensciencegrid.org was used to track support and security issues as well as certificate and membership requests. This service was retired in favor of two different ticketing systems, depending on the VOs you support at your site: If your site primarily supports... Submit new tickets to... LHC VOs GGUS Anyone else Freshdesk If you experience any problems with ticketing, please contact us at help@osg-htc.org .","title":"Support Changes"},{"location":"policy/service-migrations-spring-2018/#service-specific-details","text":"","title":"Service-specific details"},{"location":"policy/service-migrations-spring-2018/#osg-ca","text":"The OSG CA service offered certificate request, renewal, and revocation through the OIM web interface, the OIM REST API, and the osg-pki-tools command-line tool. This service was retired on May 31 but the OSG CA certificate remains in the IGTF distribution, so any certificates issued by the OSG CA remain valid until they expire. The OSG recommends using the following CA certificate services: For... We plan to use the following Certificate Authorities... Host Certificates InCommon and Let\u2019s Encrypt User Certificates CILogon Basic for non-LHC users LHC users should continue to request their user certificates from CERN. Web-Based services Let's Encrypt Note The semantics of Let's Encrypt certificates are different from those of previous CAs. Please see the security team's position on Let's Encrypt for the security and setup implications of switching to a Let's Encrypt host or service certificate. If you experience any problems acquiring host or service certificates, please contact us at help@osg-htc.org .","title":"OSG CA"},{"location":"policy/service-migrations-spring-2018/#software-repository","text":"The OSG Software repository includes the YUM repositories, client tarballs, and CA tarballs. The physical hosting location changed during the migration but was otherwise unchanged. If you experience any problems with the OSG Software repository, please contact us at help@osg-htc.org .","title":"Software Repository"},{"location":"policy/service-migrations-spring-2018/#myosg-and-oim","text":"The MyOSG service used to provide web and REST interfaces to access information about OSG resource topology, projects, and VOs. The MyOSG web interface was retired but we continue to offer the same REST interface at https://my.opensciencegrid.org . OIM served as the database for the information used by MyOSG with a web interface for data updates. The OIM web interface was retired but its data was migrated to the topology repository . Updates to the aforementioned data can be requested via email or pull request. Note Please see the OSG CA section for information regarding the OIM certificate service. If you experience any problems with MyOSG or the topology repository, please contact us at help@osg-htc.org .","title":"MyOSG and OIM"},{"location":"policy/service-migrations-spring-2018/#gracc-accounting-and-wlcg-accounting","text":"No changes were made to the GRACC accounting service during the service migration. If you experience any problems with GRACC accounting, please contact us at help@osg-htc.org .","title":"GRACC Accounting and WLCG Accounting"},{"location":"policy/service-migrations-spring-2018/#oasis-and-cvmfs","text":"The OASIS (OSG Application and Software Installation Service) is a service used to distribute common applications and software to OSG sites via CVMFS. The OSG hosts a CVMFS Stratum-0 for keysigning, a repository server, and a CVMFS Stratum-1. The physical hosting location of these services were moved to Nebraska without any other changes. If you experience any problems with OASIS or CVMFS, please contact us at help@osg-htc.org .","title":"OASIS and CVMFS"},{"location":"policy/service-migrations-spring-2018/#voms-admin-server","text":"The OSG VOMS service was used to sign VOMS attributes for members of the OSG VO and responded to queries for a list of VO members. VOMS Admin Server is deprecated in the OSG and the OSG VOMS servers were retired as planned.","title":"VOMS Admin Server"},{"location":"policy/service-migrations-spring-2018/#rsv","text":"The central RSV service was a monitoring tool that displayed every service status information about OSG sites that elected to provide it. It was retired since there was no longer a need to monitor OSG site status as a whole. If you would like to monitor your OSG services, you can access the status page of your local RSV instance.","title":"RSV"},{"location":"policy/service-migrations-spring-2018/#collector","text":"The central Collector is a central database service that provides details about pilot jobs currently running in the OSG. The physical hosting location of the central Collector was moved but there were no other changes. If you experience any problems with the central Collector, please contact us at help@osg-htc.org .","title":"Collector"},{"location":"policy/service-migrations-spring-2018/#homepage","text":"The OSG homepage was a Wordpress instance that has been moved to a static site. If you experience any problems with the homepage, please contact us at help@osg-htc.org .","title":"Homepage"},{"location":"policy/software-release/","text":"Software Release Policy \u00b6 This document contains information about the OSG Software Yum repositories and their policies. For details regarding the technical process for an OSG release, see this document . Yum Repositories \u00b6 The Software Team maintains the following Yum repositories: osg-development : This is the \"wild west\", the place where software goes while it is being worked on by the software team. osg-testing : This is where software goes when it is ready for wide-spread testing, including upstream release candidates osg-prerelease : This is where software goes just before being released, for final verification. osg-release : This is the official, production release of the software stack. This is the main repository for end-users. osg-contrib : This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful. Occasionally there may be other repositories for specific short-term purposes. Note osg-rolling and osg-release-VERSION are only present in the OSG 3.5 series. osg-rolling : This is where software goes before being included in a point release. Intended for end-users. In OSG 3.5, software goes into osg-rolling when it is put into osg-prerelease . osg-release-VERSION : This repository is created per release and its name contains the version number (e.g. osg-release-3.5.4). This is intended mostly for testing purposes, though users may occasionally find it useful. Version Numbers \u00b6 OSG 3.6+ \u00b6 The version number matches the release series. OSG 3.5 \u00b6 There is a single version number that is used to summarize the contents of the osg-release repository. Having a single version number is very useful for a variety of reasons, including: Every time changes are made to the osg-release repository, we update the version number and write release notes. We have a shorthand for referring to the state of the repository; we can talk about specific releases. However, there are important caveats about the version number: Even if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered. The version number is only meaningful in the osg-release repository, though for technical reasons it's present (as an RPM) in other repositories. The version number is communicated as follows: Every time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number. The version number will be of the form X.Y.Z. As of this writing, version numbers are 3.5.Z, where Z indicates a minor revision. Progression of Repositories \u00b6 This figure shows the progression of repositories that packages will go through: osg-development -> osg-testing -> osg-prerelease / osg-rolling -> osg-release \\ -> osg-contrib Release Policies \u00b6 Adding packages to osg-development \u00b6 New packages will only be added to osg-development with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager. Moving packages to osg-testing \u00b6 A package may be moved from osg-development to osg-testing when the individual maintainer of that package decides that it is ready for widespread testing and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team. Moving packages to osg-prerelease; Readying the release \u00b6 When we are ready to make a production release, we first move the correct subset of packages from osg-testing into osg-prerelease . This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of osg-prerelease is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the osg-testing repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to osg-prerelease , the team member doing the release will: Find the correct set of packages to push from osg-testing into osg-prerelease . At a minimum, run the automated test suite on the contents of osg-prerelease . In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.) We expect that in most cases, this process of updating and testing the osg-prerelease repository will be less than one day. If there are urgent security updates to release, this process may be shortened. Moving packages to osg-release \u00b6 When the osg-prerelease repository has been updated and verified, all of the changed software can be moved into the osg-release repository. As part of this move, three important tasks must be done: The released packages are automatically recorded in such a manner that end users/administrators can be notified if desired. Major package updates will also be recorded on the OSG 3.6 \"News\" page with links to the respective release note page or change log. An announcement is sent out whenever a major package is updated. Moving packages to osg-contrib \u00b6 The osg-contrib repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in osg-contrib . Contrib packages should be tested in osg-development first. Timing of releases \u00b6 Software is released between 9AM and 5PM Central Time on a work day that is followed by a work day. The idea is to have a working day to correct any problems with a release rather than having a problematic release persist over a weekend or holiday. We will make exceptions for urgent situations; consult with the release manager when needed. CA Certificates and VO Client packages \u00b6 Packages that contain only data are not part of the usual release cycle. Currently, these are the CA certificate packages and the VO Client packages. Updates to these packages come from the Security Team and Software Team, respectively. They still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level.","title":"Software Release Policy"},{"location":"policy/software-release/#software-release-policy","text":"This document contains information about the OSG Software Yum repositories and their policies. For details regarding the technical process for an OSG release, see this document .","title":"Software Release Policy"},{"location":"policy/software-release/#yum-repositories","text":"The Software Team maintains the following Yum repositories: osg-development : This is the \"wild west\", the place where software goes while it is being worked on by the software team. osg-testing : This is where software goes when it is ready for wide-spread testing, including upstream release candidates osg-prerelease : This is where software goes just before being released, for final verification. osg-release : This is the official, production release of the software stack. This is the main repository for end-users. osg-contrib : This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful. Occasionally there may be other repositories for specific short-term purposes. Note osg-rolling and osg-release-VERSION are only present in the OSG 3.5 series. osg-rolling : This is where software goes before being included in a point release. Intended for end-users. In OSG 3.5, software goes into osg-rolling when it is put into osg-prerelease . osg-release-VERSION : This repository is created per release and its name contains the version number (e.g. osg-release-3.5.4). This is intended mostly for testing purposes, though users may occasionally find it useful.","title":"Yum Repositories"},{"location":"policy/software-release/#version-numbers","text":"","title":"Version Numbers"},{"location":"policy/software-release/#osg-36","text":"The version number matches the release series.","title":"OSG 3.6+"},{"location":"policy/software-release/#osg-35","text":"There is a single version number that is used to summarize the contents of the osg-release repository. Having a single version number is very useful for a variety of reasons, including: Every time changes are made to the osg-release repository, we update the version number and write release notes. We have a shorthand for referring to the state of the repository; we can talk about specific releases. However, there are important caveats about the version number: Even if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered. The version number is only meaningful in the osg-release repository, though for technical reasons it's present (as an RPM) in other repositories. The version number is communicated as follows: Every time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number. The version number will be of the form X.Y.Z. As of this writing, version numbers are 3.5.Z, where Z indicates a minor revision.","title":"OSG 3.5"},{"location":"policy/software-release/#progression-of-repositories","text":"This figure shows the progression of repositories that packages will go through: osg-development -> osg-testing -> osg-prerelease / osg-rolling -> osg-release \\ -> osg-contrib","title":"Progression of Repositories"},{"location":"policy/software-release/#release-policies","text":"","title":"Release Policies"},{"location":"policy/software-release/#adding-packages-to-osg-development","text":"New packages will only be added to osg-development with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager.","title":"Adding packages to osg-development"},{"location":"policy/software-release/#moving-packages-to-osg-testing","text":"A package may be moved from osg-development to osg-testing when the individual maintainer of that package decides that it is ready for widespread testing and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team.","title":"Moving packages to osg-testing"},{"location":"policy/software-release/#moving-packages-to-osg-prerelease-readying-the-release","text":"When we are ready to make a production release, we first move the correct subset of packages from osg-testing into osg-prerelease . This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of osg-prerelease is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the osg-testing repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to osg-prerelease , the team member doing the release will: Find the correct set of packages to push from osg-testing into osg-prerelease . At a minimum, run the automated test suite on the contents of osg-prerelease . In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.) We expect that in most cases, this process of updating and testing the osg-prerelease repository will be less than one day. If there are urgent security updates to release, this process may be shortened.","title":"Moving packages to osg-prerelease; Readying the release"},{"location":"policy/software-release/#moving-packages-to-osg-release","text":"When the osg-prerelease repository has been updated and verified, all of the changed software can be moved into the osg-release repository. As part of this move, three important tasks must be done: The released packages are automatically recorded in such a manner that end users/administrators can be notified if desired. Major package updates will also be recorded on the OSG 3.6 \"News\" page with links to the respective release note page or change log. An announcement is sent out whenever a major package is updated.","title":"Moving packages to osg-release"},{"location":"policy/software-release/#moving-packages-to-osg-contrib","text":"The osg-contrib repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in osg-contrib . Contrib packages should be tested in osg-development first.","title":"Moving packages to osg-contrib"},{"location":"policy/software-release/#timing-of-releases","text":"Software is released between 9AM and 5PM Central Time on a work day that is followed by a work day. The idea is to have a working day to correct any problems with a release rather than having a problematic release persist over a weekend or holiday. We will make exceptions for urgent situations; consult with the release manager when needed.","title":"Timing of releases"},{"location":"policy/software-release/#ca-certificates-and-vo-client-packages","text":"Packages that contain only data are not part of the usual release cycle. Currently, these are the CA certificate packages and the VO Client packages. Updates to these packages come from the Security Team and Software Team, respectively. They still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level.","title":"CA Certificates and VO Client packages"},{"location":"policy/software-support/","text":"Software Support \u00b6 This document describes how OSG Technology Team members should support the OSG Software Stack, including triage duty responsibilities and when to transition from direct support inquiries to a ticketing system such as Freshdesk or Jira. Considerations \u00b6 When providing support for our users, remember the following: We are a small community and we need to take good care of our users. Please be friendly and patient even when the user is frustrated or lacking in knowledge. Always sign your ticket with your full name, so people know who is responding. If it's easy for you, include a signature at the bottom of your response. If you need to collect information about a problematic host, ask users to run osg-system-profiler . It can shorten the number of times you ask for information because it collects quite a bit for you. If you run across a problem that has a chance of being hit by other users: Is there a bug we should fix in the software? Open a Jira ticket. Is there something we could improve in the software? Notify the Software Area Coordinator. Is there a way to improve our documentation? Notify the Software Area Coordinator. Can you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users. Is this something that other Technology Team members should be aware of? Note it during the support discussion during the weekly OSG Technology meeting, or email the Technology Team if it seems more urgent. Triage Duty \u00b6 The OSG uses Freshdesk to track support issues so you will need a Freshworks account with agent privileges (contact the OSG Software Team Area Coordinator for access). Logging in as an agent Don't enter your credentials directly into the login page ! Click the agent login link instead so that you don't have to enter your credentials twice. During normal work hours, the OSG Technology Team splits responsibilities for managing incoming OSG Software support requests based upon a weekly rotation . If you are on triage duty, your responsibilities are as follows: Watch for new software tickets: review the Unresolved Software Tickets and All Unassigned Tickets filters at least three times daily for new OSG Software-related tickets. For any such unassigned tickets, assign it as follows: If you can handle an incoming ticket, assign it to yourself. Inasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people. If you cannot handle an incoming ticket, collect initial details such as relevant versions, logs, etc., and assign the ticket to the most appropriate Technology Team member. Where appropriate, CC third-parties or add relevant OSG staff with FD accounts as watchers (see this documentation ) New sites interested in joining the OSG For support requests inquiring about joining the OSG, assign the ticket to the Campus Services group. Review assigned software tickets. For tickets that are not being handled in a timely fashion (pay special attention to OVERDUE and Customer Responded tickets): If the ticket is pending and the assignee has not responded in > 2 business days, notify the ticket assignee via private note that they need to revisit the ticket. If the ticket is waiting on the customer or a third party and they have not responded in > 1 week, reply to the ticket asking if they've had the time to review the Technology Team's latest response(s). If the ticket was opened by the customer, is waiting on the customer and they have not responded in > 2 weeks, close the ticket and let the customer know that they can re-open it by responding whenever they're ready to tackle the issue again. Review and approve/deny COManage site contact registrations: Follow the instructions to review site contact registrations here . Review Topology data pull requests: Review any Topology PRs that update anything in the collaborations , projects/ , topology/ , or virtual-organizations directories. New institutions Approval of new facilities or project institutions require PATh Project Office, i.e. Irene or Janet. Re-assign non-software tickets: Tickets that have been mistakenly assigned to the Software group should be re-assigned to the appropriate group. Merge duplicate tickets: Responses to a ticket sometimes results in creation of a new ticket; these new tickets should be merged into the original ticket. See this documentation . Split off new support requests in old tickets: If a user has reopened or followed-up in a ticket with a new support request unrelated to the existing ticket (this is a judgment call) and their comment is the last one in the ticket, split off the comment into a new ticket (see this documentation ). If you are unsure if a ticket should be split, consult the Software Area Coordinator. Do not split comments other than the last one FD treats splitting tickets as \"take this comment and make it the start of a new ticket\" and not \"take this comment and all subsequent comments into a new ticket\". Clean up spam: Mark the ticket as spam and block the user. See this documentation . Clean up automated replies: announcements are often sent with Reply-to: help@osg-htc.org so automated replies (e.g. Out of Office, mailing list moderation) will generate tickets. These tickets can be closed. Question If you have questions concerning a ticket, consult the OSG Software Team Manager and/or the #software channel in the OSG Slack. Updating the triage calendar \u00b6 The current triage duty schedule can be found in the OSG Software calendar, hosted on Tim Cartwright\u2019s Google account. If you need privileges to edit the calendar, ask the OSG Software Team Manager. To update the triage duty schedule: Clone the git repo Generate next rotation: ./triage.py --generateNextRotation > rotation.txt Check and update assignments according to team member outages Load triage assignments into Google Calendar: ./triage.py --load rotation.txt To subscribe to this calendar in your calendar program, use the iCal URL: https://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics Ticket Systems \u00b6 The OSG Technology Team uses the Freshdesk and Jira ticketing systems to track support and all other work, respectively. This section describes the differences between the two as well as some OSG Technology Freshdesk conventions. Direct Email \u00b6 Sometimes users may email you directly with support inquiries. If someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are: If it's easy to handle and you can definitely do it yourself, leave it in email. If there's a chance that you can't do it in a timely fashion, turn it into a ticket. If there's a chance that you might lose track of the email, turn it into a ticket. If there's a chance that you might need help from others, turn it into a ticket. If it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket. Freshdesk \u00b6 Freshdesk access The OSG uses Freshdesk to track support issues so you will need a Freshworks account with agent privileges (contact the OSG Software Team Manager for access). Freshdesk tickets are for user support, i.e. this is where we help users debug, understand their problems, etc. When replying to or otherwise updating a Freshdesk ticket, there are a few things to note: Freshdesk auto-populates the contact's name when replying through the web interface, e.g. Hi Brian . Ensure that the name is correct, especially if there are multiple parties involved in a single ticket. If the auto-populated name looks incorrect, e.g. Hi blin.wisc , fix the contact's First and Last name fields. Make sure to set the state of the ticket, which is helpful for those on triage: State Description Open OSG staff is responsible for next actions, including when ticket has not yet been assigned (initial ticket state) Waiting on Customer Assignee needs the reporter to respond Waiting on Third Party Assignee needs a response from a CC Resolved Support is complete or the user is unresponsive. See above . Closed DO NOT SET MANUALLY . Terminal ticket state that is set by Freshdesk. If actionable Technology Team tasks arise from a Freshdesk ticket, Jira ticket(s) should be created to track that work. Resultant Jira tickets should include a link to the original Freshdesk ticket, a description of the problem or feature request, and a proposed solution or implementation. After the relevant Jira tickets have been created, ask the user if they would be ok with tracking the issue via Jira. If they say yes, close the Freshdesk ticket. Jira \u00b6 Jira is for tracking our work and it's meant for internal usage, not for user support. In general, users should not ask for support via Jira. A single user support ticket might result in zero, one, or multiple Jira tickets.","title":"Software Support"},{"location":"policy/software-support/#software-support","text":"This document describes how OSG Technology Team members should support the OSG Software Stack, including triage duty responsibilities and when to transition from direct support inquiries to a ticketing system such as Freshdesk or Jira.","title":"Software Support"},{"location":"policy/software-support/#considerations","text":"When providing support for our users, remember the following: We are a small community and we need to take good care of our users. Please be friendly and patient even when the user is frustrated or lacking in knowledge. Always sign your ticket with your full name, so people know who is responding. If it's easy for you, include a signature at the bottom of your response. If you need to collect information about a problematic host, ask users to run osg-system-profiler . It can shorten the number of times you ask for information because it collects quite a bit for you. If you run across a problem that has a chance of being hit by other users: Is there a bug we should fix in the software? Open a Jira ticket. Is there something we could improve in the software? Notify the Software Area Coordinator. Is there a way to improve our documentation? Notify the Software Area Coordinator. Can you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users. Is this something that other Technology Team members should be aware of? Note it during the support discussion during the weekly OSG Technology meeting, or email the Technology Team if it seems more urgent.","title":"Considerations"},{"location":"policy/software-support/#triage-duty","text":"The OSG uses Freshdesk to track support issues so you will need a Freshworks account with agent privileges (contact the OSG Software Team Area Coordinator for access). Logging in as an agent Don't enter your credentials directly into the login page ! Click the agent login link instead so that you don't have to enter your credentials twice. During normal work hours, the OSG Technology Team splits responsibilities for managing incoming OSG Software support requests based upon a weekly rotation . If you are on triage duty, your responsibilities are as follows: Watch for new software tickets: review the Unresolved Software Tickets and All Unassigned Tickets filters at least three times daily for new OSG Software-related tickets. For any such unassigned tickets, assign it as follows: If you can handle an incoming ticket, assign it to yourself. Inasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people. If you cannot handle an incoming ticket, collect initial details such as relevant versions, logs, etc., and assign the ticket to the most appropriate Technology Team member. Where appropriate, CC third-parties or add relevant OSG staff with FD accounts as watchers (see this documentation ) New sites interested in joining the OSG For support requests inquiring about joining the OSG, assign the ticket to the Campus Services group. Review assigned software tickets. For tickets that are not being handled in a timely fashion (pay special attention to OVERDUE and Customer Responded tickets): If the ticket is pending and the assignee has not responded in > 2 business days, notify the ticket assignee via private note that they need to revisit the ticket. If the ticket is waiting on the customer or a third party and they have not responded in > 1 week, reply to the ticket asking if they've had the time to review the Technology Team's latest response(s). If the ticket was opened by the customer, is waiting on the customer and they have not responded in > 2 weeks, close the ticket and let the customer know that they can re-open it by responding whenever they're ready to tackle the issue again. Review and approve/deny COManage site contact registrations: Follow the instructions to review site contact registrations here . Review Topology data pull requests: Review any Topology PRs that update anything in the collaborations , projects/ , topology/ , or virtual-organizations directories. New institutions Approval of new facilities or project institutions require PATh Project Office, i.e. Irene or Janet. Re-assign non-software tickets: Tickets that have been mistakenly assigned to the Software group should be re-assigned to the appropriate group. Merge duplicate tickets: Responses to a ticket sometimes results in creation of a new ticket; these new tickets should be merged into the original ticket. See this documentation . Split off new support requests in old tickets: If a user has reopened or followed-up in a ticket with a new support request unrelated to the existing ticket (this is a judgment call) and their comment is the last one in the ticket, split off the comment into a new ticket (see this documentation ). If you are unsure if a ticket should be split, consult the Software Area Coordinator. Do not split comments other than the last one FD treats splitting tickets as \"take this comment and make it the start of a new ticket\" and not \"take this comment and all subsequent comments into a new ticket\". Clean up spam: Mark the ticket as spam and block the user. See this documentation . Clean up automated replies: announcements are often sent with Reply-to: help@osg-htc.org so automated replies (e.g. Out of Office, mailing list moderation) will generate tickets. These tickets can be closed. Question If you have questions concerning a ticket, consult the OSG Software Team Manager and/or the #software channel in the OSG Slack.","title":"Triage Duty"},{"location":"policy/software-support/#updating-the-triage-calendar","text":"The current triage duty schedule can be found in the OSG Software calendar, hosted on Tim Cartwright\u2019s Google account. If you need privileges to edit the calendar, ask the OSG Software Team Manager. To update the triage duty schedule: Clone the git repo Generate next rotation: ./triage.py --generateNextRotation > rotation.txt Check and update assignments according to team member outages Load triage assignments into Google Calendar: ./triage.py --load rotation.txt To subscribe to this calendar in your calendar program, use the iCal URL: https://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics","title":"Updating the triage calendar"},{"location":"policy/software-support/#ticket-systems","text":"The OSG Technology Team uses the Freshdesk and Jira ticketing systems to track support and all other work, respectively. This section describes the differences between the two as well as some OSG Technology Freshdesk conventions.","title":"Ticket Systems"},{"location":"policy/software-support/#direct-email","text":"Sometimes users may email you directly with support inquiries. If someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are: If it's easy to handle and you can definitely do it yourself, leave it in email. If there's a chance that you can't do it in a timely fashion, turn it into a ticket. If there's a chance that you might lose track of the email, turn it into a ticket. If there's a chance that you might need help from others, turn it into a ticket. If it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket.","title":"Direct Email"},{"location":"policy/software-support/#freshdesk","text":"Freshdesk access The OSG uses Freshdesk to track support issues so you will need a Freshworks account with agent privileges (contact the OSG Software Team Manager for access). Freshdesk tickets are for user support, i.e. this is where we help users debug, understand their problems, etc. When replying to or otherwise updating a Freshdesk ticket, there are a few things to note: Freshdesk auto-populates the contact's name when replying through the web interface, e.g. Hi Brian . Ensure that the name is correct, especially if there are multiple parties involved in a single ticket. If the auto-populated name looks incorrect, e.g. Hi blin.wisc , fix the contact's First and Last name fields. Make sure to set the state of the ticket, which is helpful for those on triage: State Description Open OSG staff is responsible for next actions, including when ticket has not yet been assigned (initial ticket state) Waiting on Customer Assignee needs the reporter to respond Waiting on Third Party Assignee needs a response from a CC Resolved Support is complete or the user is unresponsive. See above . Closed DO NOT SET MANUALLY . Terminal ticket state that is set by Freshdesk. If actionable Technology Team tasks arise from a Freshdesk ticket, Jira ticket(s) should be created to track that work. Resultant Jira tickets should include a link to the original Freshdesk ticket, a description of the problem or feature request, and a proposed solution or implementation. After the relevant Jira tickets have been created, ask the user if they would be ok with tracking the issue via Jira. If they say yes, close the Freshdesk ticket.","title":"Freshdesk"},{"location":"policy/software-support/#jira","text":"Jira is for tracking our work and it's meant for internal usage, not for user support. In general, users should not ask for support via Jira. A single user support ticket might result in zero, one, or multiple Jira tickets.","title":"Jira"},{"location":"policy/topology-registration/","text":"Handling Topology and Contacts Registration \u00b6 This is an internal procedure handled by OSG Staff; the documentation is available in the operations site .","title":"Handling Topology/Contacts Registrations"},{"location":"policy/topology-registration/#handling-topology-and-contacts-registration","text":"This is an internal procedure handled by OSG Staff; the documentation is available in the operations site .","title":"Handling Topology and Contacts Registration"},{"location":"policy/voms-admin-retire/","text":"VOMS-Admin Retirement \u00b6 Introduction \u00b6 This document provides an overview of the planned retirement of support for VOMS-Admin in the OSG Software Stack. Support for the VOMS infrastructure has three major components: VOMS-Admin : A web interface for maintaining the list of authorized users in a VO and their various authorizations (group membership, roles, attributes, etc). VOMS-Server : A TCP service which signs a cryptographic extension on an X509 proxy certificate asserting the authorizations available to the authenticated user. VOMS Client : Software for extracting and validating the signed VOMS extension from an X509 proxy. The validation is meant to be distributed: the VOMS client does not need to contact the VOMS-Admin server. However, OSG has historically used software such as GUMS or edg-mkgridmap to cache a list of authorizations from the VOMS-Admin interface, creating a dependency between VOMS client and VOMS-Admin. VOMS-Admin is a large, complex Java web application. Over the last few years, upstream support has tailed off - particularly as OSG has been unable to update to VOMS-Admin version 3. As a result, the maintenance burden has largely fallen on the OSG Software team. Given that VOMS-Admin is deeply tied to X509 security infrastructure - and is maintenance-only from OSG Software - there is no path forward to eliminate the use of X509 certificates in the web browser, a high-priority goal In discussions with the OSG community, we have found very few VOs utilize VOMS-Admin to manage their VO users. Instead, the majority use VOMS-Admin to whitelist a pilot certificate: this can be done without a VOMS-Admin endpoint. OSG's plans to retire VOMS-Admin has three major components: (Sites) Enable distributed validation of VOMS extensions in the VOMS client. (VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing of VOMS proxies. (VOs) Migrate remaining VOs to a central comanage instance for managing user authorizations; maintain a plugin to enable direct callouts from VOMS-Server to comanage for authorization lookups. Site Transition Plans \u00b6 We will release a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions; this verification eludes the need to contact the VOMS-Admin interface for a list of authorizations. In 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when LCMAPS asserts the validation was performed. Hence, when GUMS sites update clients to the latest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer needed. Note : In parallel to the VOMS-Admin transition, OSG Software plans to retire GUMS . There is no need to complete one transition before the other. Sites using edg-mkgridmap will need to use its replacement, lcmaps-plugins-voms (this process is documented here ). VO Transition Plans \u00b6 Based on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain a list of authorized pilots. For these VOs, we will help convert invocations of voms-proxy-init : voms-proxy-init -voms hcc:/hcc/Role=pilot to an equivalent call to voms-proxy-fake : voms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\ -hostkey /etc/grid-security/voms/vomskey.pem \\ -fqan /hcc/Role=pilot/Capability=NULL \\ -voms hcc -uri hcc-voms.unl.edu:15000 The latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate currently on the VOMS-Admin server to be kept on the frontend host. The frontend's account may also need access to the certificate. Info See this documentation to update your GlideinWMS Frontend to use the new proxy generation command. We plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to comanage . It is not clear there are any such VOs that need support from OSG. If there are, a hosted version of comanage is expected to be available in summer 2017 from the CILogon 2.0 project. If you feel your VO is affected, please contact the OSG and we will build a custom timeline. If there are no such VOs, we will not need to adopt comanage for this use case (other uses of comanage are expected to proceed regardless). Timeline \u00b6 April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. Sites begin transition to validating VOMS extensions. Summer 2017 (completed): As necessary, VOs are given access to a hosted comanage instance. March 2017 (completed): First VOs begin to retire VOMS-Admin. May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.","title":"VOMS Admin Retirement"},{"location":"policy/voms-admin-retire/#voms-admin-retirement","text":"","title":"VOMS-Admin Retirement"},{"location":"policy/voms-admin-retire/#introduction","text":"This document provides an overview of the planned retirement of support for VOMS-Admin in the OSG Software Stack. Support for the VOMS infrastructure has three major components: VOMS-Admin : A web interface for maintaining the list of authorized users in a VO and their various authorizations (group membership, roles, attributes, etc). VOMS-Server : A TCP service which signs a cryptographic extension on an X509 proxy certificate asserting the authorizations available to the authenticated user. VOMS Client : Software for extracting and validating the signed VOMS extension from an X509 proxy. The validation is meant to be distributed: the VOMS client does not need to contact the VOMS-Admin server. However, OSG has historically used software such as GUMS or edg-mkgridmap to cache a list of authorizations from the VOMS-Admin interface, creating a dependency between VOMS client and VOMS-Admin. VOMS-Admin is a large, complex Java web application. Over the last few years, upstream support has tailed off - particularly as OSG has been unable to update to VOMS-Admin version 3. As a result, the maintenance burden has largely fallen on the OSG Software team. Given that VOMS-Admin is deeply tied to X509 security infrastructure - and is maintenance-only from OSG Software - there is no path forward to eliminate the use of X509 certificates in the web browser, a high-priority goal In discussions with the OSG community, we have found very few VOs utilize VOMS-Admin to manage their VO users. Instead, the majority use VOMS-Admin to whitelist a pilot certificate: this can be done without a VOMS-Admin endpoint. OSG's plans to retire VOMS-Admin has three major components: (Sites) Enable distributed validation of VOMS extensions in the VOMS client. (VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing of VOMS proxies. (VOs) Migrate remaining VOs to a central comanage instance for managing user authorizations; maintain a plugin to enable direct callouts from VOMS-Server to comanage for authorization lookups.","title":"Introduction"},{"location":"policy/voms-admin-retire/#site-transition-plans","text":"We will release a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions; this verification eludes the need to contact the VOMS-Admin interface for a list of authorizations. In 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when LCMAPS asserts the validation was performed. Hence, when GUMS sites update clients to the latest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer needed. Note : In parallel to the VOMS-Admin transition, OSG Software plans to retire GUMS . There is no need to complete one transition before the other. Sites using edg-mkgridmap will need to use its replacement, lcmaps-plugins-voms (this process is documented here ).","title":"Site Transition Plans"},{"location":"policy/voms-admin-retire/#vo-transition-plans","text":"Based on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain a list of authorized pilots. For these VOs, we will help convert invocations of voms-proxy-init : voms-proxy-init -voms hcc:/hcc/Role=pilot to an equivalent call to voms-proxy-fake : voms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\ -hostkey /etc/grid-security/voms/vomskey.pem \\ -fqan /hcc/Role=pilot/Capability=NULL \\ -voms hcc -uri hcc-voms.unl.edu:15000 The latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate currently on the VOMS-Admin server to be kept on the frontend host. The frontend's account may also need access to the certificate. Info See this documentation to update your GlideinWMS Frontend to use the new proxy generation command. We plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to comanage . It is not clear there are any such VOs that need support from OSG. If there are, a hosted version of comanage is expected to be available in summer 2017 from the CILogon 2.0 project. If you feel your VO is affected, please contact the OSG and we will build a custom timeline. If there are no such VOs, we will not need to adopt comanage for this use case (other uses of comanage are expected to proceed regardless).","title":"VO Transition Plans"},{"location":"policy/voms-admin-retire/#timeline","text":"April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. Sites begin transition to validating VOMS extensions. Summer 2017 (completed): As necessary, VOs are given access to a hosted comanage instance. March 2017 (completed): First VOs begin to retire VOMS-Admin. May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.","title":"Timeline"},{"location":"projects/sha2-support/","text":"SHA-2 Compliance \u00b6 When a certificate authority signs a certificate, it uses one of several possible hash algorithms. Historically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family. SHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by Google researchers. These days, the preferred hash algorithm family is SHA-2. The certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2. The table below denotes indicates the minimum releases necessary to support SHA-2 certificates. Component Version In Release Notes BeStMan 2 bestman2-2.3.0-9.osg 3.1.13 SHA-2 support; also see jGlobus, below dCache SRM client dcache-srmclient-2.2.11.1-2.osg 3.1.22 Major update includes SHA-2 support Globus GRAM globus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg 3.1.9 Critical bug fixes (not SHA-2 specific) GUMS gums-1.3.18.009-15.2.osg 3.1.13 Switched to jGlobus 2 with SHA-2 support; also see jGlobus, below jGlobus (for BeStMan 2) jglobus-2.0.5-3.osg 3.1.18 Fixed CRL refresh bug (not SHA-2 specific) VOMS voms-2.0.8-1.5.osg 3.1.17 SHA-2 fix for voms-proxy-init If a component does not appear in the above table, it already has SHA-2 support.","title":"SHA-2 Support"},{"location":"projects/sha2-support/#sha-2-compliance","text":"When a certificate authority signs a certificate, it uses one of several possible hash algorithms. Historically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family. SHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by Google researchers. These days, the preferred hash algorithm family is SHA-2. The certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2. The table below denotes indicates the minimum releases necessary to support SHA-2 certificates. Component Version In Release Notes BeStMan 2 bestman2-2.3.0-9.osg 3.1.13 SHA-2 support; also see jGlobus, below dCache SRM client dcache-srmclient-2.2.11.1-2.osg 3.1.22 Major update includes SHA-2 support Globus GRAM globus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg 3.1.9 Critical bug fixes (not SHA-2 specific) GUMS gums-1.3.18.009-15.2.osg 3.1.13 Switched to jGlobus 2 with SHA-2 support; also see jGlobus, below jGlobus (for BeStMan 2) jglobus-2.0.5-3.osg 3.1.18 Fixed CRL refresh bug (not SHA-2 specific) VOMS voms-2.0.8-1.5.osg 3.1.17 SHA-2 fix for voms-proxy-init If a component does not appear in the above table, it already has SHA-2 support.","title":"SHA-2 Compliance"},{"location":"release/announce-rft-packages/","text":"Ready for Testing Announcements \u00b6 Per our community testing policy , we must send weekly digests of packages that are ready for testing. Create the Announcement \u00b6 Step 1: Identify the packages that are \"Ready for Testing\" \u00b6 Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = \"<VERSION(S)>\" git clone https://github.com/opensciencegrid/release-tools.git cd release-tools 0 -generate-pkg-list $VERSIONS Note In the future, will we have a command the produces the package list sorted according to release series and importance. Step 2: Populate the Announcement Template \u00b6 Find the software components that the packages in the list correspond to. For example, htcondor-ce-4.4.0-1.osg35.el7 should be listed as \"HTCondor-CE 4.4.0\". Place software components into the appropriate section depending on release series and importance of the software. The major software components are listed in the community testing policy . Omit any software that does not need to tested by the community such osg-version and internal tools. Step 3: Send the \"Ready for Testing\" Announcement \u00b6 The announcement goes to: osg-sites@opensciencegrid.org cms-t2@fnal.gov usatlas-t2-l@lists.bnl.gov OIM administrative contacts Use the osg-notify tool on osg-sw-submit.chtc.wisc.edu to send the release announcement using the following command: $ osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message <PATH TO MESSAGE FILE> \\ --subject 'OSG Packages Available for Testing' \\ --recipients \"osg-sites@opensciencegrid.org cms-t2@fnal.gov usatlas-t2-l@lists.bnl.gov\" \\ --oim-recipients resources --oim-recipients vos --oim-contact-type administrative Announcement Template \u00b6 The following email template is filled out to announce that packages are ready for testing. Text between <ANGLE BRACKETS> should be replaced and sections without packages to be tested should be omitted. Omit release numbers unless they are relevant (e.g. for a packaging-only change). Each major component should have a line about what's new. Several packages are available for testing for tentative release next week. OSG 3.5 Only: - Major Components* - <Major Component Name and Version (NV)> - <DESCRIPTION> - <Major Component NV> - <DESCRIPTION> - Minor Components - <Minor Component NV> - <Minor Component NV> Both OSG 3.5 and 3.4: - Major Components* - <Major Component NV> - <DESCRIPTION> - <Major Component NV> - <DESCRIPTION> - Minor Components - <Minor Component NV> - <Minor Component NV> OSG 3.4 Only: - Major Components* - <Major Component NV> - <DESCRIPTION> - <Major Component NV> - <DESCRIPTION> - Minor Components - <Minor Component NV> - <Minor Component NV> To install any of these packages, run the following command: # yum install --enablerepo=osg-testing <PACKAGE NAME> Please test this software and send positive or negative feedback to software-discuss@osg-htc.org. Be sure to include details describing your testing platform, e.g. OSG 3.4 vs 3.5, EL6 vs EL7! If you any questions, you can always contact us at help@osg-htc.org. JIRA Ticket Summary: https://opensciencegrid.atlassian.net/issues/?filter=12355 Sincerely, The OSG Software & Release Team * As described by our Community Software Testing Policy, (https://osg-htc.org/technology/policy/community-testing/) major components of the OSG Software Stack need positive feedback and the approval of the release manager before they can be released.","title":"Ready for Testing Announcements"},{"location":"release/announce-rft-packages/#ready-for-testing-announcements","text":"Per our community testing policy , we must send weekly digests of packages that are ready for testing.","title":"Ready for Testing Announcements"},{"location":"release/announce-rft-packages/#create-the-announcement","text":"","title":"Create the Announcement"},{"location":"release/announce-rft-packages/#step-1-identify-the-packages-that-are-ready-for-testing","text":"Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = \"<VERSION(S)>\" git clone https://github.com/opensciencegrid/release-tools.git cd release-tools 0 -generate-pkg-list $VERSIONS Note In the future, will we have a command the produces the package list sorted according to release series and importance.","title":"Step 1: Identify the packages that are \"Ready for Testing\""},{"location":"release/announce-rft-packages/#step-2-populate-the-announcement-template","text":"Find the software components that the packages in the list correspond to. For example, htcondor-ce-4.4.0-1.osg35.el7 should be listed as \"HTCondor-CE 4.4.0\". Place software components into the appropriate section depending on release series and importance of the software. The major software components are listed in the community testing policy . Omit any software that does not need to tested by the community such osg-version and internal tools.","title":"Step 2: Populate the Announcement Template"},{"location":"release/announce-rft-packages/#step-3-send-the-ready-for-testing-announcement","text":"The announcement goes to: osg-sites@opensciencegrid.org cms-t2@fnal.gov usatlas-t2-l@lists.bnl.gov OIM administrative contacts Use the osg-notify tool on osg-sw-submit.chtc.wisc.edu to send the release announcement using the following command: $ osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message <PATH TO MESSAGE FILE> \\ --subject 'OSG Packages Available for Testing' \\ --recipients \"osg-sites@opensciencegrid.org cms-t2@fnal.gov usatlas-t2-l@lists.bnl.gov\" \\ --oim-recipients resources --oim-recipients vos --oim-contact-type administrative","title":"Step 3: Send the \"Ready for Testing\" Announcement"},{"location":"release/announce-rft-packages/#announcement-template","text":"The following email template is filled out to announce that packages are ready for testing. Text between <ANGLE BRACKETS> should be replaced and sections without packages to be tested should be omitted. Omit release numbers unless they are relevant (e.g. for a packaging-only change). Each major component should have a line about what's new. Several packages are available for testing for tentative release next week. OSG 3.5 Only: - Major Components* - <Major Component Name and Version (NV)> - <DESCRIPTION> - <Major Component NV> - <DESCRIPTION> - Minor Components - <Minor Component NV> - <Minor Component NV> Both OSG 3.5 and 3.4: - Major Components* - <Major Component NV> - <DESCRIPTION> - <Major Component NV> - <DESCRIPTION> - Minor Components - <Minor Component NV> - <Minor Component NV> OSG 3.4 Only: - Major Components* - <Major Component NV> - <DESCRIPTION> - <Major Component NV> - <DESCRIPTION> - Minor Components - <Minor Component NV> - <Minor Component NV> To install any of these packages, run the following command: # yum install --enablerepo=osg-testing <PACKAGE NAME> Please test this software and send positive or negative feedback to software-discuss@osg-htc.org. Be sure to include details describing your testing platform, e.g. OSG 3.4 vs 3.5, EL6 vs EL7! If you any questions, you can always contact us at help@osg-htc.org. JIRA Ticket Summary: https://opensciencegrid.atlassian.net/issues/?filter=12355 Sincerely, The OSG Software & Release Team * As described by our Community Software Testing Policy, (https://osg-htc.org/technology/policy/community-testing/) major components of the OSG Software Stack need positive feedback and the approval of the release manager before they can be released.","title":"Announcement Template"},{"location":"release/cut-sw-release/","text":"How to Cut a Software Release \u00b6 This document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found here . Due to the length of time that this process takes, it is recommended to do the release over two or more days to allow for errors to be corrected and tests to be run. Requirements \u00b6 UW netID registered with OSG's koji with build and release team privileges On laptop: kinit netid@AD.WISC.EDU An account on dumbo and UW CS to access UW's AFS On dumbo: kinit user@CS.WISC.EDU; aklog release-tools scripts in your PATH ( GitHub ) osg-build scripts in your PATH (installed via OSG yum repos or source ) Promoting Packages from Testing to Pre-release \u00b6 As packages complete testing, they are promoted from the testing repository to the pre-release repository. osg-promote -r 23 -prerelease -r 3 .6-prerelease <package name ( s ) > When promoting a package into pre-release, be sure to add the pre-release promotion table to the Jira ticket. If there are several tickets under a single umbrella ticket, adding the pre-release promotion table to just the parent ticket is sufficient. Once the pacakges have been promoted to pre-release, mark the Jira ticket \"Ready for Release\". Pick the Version Number \u00b6 The rest of this document makes references to <VERSION(S)> and <NON-UPCOMING VERSIONS(S)> , which refer to a space-delimited list of a date string plus the OSG version(s) and that same list minus the upcoming versions (e.g. 231130 3.6 3.6-upcoming 23 23-upcoming and 231130 3.6 23 ). Generally, the first number is the release date encoded as yymmdd . Also this document make references to <FULL VERSION(S)> , which refer to a space-delimited list of version numbers (e.g. 3.6.231130 23.231130 ). If you are unsure about either the version or revision, please consult the release manager. Day 0: Generate Preliminary Release List \u00b6 The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = '<VERSION(S)>' # laptop git clone https://github.com/opensciencegrid/release-tools.git cd release-tools ./0-generate-pkg-list $VERSIONS Day 1: Verify Pre-Release and Generate Tarballs \u00b6 This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs. Step 1: Verify Pre-Release \u00b6 Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated release-list in git). To do this, run the 1-verify-prerelease script from git: VERSIONS = '<VERSION(S)>' # laptop ./1-verify-prerelease $VERSIONS If there are any discrepancies, consult the release manager. You may have to tag or untag packages with the osg-koji tool. Step 2: Test Pre-Release in VM Universe \u00b6 To test pre-release, you will be kicking off a manual VM universe test run from osg-sw-submit.chtc.wisc.edu . Ensure that you meet the pre-requisites for submitting VM universe test runs Prepare the test suite by running: osg-run-tests -P 'Testing OSG pre-release' cd into the directory specified in the output of the previous command Submit the DAG: ./master-run.sh Note Test upcoming even though nothing will be released into upcoming. It is possible that a blahp (or some other) update in 3.X could affect upcoming. Note If there are failures, consult the release-manager before proceeding. Step 3: Make sure repositories are up to date for building the tarballs \u00b6 Ask koji to regenerate the repositories needed from the tarballs in this release. # laptop ./1-regen-repos $VERSIONS Make sure that the repository regeneration successfully completes by looking at the koji dashboard. Step 4: Create the client tarballs \u00b6 Create the OSG client tarballs on dumbo using the relevant script from git: FULL_VERSIONS = \"<FULL VERSION(S)>\" # dumbo.chtc.wisc.edu git clone https://github.com/opensciencegrid/tarball-client.git cd tarball-client for ver in $FULL_VERSIONS ; do ./docker-make-client-tarball --version $ver --all done The tarballs are found in the tarball-client directory. Step 5: Briefly test the client tarballs \u00b6 Test the OSG client tarballs in Docker containers on dumbo using the relevant release-tools script: As an unprivileged user , run the script: NON_UPCOMING_VERSIONS = \"<NON-UPCOMING VERSION(S)>\" # dumbo.chtc.wisc.edu ./1-verify-tarballs $NON_UPCOMING_VERSIONS If you have time, try some of the binaries, such as grid-proxy-init. Step 6: Wait \u00b6 Wait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day. Day 2: Pushing the Release \u00b6 Step 1: Upload the tarballs to AFS \u00b6 On dumbo , upload the tarballs to AFS. (This step moved to release day, since repo.opensciencegrid.org tarballs are automatically updated hourly from the VDT web site served out of AFS.) NON_UPCOMING_VERSIONS = \"<NON-UPCOMING VERSION(S)>\" # dumbo.chtc.wisc.edu ./2-upload-tarballs-to-afs $NON_UPCOMING_VERSIONS Step 2: Push from pre-release to release \u00b6 This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. VERSIONS = '<VERSION(S)>' # laptop 2 -push-release $VERSIONS Step 3: Update the Release Information \u00b6 This script updates the release information in AFS. VERSIONS = '<VERSION(S)>' # laptop 2 -update-info $VERSIONS *.txt files are created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS. Step 4: Install the tarballs into OASIS \u00b6 Note You must be an OASIS manager of the mis VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L. Get the uploader script from Git and run it with osgrun from the UW AFS install of the tarball client you made earlier. On a UW CSL machine: FULL_VERSIONS = \"<FULL VERSION(S)>\" # dumbo.chtc.wisc.edu git clone --depth 1 https://github.com/opensciencegrid/tarball-client.git cd tarball-client for ver in $FULL_VERSIONS ; do ./upload-tarballs-to-oasis $ver done The script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process. Step 5: Rebuild the Docker software base \u00b6 Note The following docker build workflows pull the latest release RPMs from repo.osg-htc.org , which refreshes based on the contents of koji roughly once every 45 minutes. Ensure that repo.osg-htc.org is updated prior to running Steps 5 and 6. Go to the build-docker-image workflow page of the opensciencegrid/docker-software-base : https://github.com/opensciencegrid/docker-software-base/actions/workflows/build-container.yml Click the Run Workflow button, select the master branch, and click Run workflow . Step 6: Update the Docker WN client \u00b6 The GitHub repository at opensciencegrid/docker-osg-wn controls the contents and tags pushed for the opensciencegrid/osg-wn container image. Navigate to the build/push workflow Click the Run workflow button and select the master branch Verify that all builds succeed Step 7: Verify the CA certificates update \u00b6 If this release contains either the osg-ca-certs package, verify that the CA web site has been updated. Wait for the CA certificates to be updated. It may take a while for the updates to reach the mirror used to update the web site. The repository is checked hourly for updated CA certificates. Once the web page is updated, run the following command to update the CA certificates in the tarball installation and verify that the version of the CA certificates match the version that was promoted to release. # moria.cs.wisc.edu /p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data Step 8: Merge any pending documentation \u00b6 For each documentation ticket in this release, merge the pull requests mentioned in the description or comments. Step 9: Update News \u00b6 Make a new entry in the News section of the release series page. For the list of changes, make an entry for each package that contains short descriptive text that would inform a system administrator whether or not this change is of concern to them. Also, link in any release announcement web page that is available for the software. Look a prior releases of the same software for hints on where to find such a page. Examine the known issues and remove any that were resolved with this release. Of course, add any new ones that have come up. Spell check the news. Locally serve up the web pages and ensure that the formatting looks good and the links work as expected. docker run --rm -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material:7.1.0 Make a pull request, get it approved, and merged. When the web page is available, you can announce the release. Step 10: Announce the release \u00b6 The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release. The release manager writes the a release announcement for each version and sends it out. The announcement should mention a handful of the most important updates. Due to downstream formatting issues, each major change should end at column 76 or earlier. Here is a sample, replace <BRACKETED TEXT> with the appropriate values: Subject: Announcing OSG Software version <VERSION> We are pleased to announce OSG Software version <VERSION>! Changes to OSG <VERSION> include: - Major Change 1 - Major Change 2 - Major Change 3 Release notes and pointers to more documentation can be found at: https://osg-htc.org/docs/release/osg-36/#latest-news The OSG Docker images on Docker Hub (https://hub.docker.com/u/opensciencegrid/) have been updated to contain the new software. Need help? Let us know: http://www.osg-htc.org/docs/common/help/ We welcome feedback on this release! The release manager uses the osg-notify tool on osg-sw-submit.chtc.wisc.edu to send the release announcement using the following command: $ osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message <PATH TO MESSAGE FILE> \\ --subject '<EMAIL SUBJECT>' \\ --recipients \"site-announce@osg-htc.org\" \\ --oim-recipients resources --oim-recipients vos --oim-contact-type administrative Replacing <EMAIL SUBJECT> with an appropriate subject for your announcement and <PATH TO MESSAGE FILE> with the path to the file containing your message in plain text. Final Steps \u00b6 On the Releases page, create version(s) that matches <FULL VERSION(S)> for this release. Open the OSG Ready for Release filter and release the tickets, setting the Resolution to Fixed and replacing the Fix versions with the specific full version(s) created above. For tickets that have version numbers in common, use the `Bulk change' functionality. Once all the tickets have been closed, go back to the Releases page and release the specific full versions.","title":"How to Cut a Release"},{"location":"release/cut-sw-release/#how-to-cut-a-software-release","text":"This document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found here . Due to the length of time that this process takes, it is recommended to do the release over two or more days to allow for errors to be corrected and tests to be run.","title":"How to Cut a Software Release"},{"location":"release/cut-sw-release/#requirements","text":"UW netID registered with OSG's koji with build and release team privileges On laptop: kinit netid@AD.WISC.EDU An account on dumbo and UW CS to access UW's AFS On dumbo: kinit user@CS.WISC.EDU; aklog release-tools scripts in your PATH ( GitHub ) osg-build scripts in your PATH (installed via OSG yum repos or source )","title":"Requirements"},{"location":"release/cut-sw-release/#promoting-packages-from-testing-to-pre-release","text":"As packages complete testing, they are promoted from the testing repository to the pre-release repository. osg-promote -r 23 -prerelease -r 3 .6-prerelease <package name ( s ) > When promoting a package into pre-release, be sure to add the pre-release promotion table to the Jira ticket. If there are several tickets under a single umbrella ticket, adding the pre-release promotion table to just the parent ticket is sufficient. Once the pacakges have been promoted to pre-release, mark the Jira ticket \"Ready for Release\".","title":"Promoting Packages from Testing to Pre-release"},{"location":"release/cut-sw-release/#pick-the-version-number","text":"The rest of this document makes references to <VERSION(S)> and <NON-UPCOMING VERSIONS(S)> , which refer to a space-delimited list of a date string plus the OSG version(s) and that same list minus the upcoming versions (e.g. 231130 3.6 3.6-upcoming 23 23-upcoming and 231130 3.6 23 ). Generally, the first number is the release date encoded as yymmdd . Also this document make references to <FULL VERSION(S)> , which refer to a space-delimited list of version numbers (e.g. 3.6.231130 23.231130 ). If you are unsure about either the version or revision, please consult the release manager.","title":"Pick the Version Number"},{"location":"release/cut-sw-release/#day-0-generate-preliminary-release-list","text":"The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = '<VERSION(S)>' # laptop git clone https://github.com/opensciencegrid/release-tools.git cd release-tools ./0-generate-pkg-list $VERSIONS","title":"Day 0: Generate Preliminary Release List"},{"location":"release/cut-sw-release/#day-1-verify-pre-release-and-generate-tarballs","text":"This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs.","title":"Day 1: Verify Pre-Release and Generate Tarballs"},{"location":"release/cut-sw-release/#step-1-verify-pre-release","text":"Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated release-list in git). To do this, run the 1-verify-prerelease script from git: VERSIONS = '<VERSION(S)>' # laptop ./1-verify-prerelease $VERSIONS If there are any discrepancies, consult the release manager. You may have to tag or untag packages with the osg-koji tool.","title":"Step 1: Verify Pre-Release"},{"location":"release/cut-sw-release/#step-2-test-pre-release-in-vm-universe","text":"To test pre-release, you will be kicking off a manual VM universe test run from osg-sw-submit.chtc.wisc.edu . Ensure that you meet the pre-requisites for submitting VM universe test runs Prepare the test suite by running: osg-run-tests -P 'Testing OSG pre-release' cd into the directory specified in the output of the previous command Submit the DAG: ./master-run.sh Note Test upcoming even though nothing will be released into upcoming. It is possible that a blahp (or some other) update in 3.X could affect upcoming. Note If there are failures, consult the release-manager before proceeding.","title":"Step 2: Test Pre-Release in VM Universe"},{"location":"release/cut-sw-release/#step-3-make-sure-repositories-are-up-to-date-for-building-the-tarballs","text":"Ask koji to regenerate the repositories needed from the tarballs in this release. # laptop ./1-regen-repos $VERSIONS Make sure that the repository regeneration successfully completes by looking at the koji dashboard.","title":"Step 3: Make sure repositories are up to date for building the tarballs"},{"location":"release/cut-sw-release/#step-4-create-the-client-tarballs","text":"Create the OSG client tarballs on dumbo using the relevant script from git: FULL_VERSIONS = \"<FULL VERSION(S)>\" # dumbo.chtc.wisc.edu git clone https://github.com/opensciencegrid/tarball-client.git cd tarball-client for ver in $FULL_VERSIONS ; do ./docker-make-client-tarball --version $ver --all done The tarballs are found in the tarball-client directory.","title":"Step 4: Create the client tarballs"},{"location":"release/cut-sw-release/#step-5-briefly-test-the-client-tarballs","text":"Test the OSG client tarballs in Docker containers on dumbo using the relevant release-tools script: As an unprivileged user , run the script: NON_UPCOMING_VERSIONS = \"<NON-UPCOMING VERSION(S)>\" # dumbo.chtc.wisc.edu ./1-verify-tarballs $NON_UPCOMING_VERSIONS If you have time, try some of the binaries, such as grid-proxy-init.","title":"Step 5: Briefly test the client tarballs"},{"location":"release/cut-sw-release/#step-6-wait","text":"Wait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day.","title":"Step 6: Wait"},{"location":"release/cut-sw-release/#day-2-pushing-the-release","text":"","title":"Day 2: Pushing the Release"},{"location":"release/cut-sw-release/#step-1-upload-the-tarballs-to-afs","text":"On dumbo , upload the tarballs to AFS. (This step moved to release day, since repo.opensciencegrid.org tarballs are automatically updated hourly from the VDT web site served out of AFS.) NON_UPCOMING_VERSIONS = \"<NON-UPCOMING VERSION(S)>\" # dumbo.chtc.wisc.edu ./2-upload-tarballs-to-afs $NON_UPCOMING_VERSIONS","title":"Step 1: Upload the tarballs to AFS"},{"location":"release/cut-sw-release/#step-2-push-from-pre-release-to-release","text":"This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. VERSIONS = '<VERSION(S)>' # laptop 2 -push-release $VERSIONS","title":"Step 2: Push from pre-release to release"},{"location":"release/cut-sw-release/#step-3-update-the-release-information","text":"This script updates the release information in AFS. VERSIONS = '<VERSION(S)>' # laptop 2 -update-info $VERSIONS *.txt files are created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS.","title":"Step 3: Update the Release Information"},{"location":"release/cut-sw-release/#step-4-install-the-tarballs-into-oasis","text":"Note You must be an OASIS manager of the mis VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L. Get the uploader script from Git and run it with osgrun from the UW AFS install of the tarball client you made earlier. On a UW CSL machine: FULL_VERSIONS = \"<FULL VERSION(S)>\" # dumbo.chtc.wisc.edu git clone --depth 1 https://github.com/opensciencegrid/tarball-client.git cd tarball-client for ver in $FULL_VERSIONS ; do ./upload-tarballs-to-oasis $ver done The script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process.","title":"Step 4: Install the tarballs into OASIS"},{"location":"release/cut-sw-release/#step-5-rebuild-the-docker-software-base","text":"Note The following docker build workflows pull the latest release RPMs from repo.osg-htc.org , which refreshes based on the contents of koji roughly once every 45 minutes. Ensure that repo.osg-htc.org is updated prior to running Steps 5 and 6. Go to the build-docker-image workflow page of the opensciencegrid/docker-software-base : https://github.com/opensciencegrid/docker-software-base/actions/workflows/build-container.yml Click the Run Workflow button, select the master branch, and click Run workflow .","title":"Step 5: Rebuild the Docker software base"},{"location":"release/cut-sw-release/#step-6-update-the-docker-wn-client","text":"The GitHub repository at opensciencegrid/docker-osg-wn controls the contents and tags pushed for the opensciencegrid/osg-wn container image. Navigate to the build/push workflow Click the Run workflow button and select the master branch Verify that all builds succeed","title":"Step 6: Update the Docker WN client"},{"location":"release/cut-sw-release/#step-7-verify-the-ca-certificates-update","text":"If this release contains either the osg-ca-certs package, verify that the CA web site has been updated. Wait for the CA certificates to be updated. It may take a while for the updates to reach the mirror used to update the web site. The repository is checked hourly for updated CA certificates. Once the web page is updated, run the following command to update the CA certificates in the tarball installation and verify that the version of the CA certificates match the version that was promoted to release. # moria.cs.wisc.edu /p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data","title":"Step 7: Verify the CA certificates update"},{"location":"release/cut-sw-release/#step-8-merge-any-pending-documentation","text":"For each documentation ticket in this release, merge the pull requests mentioned in the description or comments.","title":"Step 8: Merge any pending documentation"},{"location":"release/cut-sw-release/#step-9-update-news","text":"Make a new entry in the News section of the release series page. For the list of changes, make an entry for each package that contains short descriptive text that would inform a system administrator whether or not this change is of concern to them. Also, link in any release announcement web page that is available for the software. Look a prior releases of the same software for hints on where to find such a page. Examine the known issues and remove any that were resolved with this release. Of course, add any new ones that have come up. Spell check the news. Locally serve up the web pages and ensure that the formatting looks good and the links work as expected. docker run --rm -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material:7.1.0 Make a pull request, get it approved, and merged. When the web page is available, you can announce the release.","title":"Step 9: Update News"},{"location":"release/cut-sw-release/#step-10-announce-the-release","text":"The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release. The release manager writes the a release announcement for each version and sends it out. The announcement should mention a handful of the most important updates. Due to downstream formatting issues, each major change should end at column 76 or earlier. Here is a sample, replace <BRACKETED TEXT> with the appropriate values: Subject: Announcing OSG Software version <VERSION> We are pleased to announce OSG Software version <VERSION>! Changes to OSG <VERSION> include: - Major Change 1 - Major Change 2 - Major Change 3 Release notes and pointers to more documentation can be found at: https://osg-htc.org/docs/release/osg-36/#latest-news The OSG Docker images on Docker Hub (https://hub.docker.com/u/opensciencegrid/) have been updated to contain the new software. Need help? Let us know: http://www.osg-htc.org/docs/common/help/ We welcome feedback on this release! The release manager uses the osg-notify tool on osg-sw-submit.chtc.wisc.edu to send the release announcement using the following command: $ osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message <PATH TO MESSAGE FILE> \\ --subject '<EMAIL SUBJECT>' \\ --recipients \"site-announce@osg-htc.org\" \\ --oim-recipients resources --oim-recipients vos --oim-contact-type administrative Replacing <EMAIL SUBJECT> with an appropriate subject for your announcement and <PATH TO MESSAGE FILE> with the path to the file containing your message in plain text.","title":"Step 10: Announce the release"},{"location":"release/cut-sw-release/#final-steps","text":"On the Releases page, create version(s) that matches <FULL VERSION(S)> for this release. Open the OSG Ready for Release filter and release the tickets, setting the Resolution to Fixed and replacing the Fix versions with the specific full version(s) created above. For tickets that have version numbers in common, use the `Bulk change' functionality. Once all the tickets have been closed, go back to the Releases page and release the specific full versions.","title":"Final Steps"},{"location":"release/empty-pkgs/","text":"Procedure for updating empty-* packages \u00b6 Background \u00b6 The empty-* packages were introduced a workaround for sites that install certain software (for example HTCondor or CA certs) from tarballs or other means that do not involve Yum/RPM. The packages contain no files, and exist merely to satisfy RPM dependencies so that other packages can be installed. It is the admin's responsibility to make sure that whatever component they installed the empty package for is functional. The empty packages are kept in a separate repository to prevent them from being accidentally installed instead of the component they claim to provide. Because of this, they do not go through the normal release process of development to testing to prerelease to release, but are moved straight from osg-development into osg-empty after developer testing. Warning It is important to untag the packages from osg-development immediately after promotion to osg-empty Procedure \u00b6 Prepare the package update, but do not build yet. Coordinate with the Software and Release Managers to set aside a good time to update the package. An empty package should not remain in the development repos for longer than a few hours. Build into development. Test out of development. Be thorough , as there is no separate acceptance testing for empty packages. In the JIRA ticket, document your testing procedure and request permission from both the Software and the Release Managers. (Since there is no acceptance testing, both of them have to sign off on the new build). After receiving permission, tag the builds into the osg-empty tags, and untag them from the osg-development tags. Then regenerate the osg-empty repos. osg-koji move-pkg osg-3.3-el6-development osg-3.3-el6-empty <EL6_BUILD_NVR> osg-koji move-pkg osg-3.3-el7-development osg-3.3-el7-empty <EL7_BUILD_NVR> osg-koji regen-repo --nowait osg-3.3-el6-empty osg-koji regen-repo --nowait osg-3.3-el7-empty","title":"Empty Packages"},{"location":"release/empty-pkgs/#procedure-for-updating-empty-packages","text":"","title":"Procedure for updating empty-* packages"},{"location":"release/empty-pkgs/#background","text":"The empty-* packages were introduced a workaround for sites that install certain software (for example HTCondor or CA certs) from tarballs or other means that do not involve Yum/RPM. The packages contain no files, and exist merely to satisfy RPM dependencies so that other packages can be installed. It is the admin's responsibility to make sure that whatever component they installed the empty package for is functional. The empty packages are kept in a separate repository to prevent them from being accidentally installed instead of the component they claim to provide. Because of this, they do not go through the normal release process of development to testing to prerelease to release, but are moved straight from osg-development into osg-empty after developer testing. Warning It is important to untag the packages from osg-development immediately after promotion to osg-empty","title":"Background"},{"location":"release/empty-pkgs/#procedure","text":"Prepare the package update, but do not build yet. Coordinate with the Software and Release Managers to set aside a good time to update the package. An empty package should not remain in the development repos for longer than a few hours. Build into development. Test out of development. Be thorough , as there is no separate acceptance testing for empty packages. In the JIRA ticket, document your testing procedure and request permission from both the Software and the Release Managers. (Since there is no acceptance testing, both of them have to sign off on the new build). After receiving permission, tag the builds into the osg-empty tags, and untag them from the osg-development tags. Then regenerate the osg-empty repos. osg-koji move-pkg osg-3.3-el6-development osg-3.3-el6-empty <EL6_BUILD_NVR> osg-koji move-pkg osg-3.3-el7-development osg-3.3-el7-empty <EL7_BUILD_NVR> osg-koji regen-repo --nowait osg-3.3-el6-empty osg-koji regen-repo --nowait osg-3.3-el7-empty","title":"Procedure"},{"location":"release/itb-testing/","text":"Testing OSG Software Prereleases on the Madison ITB Site \u00b6 This document contains basic recipes for testing a OSG software prereleases on the Madison ITB site, which includes HTCondor prerelease builds and full OSG software stack prereleases from Yum. Prerequisites \u00b6 The following items are known prerequisites to using this recipe. If you are not running the Ansible commands from osghost, there are almost certainly other prerequisites that are not listed below. And even using osghost for Ansible and itb-submit for the submissions, there may be other prerequisites missing. Please improve this document by adding other prerequisites as they are identified! A checkout of the osgitb directory from our local git instance (not GitHub) Your X.509 DN in the osgitb/unmanaged/htcondor-ce/grid-mapfile file and (via Ansible) on itb-ce1 and itb-ce2 Gathering Information \u00b6 Technically skippable, this section is about checking on the state of the ITB machines before making changes. The plan is to keep the ITB machines generally up-to-date independently, so those steps are not listed here. And honestly, the steps below are just some ideas; do whatever makes sense for the given update. The commands can be run as-is from within the osgitb directory from git. Check OS versions for all current ITB hosts: ansible current -i inventory -f 20 -o -m command -a 'cat /etc/redhat-release' Check the date and time on all hosts (in case NTP stops working): ansible current -i inventory -f 20 -o -m command -a 'date' Check software versions for certain hosts (e.g., for the condor package on hosts in the workers group): ansible workers -i inventory -f 20 -o -m command -a 'rpm -q condor' Installing HTCondor Prerelease \u00b6 Use this section to install a new version of HTCondor, specifically a prerelease build from the development or upcoming-development repository, on the test hosts. Obtain the NVR of the HTCondor prerelease build from OSG to test. Do this by talking to Tim T. and checking Koji. Shut down HTCondor and HTCondor-CE on prerelease machines: ansible 'testing:&ces' -i inventory -bK -f 20 -m service -a 'name=condor-ce state=stopped' ansible 'testing:&condor' -i inventory -bK -f 20 -m service -a 'name=condor state=stopped' Install new version of HTCondor on prerelease machines: ansible 'testing:&condor' -i inventory -bK -f 10 -m command -a 'yum --enablerepo=osg-development --assumeyes update condor' or, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed: ansible 'testing:&condor' -i inventory -bK -f 10 -m command -a 'yum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp' Verify correct RPM versions across the site: ansible condor -i inventory -f 20 -o -m command -a 'rpm -q condor' Restart HTCondor and HTCondor-CE on prerelease machines: ansible 'testing:&condor' -i inventory -bK -f 20 -m service -a 'name=condor state=started' ansible 'testing:&ces' -i inventory -bK -f 20 -m service -a 'name=condor-ce state=started' Installing a Prerelease of the OSG Software Stack \u00b6 Use this section to install new versions of all OSG software from a prerelease repository in Yum. Check with the Release Manager to make sure that the prerelease repository has been populated with the desired package versions. Make sure that software is generally up-to-date on the hosts \u2014 see the Madison ITB Site doc for more details It may be desirable to update only non-OSG software at this stage, in which case one could simply disable the OSG repositories by adding command-line options to the yum update commands. Install new software on prerelease hosts: ansible testing -i inventory -bK -f 20 -m command -a 'yum --enablerepo=osg-prerelease --assumeyes update' Read the Yum output carefully, and follow up on any warnings, etc. If the osg-configure package was updated on any host(s), run the osg-configure command on the host(s): ansible testing -i inventory -bK -f 20 -m command -a 'osg-configure -v' -l [HOST(S)] ansible testing -i inventory -bK -f 20 -m command -a 'osg-configure -c' -l [HOST(S)] Verify OSG software updates by inspecting the Yum output carefully or examining specific package versions: ansible current -i inventory -f 20 -o -m command -a 'rpm -q osg-wn-client' Use an inventory group and package names that best fit the situation. Running Tests \u00b6 For the first two test workflows, use your personal space on itb-submit . Copy or checkout the osgitb/htcondor-tests directory to get the test directories. Part \u2160: Submitting jobs directly \u00b6 Change into the 1-direct-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in a few minutes. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt You should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per machine and the fact that HTCondor can and will reuse claims to process many jobs on a single host. Especially watch out for a case in which no jobs run on the newly updated hosts (at the time of writing: itb-data[456] ). (Optional) Clean up, using the make clean or make distclean commands. Use the clean target to remove intermediate result and log files generated by a workflow run but preserve the final output file; use the distclean target to remove all workflow-generated files (plus Emacs backup files). Part \u2161: Submitting jobs using HTCondor-C \u00b6 If direct submissions fail, there is probably no point to doing this step. Change into the 2-htcondor-c-jobs subdirectory If there are old result files in the directory, remove them: make distclean Get a proxy for your X.509 credentials voms-proxy-init Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the direct submission test, because of the layers of indirection. Also, status updates from the CEs back to the submit host are infrequent. For direct information about the CEs, log in to itb-ce1 and itb-ce2 to check status; don\u2019t forget to check both condor_ce_q and condor_q on the CEs, probably in that order. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt Again, look for a reasonable distribution of jobs by hostname. (Optional) Clean up, using the make clean or make distclean commands. Part \u2162: Submitting jobs from a GlideinWMS VO Frontend \u00b6 For this workflow, use your personal space on glidein3.chtc.wisc.edu . Copy or checkout the osgitb/htcondor-tests directory to get the test directories. Again, if previous steps fail, do not bother with this step. Change into the 3-frontend-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck This workflow could take much longer than the first two, maybe an hour or so. Also, unless there are active glideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs. Thus it is helpful to monitor condor_q -totals until all of the jobs are submitted (there should be 2001), then switch to monitoring condor_status until glideins start appearing. After the first jobs start running and finishing, it is probably safe to ignore the rest of the run. If the jobs do not appear in the local queue, if glideins do not appear, or if jobs do not start running on the glideins, it is time to start troubleshooting. Check the final output file: cat count-by-hostnames.txt The distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in which pilots ramp up over time and how HTCondor allocates jobs to slots. (Optional) Clean up, using the make clean or make distclean commands.","title":"ITB Prerelease Testing"},{"location":"release/itb-testing/#testing-osg-software-prereleases-on-the-madison-itb-site","text":"This document contains basic recipes for testing a OSG software prereleases on the Madison ITB site, which includes HTCondor prerelease builds and full OSG software stack prereleases from Yum.","title":"Testing OSG Software Prereleases on the Madison ITB Site"},{"location":"release/itb-testing/#prerequisites","text":"The following items are known prerequisites to using this recipe. If you are not running the Ansible commands from osghost, there are almost certainly other prerequisites that are not listed below. And even using osghost for Ansible and itb-submit for the submissions, there may be other prerequisites missing. Please improve this document by adding other prerequisites as they are identified! A checkout of the osgitb directory from our local git instance (not GitHub) Your X.509 DN in the osgitb/unmanaged/htcondor-ce/grid-mapfile file and (via Ansible) on itb-ce1 and itb-ce2","title":"Prerequisites"},{"location":"release/itb-testing/#gathering-information","text":"Technically skippable, this section is about checking on the state of the ITB machines before making changes. The plan is to keep the ITB machines generally up-to-date independently, so those steps are not listed here. And honestly, the steps below are just some ideas; do whatever makes sense for the given update. The commands can be run as-is from within the osgitb directory from git. Check OS versions for all current ITB hosts: ansible current -i inventory -f 20 -o -m command -a 'cat /etc/redhat-release' Check the date and time on all hosts (in case NTP stops working): ansible current -i inventory -f 20 -o -m command -a 'date' Check software versions for certain hosts (e.g., for the condor package on hosts in the workers group): ansible workers -i inventory -f 20 -o -m command -a 'rpm -q condor'","title":"Gathering Information"},{"location":"release/itb-testing/#installing-htcondor-prerelease","text":"Use this section to install a new version of HTCondor, specifically a prerelease build from the development or upcoming-development repository, on the test hosts. Obtain the NVR of the HTCondor prerelease build from OSG to test. Do this by talking to Tim T. and checking Koji. Shut down HTCondor and HTCondor-CE on prerelease machines: ansible 'testing:&ces' -i inventory -bK -f 20 -m service -a 'name=condor-ce state=stopped' ansible 'testing:&condor' -i inventory -bK -f 20 -m service -a 'name=condor state=stopped' Install new version of HTCondor on prerelease machines: ansible 'testing:&condor' -i inventory -bK -f 10 -m command -a 'yum --enablerepo=osg-development --assumeyes update condor' or, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed: ansible 'testing:&condor' -i inventory -bK -f 10 -m command -a 'yum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp' Verify correct RPM versions across the site: ansible condor -i inventory -f 20 -o -m command -a 'rpm -q condor' Restart HTCondor and HTCondor-CE on prerelease machines: ansible 'testing:&condor' -i inventory -bK -f 20 -m service -a 'name=condor state=started' ansible 'testing:&ces' -i inventory -bK -f 20 -m service -a 'name=condor-ce state=started'","title":"Installing HTCondor Prerelease"},{"location":"release/itb-testing/#installing-a-prerelease-of-the-osg-software-stack","text":"Use this section to install new versions of all OSG software from a prerelease repository in Yum. Check with the Release Manager to make sure that the prerelease repository has been populated with the desired package versions. Make sure that software is generally up-to-date on the hosts \u2014 see the Madison ITB Site doc for more details It may be desirable to update only non-OSG software at this stage, in which case one could simply disable the OSG repositories by adding command-line options to the yum update commands. Install new software on prerelease hosts: ansible testing -i inventory -bK -f 20 -m command -a 'yum --enablerepo=osg-prerelease --assumeyes update' Read the Yum output carefully, and follow up on any warnings, etc. If the osg-configure package was updated on any host(s), run the osg-configure command on the host(s): ansible testing -i inventory -bK -f 20 -m command -a 'osg-configure -v' -l [HOST(S)] ansible testing -i inventory -bK -f 20 -m command -a 'osg-configure -c' -l [HOST(S)] Verify OSG software updates by inspecting the Yum output carefully or examining specific package versions: ansible current -i inventory -f 20 -o -m command -a 'rpm -q osg-wn-client' Use an inventory group and package names that best fit the situation.","title":"Installing a Prerelease of the OSG Software Stack"},{"location":"release/itb-testing/#running-tests","text":"For the first two test workflows, use your personal space on itb-submit . Copy or checkout the osgitb/htcondor-tests directory to get the test directories.","title":"Running Tests"},{"location":"release/itb-testing/#part-i-submitting-jobs-directly","text":"Change into the 1-direct-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in a few minutes. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt You should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per machine and the fact that HTCondor can and will reuse claims to process many jobs on a single host. Especially watch out for a case in which no jobs run on the newly updated hosts (at the time of writing: itb-data[456] ). (Optional) Clean up, using the make clean or make distclean commands. Use the clean target to remove intermediate result and log files generated by a workflow run but preserve the final output file; use the distclean target to remove all workflow-generated files (plus Emacs backup files).","title":"Part \u2160: Submitting jobs directly"},{"location":"release/itb-testing/#part-ii-submitting-jobs-using-htcondor-c","text":"If direct submissions fail, there is probably no point to doing this step. Change into the 2-htcondor-c-jobs subdirectory If there are old result files in the directory, remove them: make distclean Get a proxy for your X.509 credentials voms-proxy-init Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the direct submission test, because of the layers of indirection. Also, status updates from the CEs back to the submit host are infrequent. For direct information about the CEs, log in to itb-ce1 and itb-ce2 to check status; don\u2019t forget to check both condor_ce_q and condor_q on the CEs, probably in that order. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt Again, look for a reasonable distribution of jobs by hostname. (Optional) Clean up, using the make clean or make distclean commands.","title":"Part \u2161: Submitting jobs using HTCondor-C"},{"location":"release/itb-testing/#part-iii-submitting-jobs-from-a-glideinwms-vo-frontend","text":"For this workflow, use your personal space on glidein3.chtc.wisc.edu . Copy or checkout the osgitb/htcondor-tests directory to get the test directories. Again, if previous steps fail, do not bother with this step. Change into the 3-frontend-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck This workflow could take much longer than the first two, maybe an hour or so. Also, unless there are active glideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs. Thus it is helpful to monitor condor_q -totals until all of the jobs are submitted (there should be 2001), then switch to monitoring condor_status until glideins start appearing. After the first jobs start running and finishing, it is probably safe to ignore the rest of the run. If the jobs do not appear in the local queue, if glideins do not appear, or if jobs do not start running on the glideins, it is time to start troubleshooting. Check the final output file: cat count-by-hostnames.txt The distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in which pilots ramp up over time and how HTCondor allocates jobs to slots. (Optional) Clean up, using the make clean or make distclean commands.","title":"Part \u2162: Submitting jobs from a GlideinWMS VO Frontend"},{"location":"release/new-os-series/","text":"How to Add a New Enterprise Linux Series \u00b6 Throughout this document, we will refer to the new Enterprise Linux series as ELX , and the previous EL series as ELX.OLD . For example, if we are adding el9, then ELX refers to el9 , and ELX.OLD refers to el8 . This document explains how to add support for a new ELX series to an existing OSG series. For adding a new OSG series, see the New Release Series documentation. See the documentation in the OSG Technology/Software and Release/Infrastructure Google Drive folder for details on the infrastructure. Prepare Koji and OSG-Build \u00b6 Add ELX Koji tags and targets Modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/master/koji/create-new-koji-elX-tags-etc In particular, update EL as appropriate (eg, el9 ), and update the ### external repos ### section with a new block of external repos to add to the dist-$EL-build tag. Then for the OSG-series specific tags, modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/master/koji/create-new-koji-osg3X-tags-etc In particular, set SERIES to the current OSG series, and include ONLY the new ELX in the EL loop (eg, el9 ). (Do not include EL versions that already exist for this series.) Then for the devops tags, modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/master/koji/create-new-koji-devops-tags-etc In particular, set SERIES to the current OSG series, and set EL to the new ELX in the EL loop (eg, el9 ). Add Koji package signing, as necessary With any luck, you can use the existing RPM signing key for the OSG series to which you are adding the new EL series. If it turns out that you need to create a new RPM signing key for ELX (because reusing the one for the current OSG series doesn't work in ELX for some unexpected reason), then you will need to generate a new key in the keyring for the Koji sign plugin, and save it (encrypted) in the Koji Ansible config. See Infrastructure Google Drive folder for details. Either way, you will need to make modifications to the osg-services repo, gitolite@git.chtc.wisc.edu:osg-services.git , so get a checkout of that ready. $ ssh koji.chtc.wisc.edu $ git clone gitolite@git.chtc.wisc.edu:osg-services.git $ cd osg-services If you are adding a new RPM signing key, you need to edit koji/roles/signplugin/vars/main.yml to add the key name, password, and list of tags the key should be used for; and koji/roles/signplugin/templates/sign.conf.j2 to add template code for generating the sign.conf config blocks for those tags. If you are using the existing RPM signing key for the OSG series, you only need to edit koji/roles/signplugin/vars/main.yml . Find the tags section for your current signing key, eg, osg3_build_tags , and add all ELX build tags to this section. Eg, for el9 to OSG 3.6: osg3_build_tags: - dist-el9-build - osg-el9-internal-build - osg-3.6-el9-build - osg-3.6-upcoming-el9-build Apply the Koji Ansible config on the Koji Hub host. # ssh koji.chtc.wisc.edu # git clone gitolite@git.chtc.wisc.edu:osg-services.git $ cd ~/osg-services/koji $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --check --diff secure.yml # If the above looks good, run again without --check to apply for real $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --diff secure.yml # -K = prompt you for sudo password ( BECOME password ) # -c local = use the \"local\" connection method # -l $( hostname -f ) = apply the changes to the current machine only # --ask-vault-pass = prompt you for the ansible-vault pw # --check = dry-run mode # --diff = show the diffs of any file changes it ( would ) make # secure.yml = the \"playbook\" of changes to apply If you are adding a new RPM signing key, export the ASCII-armored public key as RPM-GPG-OSG-KEY-OSG-<N> (where <N> is the previous key's number incremented by one), and add it to the osg-release RPM. In the script generate-repo-files.sh ensure that the logic for selecting the GPGKEY includes the correct behavior for the new ELX to reference the latest key file. Update osg-build to add the new ELX to the various dvers in python scripts, and extra_dvers in promoter.ini ; and add the new ELX tags and targets to the test scripts. See the Git commits on opensciencegrid/osg-build for SOFTWARE-5342 for details on how to do this. Use this version of osg-build for subsequent steps. Subsequent Steps \u00b6 This section is incomplete. But for starters, begin with the Build prerequisite packages section of the New Release Series documentation. In general, you will not have to repeat steps for creating a new osg-3.Y series, but you will have to create a new buildsys-macros.elX package for the new EL9 series. Good luck.","title":"New OS Series"},{"location":"release/new-os-series/#how-to-add-a-new-enterprise-linux-series","text":"Throughout this document, we will refer to the new Enterprise Linux series as ELX , and the previous EL series as ELX.OLD . For example, if we are adding el9, then ELX refers to el9 , and ELX.OLD refers to el8 . This document explains how to add support for a new ELX series to an existing OSG series. For adding a new OSG series, see the New Release Series documentation. See the documentation in the OSG Technology/Software and Release/Infrastructure Google Drive folder for details on the infrastructure.","title":"How to Add a New Enterprise Linux Series"},{"location":"release/new-os-series/#prepare-koji-and-osg-build","text":"Add ELX Koji tags and targets Modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/master/koji/create-new-koji-elX-tags-etc In particular, update EL as appropriate (eg, el9 ), and update the ### external repos ### section with a new block of external repos to add to the dist-$EL-build tag. Then for the OSG-series specific tags, modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/master/koji/create-new-koji-osg3X-tags-etc In particular, set SERIES to the current OSG series, and include ONLY the new ELX in the EL loop (eg, el9 ). (Do not include EL versions that already exist for this series.) Then for the devops tags, modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/master/koji/create-new-koji-devops-tags-etc In particular, set SERIES to the current OSG series, and set EL to the new ELX in the EL loop (eg, el9 ). Add Koji package signing, as necessary With any luck, you can use the existing RPM signing key for the OSG series to which you are adding the new EL series. If it turns out that you need to create a new RPM signing key for ELX (because reusing the one for the current OSG series doesn't work in ELX for some unexpected reason), then you will need to generate a new key in the keyring for the Koji sign plugin, and save it (encrypted) in the Koji Ansible config. See Infrastructure Google Drive folder for details. Either way, you will need to make modifications to the osg-services repo, gitolite@git.chtc.wisc.edu:osg-services.git , so get a checkout of that ready. $ ssh koji.chtc.wisc.edu $ git clone gitolite@git.chtc.wisc.edu:osg-services.git $ cd osg-services If you are adding a new RPM signing key, you need to edit koji/roles/signplugin/vars/main.yml to add the key name, password, and list of tags the key should be used for; and koji/roles/signplugin/templates/sign.conf.j2 to add template code for generating the sign.conf config blocks for those tags. If you are using the existing RPM signing key for the OSG series, you only need to edit koji/roles/signplugin/vars/main.yml . Find the tags section for your current signing key, eg, osg3_build_tags , and add all ELX build tags to this section. Eg, for el9 to OSG 3.6: osg3_build_tags: - dist-el9-build - osg-el9-internal-build - osg-3.6-el9-build - osg-3.6-upcoming-el9-build Apply the Koji Ansible config on the Koji Hub host. # ssh koji.chtc.wisc.edu # git clone gitolite@git.chtc.wisc.edu:osg-services.git $ cd ~/osg-services/koji $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --check --diff secure.yml # If the above looks good, run again without --check to apply for real $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --diff secure.yml # -K = prompt you for sudo password ( BECOME password ) # -c local = use the \"local\" connection method # -l $( hostname -f ) = apply the changes to the current machine only # --ask-vault-pass = prompt you for the ansible-vault pw # --check = dry-run mode # --diff = show the diffs of any file changes it ( would ) make # secure.yml = the \"playbook\" of changes to apply If you are adding a new RPM signing key, export the ASCII-armored public key as RPM-GPG-OSG-KEY-OSG-<N> (where <N> is the previous key's number incremented by one), and add it to the osg-release RPM. In the script generate-repo-files.sh ensure that the logic for selecting the GPGKEY includes the correct behavior for the new ELX to reference the latest key file. Update osg-build to add the new ELX to the various dvers in python scripts, and extra_dvers in promoter.ini ; and add the new ELX tags and targets to the test scripts. See the Git commits on opensciencegrid/osg-build for SOFTWARE-5342 for details on how to do this. Use this version of osg-build for subsequent steps.","title":"Prepare Koji and OSG-Build"},{"location":"release/new-os-series/#subsequent-steps","text":"This section is incomplete. But for starters, begin with the Build prerequisite packages section of the New Release Series documentation. In general, you will not have to repeat steps for creating a new osg-3.Y series, but you will have to create a new buildsys-macros.elX package for the new EL9 series. Good luck.","title":"Subsequent Steps"},{"location":"release/new-release-series/","text":"How to Prepare a New Release Series \u00b6 Throughout this document, we will refer to the new release series as 2X , and the previous release series as OLD . For example, if we are creating OSG 25, then 2X refers to 25 , and OLD refers to 24 . See the documentation in the OSG Technology/Software and Release/Infrastructure Google Drive folder for details on the infrastructure. Prepare Koji and OSG-Build \u00b6 Add 2X-main, 2X-internal, 2X-upcoming, 2X-empty, and 2X-contrib Koji tags and targets Duplicate this set of scripts into a new koji/osg-2X directory, modify as appropriate, and run: https://github.com/opensciencegrid/osg-next-tools/tree/master/koji/osg-24 In particular, update SERIES as appropriate, and include any applicable Enterprise Linux versions to the el loop (eg, el8 el9 ) Add Koji package signing Starting with OSG 23, we've been using a set of two RPM signing keys for each new release series: An \"auto\" key, used to sign development RPMs on-build in Kojihub. A \"developer\" key, used to sign RPMs upon promotion from development to testing. These keys should be generated and placed on a Yubikey, then installed on the Kojihub host. See OSG Yubikey Generation for up-to-date documentation on this process. Edit koji/roles/signplugin/vars/main.yml and koji/roles/signplugin/templates/sign.conf.j2 in the Koji Ansible config to add the key name, Yubikey PIN, list of tags the key should be used for, and template code for generating the sign.conf config blocks for those tags. Tags for EL9 and newer distros should have gpg_digest_algo = sha256 set. Apply the Koji Ansible config on the Koji Hub host. $ ssh koji.chtc.wisc.edu $ git clone gitolite@git.chtc.wisc.edu:osg-services.git $ cd osg-services/koji $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --check --diff secure.yml # If the above looks good, run again without --check to apply for real $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --diff secure.yml # -K = prompt you for sudo password ( BECOME password ) # -c local = use the \"local\" connection method # -l $( hostname -f ) = apply the changes to the current machine only # --ask-vault-pass = prompt you for the ansible-vault pw # --check = dry-run mode # --diff = show the diffs of any file changes it ( would ) make # secure.yml = the \"playbook\" of changes to apply Export the ASCII-armored public key as RPM-GPG-OSG-KEY-OSG-2X-auto , and add it to the osg-release RPM. Add the file and modify the template.repo.* files to reference the new key file. Update Koji policy as needed (for new distro versions); see SOFTWARE-5426 for details. Update osg-build to use the new koji tags and targets (not by default of course). See the Git commits on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this. Use this version for subsequent steps. Build prerequisite packages \u00b6 Create a blank X-main SVN branch and add buildsys-macros.elY packages, one for each supported distro version. As an example, here's what you'd do for osg-24 and el8: svn copy the buildsys-macros.elX directories from the osg-OLD branch and hand-edit it to hardcode the new osg_version and dver values. $ cd native/redhat/branches $ svn mkdir 24 -main $ svn copy 23 -main/buildsys-macros.el8 24 -main/buildsys-macros.el8 $ cd 24 -main/buildsys-macros.el8 $ $EDITOR osg/*.spec # ## change the osg_version and dver values as appropriate Build locally and import the resulting RPMs (need Koji admin permissions). Run the following commands (adjust the NVR and distro version as necessary): :::console $ osg-build rpmbuild --el8 $ osg-koji import _build_results/buildsys-macros-*.el8.src.rpm $ osg-koji import _build_results/buildsys-macros-*.el8.noarch.rpm $ pkg=$(basename _build_results/buildsys-macros-*.el8.src.rpm .src.rpm) $ osg-koji tag-pkg osg-23-main-el8-development \"$pkg\" Bump the revision in each buildsys-macros.elY spec file and edit the %changelog , svn commit , then do Koji builds of them. Again, with osg-24 and el8: $ osg-build koji --repo = 24 -main --el8 24 -main/buildsys-macros.el8 Repeat the previous steps for each of the following: 2X-upcoming 2X-internal 2X-empty 2X-contrib Update tarball-client bundles.ini patches/ upload-tarballs-to-oasis (for X, foreach_dver_arch will need to be updated for the new set of X dver_arches ) Add relase-series specific repos/osg-23-main-el<DVER>.repo.in for each supported distro version (e.g., 8 , 9 ) Populate the bootstrap tags Need to have them inherit from the OLD development tags, but only packages, not builds (hence the --noconfig ; yes, the name is weird) # set OLD and NEW as appropriate, specify any relevant dvers for el $ for el in el8 el9 ; do osg-koji add-tag-inheritance --noconfig --priority=2 \\ osg-$NEW-main-$el-bootstrap osg-$OLD-main-$el-development; osg-koji add-tag-inheritance --noconfig --priority=3 \\ osg-$NEW-main-$el-bootstrap osg-$OLD-upcoming-$el-development; done Get the actual NVRs to tag I put Brian's spreadsheet into Excel and used its filtering feature to separate out: the packages going into 2X.0 package differences between each dver (eg, el7 vs el8) save the NVRs for each dver to a separate file, eg, pkgtotag-el7.txt and pkgtotag-el8.txt Tagging: # set X as appropriate, specify any relevant dvers for el $ for el in el8 el9 ; do \\ xargs -a pkgtotag-$el osg-koji tag-pkg osg-X-main-$el-bootstrap; \\ done (btw, xargs -a doesn't work on a Mac) In order to make testing easier, build the new osg-release and osg-release-itb packages and promote them all the way to release, so that all the 2X repos exist and have at least one rpm in them. Prepare repo and test infrastructure \u00b6 Update mash to pull from the new tags, using the new key On repo-itb On repo Put the new public key on repo and repo-itb Update documentation here Update osg-test / vmu-test-runs They're only going to test from minefield (and eventually testing) until the release Build software \u00b6 Populate SVN branches and tags (as in fill it with the packages we're going to release for 2X-main, 2X-upcoming, etc.) Mass rebuild Drop the osg-X-main-elY-bootstrap koji tags (after the successful mass rebuild only) Update docker-software-base and any container images that are based on it Release! \u00b6 Update release tools scripts as necessary Cut a release Have a release party Update this document and the Infrastructure Google Drive folder with issues you ran into Post-release \u00b6 Update osg-test / vmu-test-runs again to add release and release -> testing tests Update the tarball that is used to keep the CA certificates and VO data current in CVMFS. Logon as ouser.mis@oasis-login.opensciencegrid.org and follow the directions in the README file. Update the koji osg-elY build targets to build from and to X instead of OLD ; notify the software-discuss list of this change Update the docker-osg-wn-client scripts to build from X (need direct push access) Update the constants in the genbranches script in the docker-osg-wn-scripts repo Update the branches in docker-osg-wn-client ; a script like this ought to work: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git cd docker-osg-wn-scripts ./genbranches cd ../docker-osg-wn for bpath in ../docker-osg-wn-scripts/branches/* ; do b = ${ bpath ##*/ } git checkout -b $b master && \\ mv $bpath Dockerfile.in && \\ git add Dockerfile.in && \\ git commit -m \"Add branch $b \" done and then run a similar script to update the existing branches Check the results before pushing, and then run git push --all Update the arrays in update-all and osg-wn-nightly-build in docker-osg-wn-scripts Update the default promotion route aliases in osg-promote Update documentation again to reflect that X is now the main branch and OLD is the maintenance branch","title":"New Release Series"},{"location":"release/new-release-series/#how-to-prepare-a-new-release-series","text":"Throughout this document, we will refer to the new release series as 2X , and the previous release series as OLD . For example, if we are creating OSG 25, then 2X refers to 25 , and OLD refers to 24 . See the documentation in the OSG Technology/Software and Release/Infrastructure Google Drive folder for details on the infrastructure.","title":"How to Prepare a New Release Series"},{"location":"release/new-release-series/#prepare-koji-and-osg-build","text":"Add 2X-main, 2X-internal, 2X-upcoming, 2X-empty, and 2X-contrib Koji tags and targets Duplicate this set of scripts into a new koji/osg-2X directory, modify as appropriate, and run: https://github.com/opensciencegrid/osg-next-tools/tree/master/koji/osg-24 In particular, update SERIES as appropriate, and include any applicable Enterprise Linux versions to the el loop (eg, el8 el9 ) Add Koji package signing Starting with OSG 23, we've been using a set of two RPM signing keys for each new release series: An \"auto\" key, used to sign development RPMs on-build in Kojihub. A \"developer\" key, used to sign RPMs upon promotion from development to testing. These keys should be generated and placed on a Yubikey, then installed on the Kojihub host. See OSG Yubikey Generation for up-to-date documentation on this process. Edit koji/roles/signplugin/vars/main.yml and koji/roles/signplugin/templates/sign.conf.j2 in the Koji Ansible config to add the key name, Yubikey PIN, list of tags the key should be used for, and template code for generating the sign.conf config blocks for those tags. Tags for EL9 and newer distros should have gpg_digest_algo = sha256 set. Apply the Koji Ansible config on the Koji Hub host. $ ssh koji.chtc.wisc.edu $ git clone gitolite@git.chtc.wisc.edu:osg-services.git $ cd osg-services/koji $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --check --diff secure.yml # If the above looks good, run again without --check to apply for real $ ansible-playbook -K -c local -l $( hostname -f ) --ask-vault-pass --diff secure.yml # -K = prompt you for sudo password ( BECOME password ) # -c local = use the \"local\" connection method # -l $( hostname -f ) = apply the changes to the current machine only # --ask-vault-pass = prompt you for the ansible-vault pw # --check = dry-run mode # --diff = show the diffs of any file changes it ( would ) make # secure.yml = the \"playbook\" of changes to apply Export the ASCII-armored public key as RPM-GPG-OSG-KEY-OSG-2X-auto , and add it to the osg-release RPM. Add the file and modify the template.repo.* files to reference the new key file. Update Koji policy as needed (for new distro versions); see SOFTWARE-5426 for details. Update osg-build to use the new koji tags and targets (not by default of course). See the Git commits on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this. Use this version for subsequent steps.","title":"Prepare Koji and OSG-Build"},{"location":"release/new-release-series/#build-prerequisite-packages","text":"Create a blank X-main SVN branch and add buildsys-macros.elY packages, one for each supported distro version. As an example, here's what you'd do for osg-24 and el8: svn copy the buildsys-macros.elX directories from the osg-OLD branch and hand-edit it to hardcode the new osg_version and dver values. $ cd native/redhat/branches $ svn mkdir 24 -main $ svn copy 23 -main/buildsys-macros.el8 24 -main/buildsys-macros.el8 $ cd 24 -main/buildsys-macros.el8 $ $EDITOR osg/*.spec # ## change the osg_version and dver values as appropriate Build locally and import the resulting RPMs (need Koji admin permissions). Run the following commands (adjust the NVR and distro version as necessary): :::console $ osg-build rpmbuild --el8 $ osg-koji import _build_results/buildsys-macros-*.el8.src.rpm $ osg-koji import _build_results/buildsys-macros-*.el8.noarch.rpm $ pkg=$(basename _build_results/buildsys-macros-*.el8.src.rpm .src.rpm) $ osg-koji tag-pkg osg-23-main-el8-development \"$pkg\" Bump the revision in each buildsys-macros.elY spec file and edit the %changelog , svn commit , then do Koji builds of them. Again, with osg-24 and el8: $ osg-build koji --repo = 24 -main --el8 24 -main/buildsys-macros.el8 Repeat the previous steps for each of the following: 2X-upcoming 2X-internal 2X-empty 2X-contrib Update tarball-client bundles.ini patches/ upload-tarballs-to-oasis (for X, foreach_dver_arch will need to be updated for the new set of X dver_arches ) Add relase-series specific repos/osg-23-main-el<DVER>.repo.in for each supported distro version (e.g., 8 , 9 ) Populate the bootstrap tags Need to have them inherit from the OLD development tags, but only packages, not builds (hence the --noconfig ; yes, the name is weird) # set OLD and NEW as appropriate, specify any relevant dvers for el $ for el in el8 el9 ; do osg-koji add-tag-inheritance --noconfig --priority=2 \\ osg-$NEW-main-$el-bootstrap osg-$OLD-main-$el-development; osg-koji add-tag-inheritance --noconfig --priority=3 \\ osg-$NEW-main-$el-bootstrap osg-$OLD-upcoming-$el-development; done Get the actual NVRs to tag I put Brian's spreadsheet into Excel and used its filtering feature to separate out: the packages going into 2X.0 package differences between each dver (eg, el7 vs el8) save the NVRs for each dver to a separate file, eg, pkgtotag-el7.txt and pkgtotag-el8.txt Tagging: # set X as appropriate, specify any relevant dvers for el $ for el in el8 el9 ; do \\ xargs -a pkgtotag-$el osg-koji tag-pkg osg-X-main-$el-bootstrap; \\ done (btw, xargs -a doesn't work on a Mac) In order to make testing easier, build the new osg-release and osg-release-itb packages and promote them all the way to release, so that all the 2X repos exist and have at least one rpm in them.","title":"Build prerequisite packages"},{"location":"release/new-release-series/#prepare-repo-and-test-infrastructure","text":"Update mash to pull from the new tags, using the new key On repo-itb On repo Put the new public key on repo and repo-itb Update documentation here Update osg-test / vmu-test-runs They're only going to test from minefield (and eventually testing) until the release","title":"Prepare repo and test infrastructure"},{"location":"release/new-release-series/#build-software","text":"Populate SVN branches and tags (as in fill it with the packages we're going to release for 2X-main, 2X-upcoming, etc.) Mass rebuild Drop the osg-X-main-elY-bootstrap koji tags (after the successful mass rebuild only) Update docker-software-base and any container images that are based on it","title":"Build software"},{"location":"release/new-release-series/#release","text":"Update release tools scripts as necessary Cut a release Have a release party Update this document and the Infrastructure Google Drive folder with issues you ran into","title":"Release!"},{"location":"release/new-release-series/#post-release","text":"Update osg-test / vmu-test-runs again to add release and release -> testing tests Update the tarball that is used to keep the CA certificates and VO data current in CVMFS. Logon as ouser.mis@oasis-login.opensciencegrid.org and follow the directions in the README file. Update the koji osg-elY build targets to build from and to X instead of OLD ; notify the software-discuss list of this change Update the docker-osg-wn-client scripts to build from X (need direct push access) Update the constants in the genbranches script in the docker-osg-wn-scripts repo Update the branches in docker-osg-wn-client ; a script like this ought to work: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git cd docker-osg-wn-scripts ./genbranches cd ../docker-osg-wn for bpath in ../docker-osg-wn-scripts/branches/* ; do b = ${ bpath ##*/ } git checkout -b $b master && \\ mv $bpath Dockerfile.in && \\ git add Dockerfile.in && \\ git commit -m \"Add branch $b \" done and then run a similar script to update the existing branches Check the results before pushing, and then run git push --all Update the arrays in update-all and osg-wn-nightly-build in docker-osg-wn-scripts Update the default promotion route aliases in osg-promote Update documentation again to reflect that X is now the main branch and OLD is the maintenance branch","title":"Post-release"},{"location":"release/release-eol/","text":"Release Series End-of-Life \u00b6 When a release reaches end-of-life, we need to discountinue a few items. Remove the retired series from the OSG VMU tests. However, retain the upgrade test from the retired series to the current series. Remove any github actions that build software-base or osg-wn-client Docker images for the retired series. Previous Release Series Removal Plan \u00b6 In order to reduce clutter and disk usage on our repositories and build system, we will remove older OSG Software release series. This will result in packages from those series becoming unavailable, so we will remove a release series when its packages are no longer needed. We will remove a release series no earlier than when the following series is completely out of support. For example, OSG 3.1 will be removed when OSG 3.2 is out of support, and OSG 3.2 will be removed when OSG 3.3 is out of support. Tasks \u00b6 Removing a release series requires work from both Operations and Software & Release. The first step is to create a JIRA ticket in the SOFTWARE project to track the work. Second, Software & Release will enumerate the directories for Operations to remove. Operations tasks should be completed before Software & Release tasks. Operations \u00b6 These tasks should be completed in order. Two weeks in advance, notify sites (including mirror sites) that the release series is going away. See the template email below. Remove the series from the mash configs on the repo.opensciencegrid.org machines: Add the koji tags for the old series to the /usr/local/osg-tags.excluded file: # cd /usr/local # fgrep osg-3.1 osg-tags osg-3.1-el5-contrib osg-3.1-el5-development osg-3.1-el5-release osg-3.1-el5-testing osg-3.1-el6-contrib osg-3.1-el6-development osg-3.1-el6-release osg-3.1-el6-testing # fgrep osg-3.1 osg-tags >> osg-tags.exclude Re-run update_mashfiles.sh to update the mash config files: # ./update_mashfiles.sh Remove the appropriate repo directories from /usr/local/repo/osg . # rm -rf repo*/osg/3.1/ Reclaim space from any cached rpms in the mash cache which are no longer linked elsewhere: # find mash/cache/ -name \\* .rpm -type f -links 1 -delete Wait for mash to run and verify that the repos are no longer getting updated: Look at the mash logs in /var/log/repo . Verify that mash did not recreate the repo directory under /usr/local/repo/osg corresponding to the old release series. Remove tarballs from repo and OASIS Software & Release \u00b6 These tasks can be completed in any order. Tag and remove the SVN branch corresponding to the release series. Edit vm-test-runs and remove any \"long tail\" tests that reference the series. Edit tarball-client : Remove bundles from bundles.ini . Remove patch and other files that were used only by those bundles. Test that the current bundles didn't get broken by your changes. Edit osg-build : Remove the promotion routes from promoter.ini . Remove references in constants.py . Test your changes; also run the unit tests. Remove things from Koji: All targets referencing the series. All tags referencing the series. Remove references to the series from opensciencegrid/docker-osg-wn on GitHub. Move files in /p/vdt/public/html/release-info to its attic subdirectory. Undoing \u00b6 If we really need RPMs from a removed release series, we can look at the text files in /p/vdt/public/html/release-info/attic to determine the exact NVRs we need, and download them from Koji. Template Email \u00b6 Subject: OSG 3.X packages will be removed from the repositories YYYY-MM-DD On <DAYNAME, MONTH DAY>, the OSG will be removing the OSG <3.X> release series from our repositories. This includes both RPMs and tarballs hosted on repo.opensciencegrid.org. As a reminder, support for OSG <3.X> ended after <MONTH YEAR>. If your site is running OSG <3.X>, you should upgrade to the current release series, OSG 3.Y. See our upgrade documentation [1] for instructions. If you need assistance upgrading, please contact us at help@osg-htc.org. [1] https://osg-htc.org/docs/release/release_series/#updating-from-osg-31-32-33-to-34 If we're dropping support for a distro (e.g. EL 5 when we drop OSG 3.2), add the following after the first paragraph: Note that OSG <3.X> was the last release that supported Enterprise Linux <Z> distributions. If you believe that you still need support for this operating system series, please contact us at help@osg-htc.org. Since we're dropping support for i386 (32-bit) when we drop OSG 3.3, add the following after the first paragraph: Note that OSG 3.3 was the last release that contained 32-bit packages. If you believe that you still need support for this architecture, please contacts us at help@osg-htc.org.","title":"Release Series End-of-Life"},{"location":"release/release-eol/#release-series-end-of-life","text":"When a release reaches end-of-life, we need to discountinue a few items. Remove the retired series from the OSG VMU tests. However, retain the upgrade test from the retired series to the current series. Remove any github actions that build software-base or osg-wn-client Docker images for the retired series.","title":"Release Series End-of-Life"},{"location":"release/release-eol/#previous-release-series-removal-plan","text":"In order to reduce clutter and disk usage on our repositories and build system, we will remove older OSG Software release series. This will result in packages from those series becoming unavailable, so we will remove a release series when its packages are no longer needed. We will remove a release series no earlier than when the following series is completely out of support. For example, OSG 3.1 will be removed when OSG 3.2 is out of support, and OSG 3.2 will be removed when OSG 3.3 is out of support.","title":"Previous Release Series Removal Plan"},{"location":"release/release-eol/#tasks","text":"Removing a release series requires work from both Operations and Software & Release. The first step is to create a JIRA ticket in the SOFTWARE project to track the work. Second, Software & Release will enumerate the directories for Operations to remove. Operations tasks should be completed before Software & Release tasks.","title":"Tasks"},{"location":"release/release-eol/#operations","text":"These tasks should be completed in order. Two weeks in advance, notify sites (including mirror sites) that the release series is going away. See the template email below. Remove the series from the mash configs on the repo.opensciencegrid.org machines: Add the koji tags for the old series to the /usr/local/osg-tags.excluded file: # cd /usr/local # fgrep osg-3.1 osg-tags osg-3.1-el5-contrib osg-3.1-el5-development osg-3.1-el5-release osg-3.1-el5-testing osg-3.1-el6-contrib osg-3.1-el6-development osg-3.1-el6-release osg-3.1-el6-testing # fgrep osg-3.1 osg-tags >> osg-tags.exclude Re-run update_mashfiles.sh to update the mash config files: # ./update_mashfiles.sh Remove the appropriate repo directories from /usr/local/repo/osg . # rm -rf repo*/osg/3.1/ Reclaim space from any cached rpms in the mash cache which are no longer linked elsewhere: # find mash/cache/ -name \\* .rpm -type f -links 1 -delete Wait for mash to run and verify that the repos are no longer getting updated: Look at the mash logs in /var/log/repo . Verify that mash did not recreate the repo directory under /usr/local/repo/osg corresponding to the old release series. Remove tarballs from repo and OASIS","title":"Operations"},{"location":"release/release-eol/#software-release","text":"These tasks can be completed in any order. Tag and remove the SVN branch corresponding to the release series. Edit vm-test-runs and remove any \"long tail\" tests that reference the series. Edit tarball-client : Remove bundles from bundles.ini . Remove patch and other files that were used only by those bundles. Test that the current bundles didn't get broken by your changes. Edit osg-build : Remove the promotion routes from promoter.ini . Remove references in constants.py . Test your changes; also run the unit tests. Remove things from Koji: All targets referencing the series. All tags referencing the series. Remove references to the series from opensciencegrid/docker-osg-wn on GitHub. Move files in /p/vdt/public/html/release-info to its attic subdirectory.","title":"Software &amp; Release"},{"location":"release/release-eol/#undoing","text":"If we really need RPMs from a removed release series, we can look at the text files in /p/vdt/public/html/release-info/attic to determine the exact NVRs we need, and download them from Koji.","title":"Undoing"},{"location":"release/release-eol/#template-email","text":"Subject: OSG 3.X packages will be removed from the repositories YYYY-MM-DD On <DAYNAME, MONTH DAY>, the OSG will be removing the OSG <3.X> release series from our repositories. This includes both RPMs and tarballs hosted on repo.opensciencegrid.org. As a reminder, support for OSG <3.X> ended after <MONTH YEAR>. If your site is running OSG <3.X>, you should upgrade to the current release series, OSG 3.Y. See our upgrade documentation [1] for instructions. If you need assistance upgrading, please contact us at help@osg-htc.org. [1] https://osg-htc.org/docs/release/release_series/#updating-from-osg-31-32-33-to-34 If we're dropping support for a distro (e.g. EL 5 when we drop OSG 3.2), add the following after the first paragraph: Note that OSG <3.X> was the last release that supported Enterprise Linux <Z> distributions. If you believe that you still need support for this operating system series, please contact us at help@osg-htc.org. Since we're dropping support for i386 (32-bit) when we drop OSG 3.3, add the following after the first paragraph: Note that OSG 3.3 was the last release that contained 32-bit packages. If you believe that you still need support for this architecture, please contacts us at help@osg-htc.org.","title":"Template Email"},{"location":"software/ce-test-scaling/","text":"How to Run Scalability Tests on a CE \u00b6 Introduction \u00b6 This document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an HTCondor-CE , but they should be applicable for other CE software. The focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued. Sleeper Pool \u00b6 With the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs. When large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work. Setting Up a Sleeper Pool \u00b6 A sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine: START = ( \\ (SlotID >= 1) && \\ (SlotID < 33) && \\ (RequiresWholeMachine =!= TRUE ) && \\ (SleepSlot =!= TRUE) && \\ (distro =?= \"RHEL6\" ) && \\ (CPU_Only == TRUE ) \\ ) || \\ ( (SlotID >= 33) && (distro =?= \"RHEL6\" ) && (SleepSlot == TRUE) ) Usual Topology of the Tests \u00b6 A brief introduction to the topology involved in the tests. Batch System and Sleeper Pool \u00b6 This is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator. CE \u00b6 This is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications. Submitter \u00b6 An HTCondor submit host. It can be a virtual machine for most test submissions. Monitoring tools \u00b6 To monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example: condor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const 'JobStatus=?=2' | wc -l gmetric --name RunningJobsCE Generating Load \u00b6 Location \u00b6 The load_generators are found in the OSgscal github repo . The binary of interest here is loadtest_condor Use \u00b6 Just untar it or check it out from mas on the HTCondor submit node (see above): git checkout https://github.com/efajardo/osgscal cd load_generators/loadtest_condor/trunk/bin Keep in mind that you also need a valid proxy for grid submissions. For example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command: ./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50","title":"CE Scale Testing"},{"location":"software/ce-test-scaling/#how-to-run-scalability-tests-on-a-ce","text":"","title":"How to Run Scalability Tests on a CE"},{"location":"software/ce-test-scaling/#introduction","text":"This document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an HTCondor-CE , but they should be applicable for other CE software. The focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued.","title":"Introduction"},{"location":"software/ce-test-scaling/#sleeper-pool","text":"With the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs. When large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work.","title":"Sleeper Pool"},{"location":"software/ce-test-scaling/#setting-up-a-sleeper-pool","text":"A sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine: START = ( \\ (SlotID >= 1) && \\ (SlotID < 33) && \\ (RequiresWholeMachine =!= TRUE ) && \\ (SleepSlot =!= TRUE) && \\ (distro =?= \"RHEL6\" ) && \\ (CPU_Only == TRUE ) \\ ) || \\ ( (SlotID >= 33) && (distro =?= \"RHEL6\" ) && (SleepSlot == TRUE) )","title":"Setting Up a Sleeper Pool"},{"location":"software/ce-test-scaling/#usual-topology-of-the-tests","text":"A brief introduction to the topology involved in the tests.","title":"Usual Topology of the Tests"},{"location":"software/ce-test-scaling/#batch-system-and-sleeper-pool","text":"This is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator.","title":"Batch System and Sleeper Pool"},{"location":"software/ce-test-scaling/#ce","text":"This is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications.","title":"CE"},{"location":"software/ce-test-scaling/#submitter","text":"An HTCondor submit host. It can be a virtual machine for most test submissions.","title":"Submitter"},{"location":"software/ce-test-scaling/#monitoring-tools","text":"To monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example: condor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const 'JobStatus=?=2' | wc -l gmetric --name RunningJobsCE","title":"Monitoring tools"},{"location":"software/ce-test-scaling/#generating-load","text":"","title":"Generating Load"},{"location":"software/ce-test-scaling/#location","text":"The load_generators are found in the OSgscal github repo . The binary of interest here is loadtest_condor","title":"Location"},{"location":"software/ce-test-scaling/#use","text":"Just untar it or check it out from mas on the HTCondor submit node (see above): git checkout https://github.com/efajardo/osgscal cd load_generators/loadtest_condor/trunk/bin Keep in mind that you also need a valid proxy for grid submissions. For example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command: ./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50","title":"Use"},{"location":"software/container-development-guide/","text":"Container Development Guide \u00b6 This document contains instructions for OSG Technology Team members, including: How to to develop OSG Software container images that are automatically pushed to Docker Hub that adhere to our container release policy How to build a new version of an existing image How to manage tags for images in the OSG DockerHub organization Tips for container image development Creating New OSG Software Containers \u00b6 OSG Software service container images intended for OSG site admin use need to be automatically updated once per week to pick up any OS updates, as well as upon any changes to the images themselves. To do this, we use GitHub Actions to: Build new images on commits to master or main Update the docker-software-base on a schedule, which triggers builds for all image repos through repository dispatch Push container images to Docker Hub and the OSG Container Registry (Harbor) Code for new container images should now be stored in the GitHub repository opensciencegrid/images ; see below for instructions . The previous convention was to use individual GitHub repos for each image; these are described below . Add image code to the opensciencegrid/images repo \u00b6 The opensciencegrid/images repository is a central repository containing multiple container images based on the OSG Software Stack. The repository uses GitHub Actions CI to automatically build and push images, both on changes to individual images and upon updates to the upstream opensciencegrid/docker-software-base image. This repository is intended for images owned by OSG Staff with relatively simple CI needs: Images using OSG Yum repositories, especially those based on opensciencegrid/software-base Images that need tags based on the development , testing , and release tags for the supported OSG release series Images that push to Docker Hub and OSG Harbor Images that do not need CI tests See SOFTWARE-5013 for additional considerations. Creating new images from scratch \u00b6 Images are automatically built from the subdirectories of the opensciencegrid directory . A set of images will be built for each subdirectory, with each set containing multiple images based on OSG release series (e.g. 3.5 , 3.6 ) and release level (e.g. testing , release ). Fork and clone the https://github.com/opensciencegrid/images repo Create a subdirectory under opensciencegrid/ Create a README.md file describing the software provided by the image Create a LICENSE file containing the Apache 2.0 license text Create a Dockerfile building from the OSG Software Base image: ARG BASE_OSG_SERIES=3.6 ARG BASE_YUM_REPO=release FROM opensciencegrid/software-base:$BASE_OSG_SERIES-<DISTRO VERSION>-$BASE_YUM_REPO # Previous instance has gone out of scope ARG BASE_OSG_SERIES=3.6 ARG BASE_YUM_REPO=release LABEL maintainer OSG Software <help@osg-htc.org> RUN yum install -y <PACKAGE(S)> && \\ yum clean all && \\ rm -rf /var/cache/yum/* Replacing <DISTRO VERSION> with the Enterprise Linux version abbreviation (e.g., el7 , el8 ), and <PACKAGE(S)> with the RPM(s) you'd like to provide in this image. Hardcoding OSG series or release level If you do not want to build your image for all release series (for example, it's 3.6-only), or you do not want to build your image for all release levels (for example, always build from release), hardcode those instead of using the arguments, as in: ARG BASE_OSG_SERIES=3.6 FROM opensciencegrid/software-base:$BASE_OSG_SERIES-el8-release Adding an existing image from another repository \u00b6 If there is an existing source repository for an image that you would like to pull into the opensciencegrid/images central repository (e.g., images that make use of OSG Yum repositories), use the following instructions to retain history from the old repository. Install the git filter-repo plugin. For example, on an RPM-based operating system: yum install git-filter-repo Checkout opensciencegrid/images and your other source repository or make sure your local main branches are up-to-date cd to your other source repository and run the following: git filter-repo --to-subdirectory-filter opensciencegrid/<IMAGE NAME> cd to your local clone of the images repository and add your local repo as a remote using filesystem paths. For example: git remote add <IMAGE NAME> <PATH TO OTHER SOURCE REPO> In the images repo, create a branch based off of main for your work In the images repo, make sure your fork knows all the refs from the other source repo remote with the following: git fetch <IMAGE NAME> --tags While on your your new branch, do the merge. For example, if the main branch of your other source repository is main : git merge --allow-unrelated-histories <IMAGE NAME>/main Update the merged in Dockerfile to accept the BASE_OSG_SERIES and BASE_YUM_REPO arguments. Prepare the Docker Hub repository \u00b6 Ask the Software Manager to create a Docker Hub repo in the OSG organization. The name should generally match the subdirectory name under the images repo. Go to the permissions tab and give the robots and technology teams Read & Write access Old Image Repositories \u00b6 Some container images are stored in individual repositories under the opensciencegrid organization, with names prefixed with docker- and with the container repository topic. New images should not be created this way, but existing images may need to be updated or fixed; this section describes their layout and mechanics. Old image repos will have: A README.md file describing the software provided by the image A LICENSE file containing the Apache 2.0 license text A Dockerfile based off of the OSG Software Base image: FROM opensciencegrid/software-base:<OSG RELEASE SERIES>-<EL MAJOR VERSION>-release LABEL maintainer OSG Software <help@osg-htc.org> RUN yum update -y && \\ yum clean all && \\ rm -rf /var/cache/yum/* RUN yum install -y <PACKAGE> && \\ yum clean all && \\ rm -rf /var/cache/yum/* Replacing <PACKAGE> with the name of the RPM you'd like to provide in this container image, <OSG RELEASE SERIES> with the OSG release series version (e.g., 3.6 ), and <EL MAJOR VERSION> with the Enterprise Linux major version (e.g., 7 ). The BASE_OSG_SERIES and BASE_YUM_REPO arguments may or may not be used. The pre-defined Publish OSG Software container image workflow, found under the Actions tab. The user \"osg-bot\" needs to have the \"Write\" role for this repo in order to trigger automatic builds. Access to the following organizational secrets DOCKER_USERNAME DOCKER_PASSWORD OSG_HARBOR_ROBOT_USER OSG_HARBOR_ROBOT_PASSWORD The repo may also have access to the REPO_ACCESS_TOKEN organization secret, if it needs to send dispatches to another repository (e.g. docker-software-base ). A Docker Hub repository with a name matching the GitHub repo name, without the docker- prefix, with Read & Write access for the robots and technology teams. In addition, for repository dispatch from docker-software-base, they are listed in the GitHub Actions workflow for docker-software-base , in the dispatch-repo: list (under jobs: dispatch: strategy: matrix: ). Triggering Container Image Builds \u00b6 To build a new version of an existing container image , e.g. for a new RPM version of software in the container, you can kick off a new build in one of two ways: If there are no changes necessary to the container packaging: go to the GitHub repository's latest build under Actions, e.g. https://github.com/opensciencegrid/docker-frontier-squid/actions/ , and click \"Re-run jobs\" -> \"Re-run all jobs\". If changes need to be made to the container packaging: submit a pull request with your changes to the relevant GitHub repository and request that another team member review it. Once merged into master or main , a GitHub Actions build should start automatically. If the GitHub Actions build completes successfully, you should shortly see new fresh and timestamp tags appear in the DockerHub repository. Automatic weekly rebuilds If the repo's GitHub Actions are configured as above, container images will automatically rebuild, and therefore pick up new packages available in minefield once per week. Managing Tags in DockerHub \u00b6 Adding tags \u00b6 Images that have passed acceptance testing should be tagged as stable : Install the jq utility: yum install jq Get the sha256 repo digest of the image that the user has tested. All you need is the part that starts with sha256:... (aka the <DIGEST> ). A Kubernetes user can get the digest from the \"Image ID\" line obtained by running: kubectl describe pod <POD> A Docker user can get the digest by running: docker image inspect <IMAGE NAME>:<TAG> | jq '.[0].Id' (Optional) If you are tagging multiple images, you can enter your Docker Hub username and password into environment variables, to avoid having to re-type them. Otherwise the script will prompt for them. read user # enter dockerhub username read -s pass # enter dockerhub password export user pass Run the Docker container image tagging command from release-tools : ./dockerhub-tag-fresh-to-stable.sh <IMAGE NAME> <DIGEST> Removing tags \u00b6 Run the Docker container image pruning command from release-tools : ./dockerhub-prune-tags.py <IMAGE NAME> Making Slim Containers \u00b6 Here are some resources for creating slim, efficient containers: https://developers.redhat.com/blog/2016/03/09/more-about-docker-images-size/ https://github.com/opensciencegrid/topology/pull/399 https://docs.docker.com/develop/develop-images/multistage-build/","title":"Container Development Guide"},{"location":"software/container-development-guide/#container-development-guide","text":"This document contains instructions for OSG Technology Team members, including: How to to develop OSG Software container images that are automatically pushed to Docker Hub that adhere to our container release policy How to build a new version of an existing image How to manage tags for images in the OSG DockerHub organization Tips for container image development","title":"Container Development Guide"},{"location":"software/container-development-guide/#creating-new-osg-software-containers","text":"OSG Software service container images intended for OSG site admin use need to be automatically updated once per week to pick up any OS updates, as well as upon any changes to the images themselves. To do this, we use GitHub Actions to: Build new images on commits to master or main Update the docker-software-base on a schedule, which triggers builds for all image repos through repository dispatch Push container images to Docker Hub and the OSG Container Registry (Harbor) Code for new container images should now be stored in the GitHub repository opensciencegrid/images ; see below for instructions . The previous convention was to use individual GitHub repos for each image; these are described below .","title":"Creating New OSG Software Containers"},{"location":"software/container-development-guide/#add-image-code-to-the-opensciencegridimages-repo","text":"The opensciencegrid/images repository is a central repository containing multiple container images based on the OSG Software Stack. The repository uses GitHub Actions CI to automatically build and push images, both on changes to individual images and upon updates to the upstream opensciencegrid/docker-software-base image. This repository is intended for images owned by OSG Staff with relatively simple CI needs: Images using OSG Yum repositories, especially those based on opensciencegrid/software-base Images that need tags based on the development , testing , and release tags for the supported OSG release series Images that push to Docker Hub and OSG Harbor Images that do not need CI tests See SOFTWARE-5013 for additional considerations.","title":"Add image code to the opensciencegrid/images repo"},{"location":"software/container-development-guide/#creating-new-images-from-scratch","text":"Images are automatically built from the subdirectories of the opensciencegrid directory . A set of images will be built for each subdirectory, with each set containing multiple images based on OSG release series (e.g. 3.5 , 3.6 ) and release level (e.g. testing , release ). Fork and clone the https://github.com/opensciencegrid/images repo Create a subdirectory under opensciencegrid/ Create a README.md file describing the software provided by the image Create a LICENSE file containing the Apache 2.0 license text Create a Dockerfile building from the OSG Software Base image: ARG BASE_OSG_SERIES=3.6 ARG BASE_YUM_REPO=release FROM opensciencegrid/software-base:$BASE_OSG_SERIES-<DISTRO VERSION>-$BASE_YUM_REPO # Previous instance has gone out of scope ARG BASE_OSG_SERIES=3.6 ARG BASE_YUM_REPO=release LABEL maintainer OSG Software <help@osg-htc.org> RUN yum install -y <PACKAGE(S)> && \\ yum clean all && \\ rm -rf /var/cache/yum/* Replacing <DISTRO VERSION> with the Enterprise Linux version abbreviation (e.g., el7 , el8 ), and <PACKAGE(S)> with the RPM(s) you'd like to provide in this image. Hardcoding OSG series or release level If you do not want to build your image for all release series (for example, it's 3.6-only), or you do not want to build your image for all release levels (for example, always build from release), hardcode those instead of using the arguments, as in: ARG BASE_OSG_SERIES=3.6 FROM opensciencegrid/software-base:$BASE_OSG_SERIES-el8-release","title":"Creating new images from scratch"},{"location":"software/container-development-guide/#adding-an-existing-image-from-another-repository","text":"If there is an existing source repository for an image that you would like to pull into the opensciencegrid/images central repository (e.g., images that make use of OSG Yum repositories), use the following instructions to retain history from the old repository. Install the git filter-repo plugin. For example, on an RPM-based operating system: yum install git-filter-repo Checkout opensciencegrid/images and your other source repository or make sure your local main branches are up-to-date cd to your other source repository and run the following: git filter-repo --to-subdirectory-filter opensciencegrid/<IMAGE NAME> cd to your local clone of the images repository and add your local repo as a remote using filesystem paths. For example: git remote add <IMAGE NAME> <PATH TO OTHER SOURCE REPO> In the images repo, create a branch based off of main for your work In the images repo, make sure your fork knows all the refs from the other source repo remote with the following: git fetch <IMAGE NAME> --tags While on your your new branch, do the merge. For example, if the main branch of your other source repository is main : git merge --allow-unrelated-histories <IMAGE NAME>/main Update the merged in Dockerfile to accept the BASE_OSG_SERIES and BASE_YUM_REPO arguments.","title":"Adding an existing image from another repository"},{"location":"software/container-development-guide/#prepare-the-docker-hub-repository","text":"Ask the Software Manager to create a Docker Hub repo in the OSG organization. The name should generally match the subdirectory name under the images repo. Go to the permissions tab and give the robots and technology teams Read & Write access","title":"Prepare the Docker Hub repository"},{"location":"software/container-development-guide/#old-image-repositories","text":"Some container images are stored in individual repositories under the opensciencegrid organization, with names prefixed with docker- and with the container repository topic. New images should not be created this way, but existing images may need to be updated or fixed; this section describes their layout and mechanics. Old image repos will have: A README.md file describing the software provided by the image A LICENSE file containing the Apache 2.0 license text A Dockerfile based off of the OSG Software Base image: FROM opensciencegrid/software-base:<OSG RELEASE SERIES>-<EL MAJOR VERSION>-release LABEL maintainer OSG Software <help@osg-htc.org> RUN yum update -y && \\ yum clean all && \\ rm -rf /var/cache/yum/* RUN yum install -y <PACKAGE> && \\ yum clean all && \\ rm -rf /var/cache/yum/* Replacing <PACKAGE> with the name of the RPM you'd like to provide in this container image, <OSG RELEASE SERIES> with the OSG release series version (e.g., 3.6 ), and <EL MAJOR VERSION> with the Enterprise Linux major version (e.g., 7 ). The BASE_OSG_SERIES and BASE_YUM_REPO arguments may or may not be used. The pre-defined Publish OSG Software container image workflow, found under the Actions tab. The user \"osg-bot\" needs to have the \"Write\" role for this repo in order to trigger automatic builds. Access to the following organizational secrets DOCKER_USERNAME DOCKER_PASSWORD OSG_HARBOR_ROBOT_USER OSG_HARBOR_ROBOT_PASSWORD The repo may also have access to the REPO_ACCESS_TOKEN organization secret, if it needs to send dispatches to another repository (e.g. docker-software-base ). A Docker Hub repository with a name matching the GitHub repo name, without the docker- prefix, with Read & Write access for the robots and technology teams. In addition, for repository dispatch from docker-software-base, they are listed in the GitHub Actions workflow for docker-software-base , in the dispatch-repo: list (under jobs: dispatch: strategy: matrix: ).","title":"Old Image Repositories"},{"location":"software/container-development-guide/#triggering-container-image-builds","text":"To build a new version of an existing container image , e.g. for a new RPM version of software in the container, you can kick off a new build in one of two ways: If there are no changes necessary to the container packaging: go to the GitHub repository's latest build under Actions, e.g. https://github.com/opensciencegrid/docker-frontier-squid/actions/ , and click \"Re-run jobs\" -> \"Re-run all jobs\". If changes need to be made to the container packaging: submit a pull request with your changes to the relevant GitHub repository and request that another team member review it. Once merged into master or main , a GitHub Actions build should start automatically. If the GitHub Actions build completes successfully, you should shortly see new fresh and timestamp tags appear in the DockerHub repository. Automatic weekly rebuilds If the repo's GitHub Actions are configured as above, container images will automatically rebuild, and therefore pick up new packages available in minefield once per week.","title":"Triggering Container Image Builds"},{"location":"software/container-development-guide/#managing-tags-in-dockerhub","text":"","title":"Managing Tags in DockerHub"},{"location":"software/container-development-guide/#adding-tags","text":"Images that have passed acceptance testing should be tagged as stable : Install the jq utility: yum install jq Get the sha256 repo digest of the image that the user has tested. All you need is the part that starts with sha256:... (aka the <DIGEST> ). A Kubernetes user can get the digest from the \"Image ID\" line obtained by running: kubectl describe pod <POD> A Docker user can get the digest by running: docker image inspect <IMAGE NAME>:<TAG> | jq '.[0].Id' (Optional) If you are tagging multiple images, you can enter your Docker Hub username and password into environment variables, to avoid having to re-type them. Otherwise the script will prompt for them. read user # enter dockerhub username read -s pass # enter dockerhub password export user pass Run the Docker container image tagging command from release-tools : ./dockerhub-tag-fresh-to-stable.sh <IMAGE NAME> <DIGEST>","title":"Adding tags"},{"location":"software/container-development-guide/#removing-tags","text":"Run the Docker container image pruning command from release-tools : ./dockerhub-prune-tags.py <IMAGE NAME>","title":"Removing tags"},{"location":"software/container-development-guide/#making-slim-containers","text":"Here are some resources for creating slim, efficient containers: https://developers.redhat.com/blog/2016/03/09/more-about-docker-images-size/ https://github.com/opensciencegrid/topology/pull/399 https://docs.docker.com/develop/develop-images/multistage-build/","title":"Making Slim Containers"},{"location":"software/create-vo-client/","text":"Creating the VO Client Package \u00b6 Overview \u00b6 This document will explain the step-by-step procedures for creating and releasing the VO Client Package. The VO Client Package sources can be found here: https://github.com/opensciencegrid/osg-vo-config When upstream changes have been made and are ready for a new VO Client Package release, these sources will be used to prepare a release tarball, which will in turn be used for the RPMs. In order to build the RPM, one needs: The tarball containing the: edg-mkgridmap.conf file gums.config.template file grid-vorolemap file (generated) voms-mapfile-default file (generated) vomses file vomsdir directory tree, containing the .lsc files. The RPM spec file, maintained in the OSG packaging area. JIRA Ticket for the Release \u00b6 There should be an associated JIRA ticket with a summary line of the form \"Release VO Package 85\". (Throughout this document, this release number will be referred to as <NN> .) The JIRA ticket should contain the details of the changes expected in the new VO Client Package release, which you should verify before proceeding. You can verify this with your favorite git tool (eg, git diff or gitk ), or just view the changes directly on GitHub: https://github.com/opensciencegrid/osg-vo-config/compare/release-84...master Here, release-84 is the previous release tag, which you are comparing to the latest changes in master . To use GitHub to view the comparison, you need to specify whatever is the most recent previous release tag. Alternatively, you can proceed to make the tarball , and compare the result to the previous vo-client tarball (from the upstream source cache) before publishing the new release . However you choose to do it, the point is to verify that the changes going into the release match what is expected in the JIRA ticket before publishing a new release. Updates to the GUMS Template \u00b6 Most commonly, VO Client Package releases do not involve changes to the gums.config.template file, though on occasion it needs to be updated. Before proceeding, any changes to gums.config.template related to this release should be committed to git and pushed to the upstream repo on GitHub. The procedure for updating gums.config.template is outside the scope of this document, but the main important point is that any updates to this file should be done with the GUMS web interface rather than editing its xml contents by hand. Making the Tarball \u00b6 The process to make a new tarball has been mostly scripted. To make the tarball: Start with a clean checkout of the latest master branch of the osg-vo-config source repo . This checked out commit should be the one intended to be tagged for the new release. - Run the mk-vo-client-tarball script with the new release number <NN> : $ ./bin/mk-vo-client-tarball <NN> For example: $ ./bin/mk-vo-client-tarball 85 This will create a file vo-client-<NN>-osg.tar.gz in the current directory. Once the tarball is created: If you have not already verified the changes expected in the JIRA ticket, compare the contents of the new tarball with the previous version in the upstream source cache . Upload the tarball into the upstream source cache , under the vo-client/<NN>/ directory. RPM Spec File Maintenance \u00b6 The OSG RPM spec file is maintained in Subversion . The VO Client package is located in native/redhat/trunk/vo-client ; that is, here . There are two files that need to be maintained: osg/vo-client.spec The Version: field should be updated to match the <NN> number for the release A %changelog entry should be added for the new release, mentioning any changes and their associated tickets upstream/release_tarball.source Update the relative path for the new tarball within the upstream source cache . Typically this will be vo-client/<NN>/vo-client-<NN>-osg.tar.gz . RPM Building \u00b6 After installing the osg-build tools , check out a clean copy of the vo-client packaging directory from svn, then: osg-build prebuild . Once there are no errors, run osg-build koji . --scratch . (This can be done without making any permanent change.) Once that builds successfully, run osg-build koji . (This is permanent, unlike when you ran with --scratch .) You cannot rebuild this version of the RPM again; to rebuild with changes, you must bump the release number and edit the changelog. This will push the RPMs into the OSG development repository. Note Koji requires additional setup compared to rpmbuild; see the documentation here . Publishing the New Release \u00b6 The final version of the sources in the osg-vo-config , which was used to create the tarball that was used in the koji build, needs be tagged in git with a release-<NN> tag (eg, release-85 ) and published as a release on GitHub. You can create and push the release-<NN> from your git checkout of osg-vo-config , OR create the tag while publishing the release on GitHub (recommended). To publish the new release on GitHub: Go to https://github.com/opensciencegrid/osg-vo-config/releases/new In the \"Tag version\" field, enter release-<NN> (eg, release-85 ) If you are creating this tag on GitHub, click the \"Target\" dropdown button, and under the \"Recent Commits\" tab, make sure to select the commit you used when creating the tarball (It should be the first one) In the \"Release title\" field, enter <MONTH> <YEAR> VO Package Release <NN> (eg, December 2018 VO Package Release 85 ) In the release description, list the changes in this release and their associated ticket numbers, similar to the new %changelog entry added in the rpm spec file (You can view the releases page for examples) - Click the \"Publish release\" button Promotion to Testing and Release: \u00b6 Read Release Policy . Note that the vo-client package frequently is part of a separate -data release; it does not necessarily have to wait for the main release cycle.","title":"Creating the VO Client Package"},{"location":"software/create-vo-client/#creating-the-vo-client-package","text":"","title":"Creating the VO Client Package"},{"location":"software/create-vo-client/#overview","text":"This document will explain the step-by-step procedures for creating and releasing the VO Client Package. The VO Client Package sources can be found here: https://github.com/opensciencegrid/osg-vo-config When upstream changes have been made and are ready for a new VO Client Package release, these sources will be used to prepare a release tarball, which will in turn be used for the RPMs. In order to build the RPM, one needs: The tarball containing the: edg-mkgridmap.conf file gums.config.template file grid-vorolemap file (generated) voms-mapfile-default file (generated) vomses file vomsdir directory tree, containing the .lsc files. The RPM spec file, maintained in the OSG packaging area.","title":"Overview"},{"location":"software/create-vo-client/#jira-ticket-for-the-release","text":"There should be an associated JIRA ticket with a summary line of the form \"Release VO Package 85\". (Throughout this document, this release number will be referred to as <NN> .) The JIRA ticket should contain the details of the changes expected in the new VO Client Package release, which you should verify before proceeding. You can verify this with your favorite git tool (eg, git diff or gitk ), or just view the changes directly on GitHub: https://github.com/opensciencegrid/osg-vo-config/compare/release-84...master Here, release-84 is the previous release tag, which you are comparing to the latest changes in master . To use GitHub to view the comparison, you need to specify whatever is the most recent previous release tag. Alternatively, you can proceed to make the tarball , and compare the result to the previous vo-client tarball (from the upstream source cache) before publishing the new release . However you choose to do it, the point is to verify that the changes going into the release match what is expected in the JIRA ticket before publishing a new release.","title":"JIRA Ticket for the Release"},{"location":"software/create-vo-client/#updates-to-the-gums-template","text":"Most commonly, VO Client Package releases do not involve changes to the gums.config.template file, though on occasion it needs to be updated. Before proceeding, any changes to gums.config.template related to this release should be committed to git and pushed to the upstream repo on GitHub. The procedure for updating gums.config.template is outside the scope of this document, but the main important point is that any updates to this file should be done with the GUMS web interface rather than editing its xml contents by hand.","title":"Updates to the GUMS Template"},{"location":"software/create-vo-client/#making-the-tarball","text":"The process to make a new tarball has been mostly scripted. To make the tarball: Start with a clean checkout of the latest master branch of the osg-vo-config source repo . This checked out commit should be the one intended to be tagged for the new release. - Run the mk-vo-client-tarball script with the new release number <NN> : $ ./bin/mk-vo-client-tarball <NN> For example: $ ./bin/mk-vo-client-tarball 85 This will create a file vo-client-<NN>-osg.tar.gz in the current directory. Once the tarball is created: If you have not already verified the changes expected in the JIRA ticket, compare the contents of the new tarball with the previous version in the upstream source cache . Upload the tarball into the upstream source cache , under the vo-client/<NN>/ directory.","title":"Making the Tarball"},{"location":"software/create-vo-client/#rpm-spec-file-maintenance","text":"The OSG RPM spec file is maintained in Subversion . The VO Client package is located in native/redhat/trunk/vo-client ; that is, here . There are two files that need to be maintained: osg/vo-client.spec The Version: field should be updated to match the <NN> number for the release A %changelog entry should be added for the new release, mentioning any changes and their associated tickets upstream/release_tarball.source Update the relative path for the new tarball within the upstream source cache . Typically this will be vo-client/<NN>/vo-client-<NN>-osg.tar.gz .","title":"RPM Spec File Maintenance"},{"location":"software/create-vo-client/#rpm-building","text":"After installing the osg-build tools , check out a clean copy of the vo-client packaging directory from svn, then: osg-build prebuild . Once there are no errors, run osg-build koji . --scratch . (This can be done without making any permanent change.) Once that builds successfully, run osg-build koji . (This is permanent, unlike when you ran with --scratch .) You cannot rebuild this version of the RPM again; to rebuild with changes, you must bump the release number and edit the changelog. This will push the RPMs into the OSG development repository. Note Koji requires additional setup compared to rpmbuild; see the documentation here .","title":"RPM Building"},{"location":"software/create-vo-client/#publishing-the-new-release","text":"The final version of the sources in the osg-vo-config , which was used to create the tarball that was used in the koji build, needs be tagged in git with a release-<NN> tag (eg, release-85 ) and published as a release on GitHub. You can create and push the release-<NN> from your git checkout of osg-vo-config , OR create the tag while publishing the release on GitHub (recommended). To publish the new release on GitHub: Go to https://github.com/opensciencegrid/osg-vo-config/releases/new In the \"Tag version\" field, enter release-<NN> (eg, release-85 ) If you are creating this tag on GitHub, click the \"Target\" dropdown button, and under the \"Recent Commits\" tab, make sure to select the commit you used when creating the tarball (It should be the first one) In the \"Release title\" field, enter <MONTH> <YEAR> VO Package Release <NN> (eg, December 2018 VO Package Release 85 ) In the release description, list the changes in this release and their associated ticket numbers, similar to the new %changelog entry added in the rpm spec file (You can view the releases page for examples) - Click the \"Publish release\" button","title":"Publishing the New Release"},{"location":"software/create-vo-client/#promotion-to-testing-and-release","text":"Read Release Policy . Note that the vo-client package frequently is part of a separate -data release; it does not necessarily have to wait for the main release cycle.","title":"Promotion to Testing and Release:"},{"location":"software/development-process/","text":"Software Development Process \u00b6 This page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.) Overall Development Cycle \u00b6 For a typical update to an existing package, the overall development cycle is roughly as follows: Download the new upstream source (tarball, source RPM, checkout) into the CHTC upstream area In a checkout of our packaging code , update the reference to the upstream file and, as needed, the RPM spec file Use osg-build to perform a scratch build of the updated package Verify that the build succeeded; if not, redo previous steps until success Optionally, lightly test the new RPM(s); if there are problems, redo previous steps until success Use osg-build to perform an official build of the updated package (which will go into the development repos) Perform standard developer testing of the new RPMs; this generally means running the VM Universe tests - see below for details Have another software team member review your testing and give you permission to promote the package Promote the package to testing \u2014 see below for details Build Procedures \u00b6 Initial repo setup \u00b6 Create your own fork of the https://github.com/osg-htc/software-packaging repository. Clone your fork, then add the upstream repository as a remote (replacing <YOURNAME> with your GitHub username): $ git clone https://github.com/<YOURNAME>/software-packaging $ git remote add upstream https://github.com/osg-htc/software-packaging Basic building \u00b6 The software-packaging repository is laid out into <SUBTREE>/<PACKAGE> directories, where <SUBTREE> corresponds to a set of repos, such as 24-main or 25-upcoming , and <PACKAGE> is the name of a package to be built. After making changes to a package, do a scratch build in Koji by running the following from the package directory: $ osg-build koji --scratch Koji will make builds for the tags appropriate to the subtree the directory is under. For example, if you build 24-main/xrootd , it will be built for the osg-24-main-* tags. osg-build will print links to where you can watch the build tasks and download the results. Non-scratch builds require the changes to be pushed to the main branch in the upstream osg-htc/software-packaging repository. osg-build will warn you if your local checkout is not up-to-date with upstream, or if you are not on the main branch. You should have write permissions to the repository, and be able to push your changes directly, e.g. $ git push upstream main Once the changes are upstream in main , you can do a non-scratch build: $ osg-build koji Building packages for multiple OSG release series \u00b6 The OSG Software team supports two release series in parallel; many of the packages are identical or very similar between release series, and you will be making the same change to a package across both release series. We recommend using a diffing tool such as Meld (Linux) or WinMerge (Windows) that is capable of comparing directories and not just individual files, to make sure all the changes are carried over. This is one example workflow, using the \"24-main\" and \"25-main\" release series, and the package \"xrdcl-pelican\" which is identical between the two release series. Make changes to 24-main/xrdcl-pelican Make a scratch build (e.g. osg-build koji --scratch 24-main/xrdcl-pelican ) Open 24-main/xrdcl-pelican and 25-main/xrdcl-pelican in a diffing tool, and copy over all the changes git add and git commit the changes; committing the changes to both series in the same commit means you don't have to write the commit message twice, and git blame will be accurate for both release series git push to the upstream repo Make non-scratch builds; use the following procedure to submit both at the same time: $ osg-build koji --nowait 25 -main/xrdcl-pelican $ osg-build koji --nowait 24 -main/xrdcl-pelican $ osg-koji watch-task --mine If your development process is more iterative and you need multiple commits, you could do something like the following (using xrootd as an example): Make changes to 24-main/xrootd Make a scratch build as above Commit your changes as a \"checkpoint\" Repeat 1-3 as necessary until you think you are ready for a non-scratch build Copy your changes to 25-main using a diffing tool as above You now have two options: If you haven't pushed, then you can do an interactive rebase to squash your changes to 24-main and 25-main down to one commit If you have already pushed, get a log of the recent changes to 24-main/xrootd : $ git --no-pager log --oneline --since = 'last week' --reverse -- 24 -main/xrootd Copy and paste that to a text editor. Then, when you write the commit message for the commit to 24-main/xrootd , reference the commits from that command. For example: 25-main/xrootd: Update xrootd to 5.8.4 and add various patches Includes the following commits from 24-main/xrootd: 8061d7d40 24-main/xrootd: update to 5.8.4; update patches, bring spec file closer to upstream 13b858dce 24-main/xrootd: add some more patches 773f57f21 24-main/xrootd: Add TPC worker pool patch Keeping the Git hashes will make future archaeology easier. Testing Procedures \u00b6 Before promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing. If you are not doing your own development testing for a package , contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released. Testing should be performed for all distro versions the package is built for, and all supported release series. In addition to a fresh installation, it is also important to test upgrades from a previous version of the software. VM Universe (VMU) tests \u00b6 These are OSG Software's automated tests, which run in VMs powered by HTCondor's VM universe. Each run tests installation and/or upgrades of the listed software, using the OSG-Test test suite of integration tests. Even if there are no functionality tests for your package in OSG-Test, you should run the VMU tests to validate the RPM installation and upgrade process. Make sure you meet the pre-requisites required to submit VM Universe jobs on osgsw-ap.chtc.wisc.edu . If you do not have permission to submit VMU runs, mention \"@Software and Release\" on the Jira ticket and a Software Team member will run the tests for you. After that's done, prepare the test suite with a comment describing the test run. For example, if you were testing a new htcondor-ce package: osg-run-tests 'Testing htcondor-ce-3.2.1-1 (SOFTWARE-####)' After you cd into the directory specified in the output of the previous command, you will need to edit the *.yaml files in parameters.d to reflect the tests that you will want to run, i.e. clean installs, upgrade installs and upgrade installs between OSG versions. Once you're satisfied with your list of parameters, submit the dag: ./master-run.sh Promoting a Package to Testing \u00b6 Once development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle. Preparing a Good Promotion Request \u00b6 Developers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected Jira ticket and will be answered there as well. Below are some tips for writing a good promotion request: Make sure that relevant information about goals, history, and resolution is in the associated ticket(s) List the NVRs of the builds to be promoted, as well as the tags they will be promoted into. If a version was not built for a particular platform, mention why not. Link to the results page of the VMU tests. Make sure the tests actually installed the version to be promoted. Explain any failures. If you ran manual tests, summarize your tests and findings, and explain any failures. For example (hypothetical promotion request for XRootD): VMU test results are at <LINK TO TEST RESULTS PAGE>; tests passed except for StashCache tests on EL10 because the package is not available on that platform. I tested turning on the new knob in a container and was able to pull a file from the server successfully. @Software and Release, permission to promote xrootd-5.9.9-9 to 24-main-testing and 25-main-testing ? Promoting \u00b6 Follow these steps to request promotion, promote a package, and note the promotion in JIRA: Make sure the package update has at least one associated Jira ticket; if there is no ticket, create one for releasing the package(s). Obtain permission to promote the package(s) from one of the Software Team members, by mentioning \"@Software and Release\" in the Jira ticket. Use osg-promote to promote the package(s) from development to testing Comment on the associated Jira ticket(s) with osg-promote's Jira-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing. Update the Jira ticket description with a short blurb about the important changes in the new version vs. the old version, so it can be used in the release notes. Mark the Jira ticket for the package as \u201cReady For Testing\u201d; tickets related to this version (e.g. for specific changes) should also be marked \u201cReady For Testing\u201d.","title":"Development Process"},{"location":"software/development-process/#software-development-process","text":"This page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.)","title":"Software Development Process"},{"location":"software/development-process/#overall-development-cycle","text":"For a typical update to an existing package, the overall development cycle is roughly as follows: Download the new upstream source (tarball, source RPM, checkout) into the CHTC upstream area In a checkout of our packaging code , update the reference to the upstream file and, as needed, the RPM spec file Use osg-build to perform a scratch build of the updated package Verify that the build succeeded; if not, redo previous steps until success Optionally, lightly test the new RPM(s); if there are problems, redo previous steps until success Use osg-build to perform an official build of the updated package (which will go into the development repos) Perform standard developer testing of the new RPMs; this generally means running the VM Universe tests - see below for details Have another software team member review your testing and give you permission to promote the package Promote the package to testing \u2014 see below for details","title":"Overall Development Cycle"},{"location":"software/development-process/#build-procedures","text":"","title":"Build Procedures"},{"location":"software/development-process/#initial-repo-setup","text":"Create your own fork of the https://github.com/osg-htc/software-packaging repository. Clone your fork, then add the upstream repository as a remote (replacing <YOURNAME> with your GitHub username): $ git clone https://github.com/<YOURNAME>/software-packaging $ git remote add upstream https://github.com/osg-htc/software-packaging","title":"Initial repo setup"},{"location":"software/development-process/#basic-building","text":"The software-packaging repository is laid out into <SUBTREE>/<PACKAGE> directories, where <SUBTREE> corresponds to a set of repos, such as 24-main or 25-upcoming , and <PACKAGE> is the name of a package to be built. After making changes to a package, do a scratch build in Koji by running the following from the package directory: $ osg-build koji --scratch Koji will make builds for the tags appropriate to the subtree the directory is under. For example, if you build 24-main/xrootd , it will be built for the osg-24-main-* tags. osg-build will print links to where you can watch the build tasks and download the results. Non-scratch builds require the changes to be pushed to the main branch in the upstream osg-htc/software-packaging repository. osg-build will warn you if your local checkout is not up-to-date with upstream, or if you are not on the main branch. You should have write permissions to the repository, and be able to push your changes directly, e.g. $ git push upstream main Once the changes are upstream in main , you can do a non-scratch build: $ osg-build koji","title":"Basic building"},{"location":"software/development-process/#building-packages-for-multiple-osg-release-series","text":"The OSG Software team supports two release series in parallel; many of the packages are identical or very similar between release series, and you will be making the same change to a package across both release series. We recommend using a diffing tool such as Meld (Linux) or WinMerge (Windows) that is capable of comparing directories and not just individual files, to make sure all the changes are carried over. This is one example workflow, using the \"24-main\" and \"25-main\" release series, and the package \"xrdcl-pelican\" which is identical between the two release series. Make changes to 24-main/xrdcl-pelican Make a scratch build (e.g. osg-build koji --scratch 24-main/xrdcl-pelican ) Open 24-main/xrdcl-pelican and 25-main/xrdcl-pelican in a diffing tool, and copy over all the changes git add and git commit the changes; committing the changes to both series in the same commit means you don't have to write the commit message twice, and git blame will be accurate for both release series git push to the upstream repo Make non-scratch builds; use the following procedure to submit both at the same time: $ osg-build koji --nowait 25 -main/xrdcl-pelican $ osg-build koji --nowait 24 -main/xrdcl-pelican $ osg-koji watch-task --mine If your development process is more iterative and you need multiple commits, you could do something like the following (using xrootd as an example): Make changes to 24-main/xrootd Make a scratch build as above Commit your changes as a \"checkpoint\" Repeat 1-3 as necessary until you think you are ready for a non-scratch build Copy your changes to 25-main using a diffing tool as above You now have two options: If you haven't pushed, then you can do an interactive rebase to squash your changes to 24-main and 25-main down to one commit If you have already pushed, get a log of the recent changes to 24-main/xrootd : $ git --no-pager log --oneline --since = 'last week' --reverse -- 24 -main/xrootd Copy and paste that to a text editor. Then, when you write the commit message for the commit to 24-main/xrootd , reference the commits from that command. For example: 25-main/xrootd: Update xrootd to 5.8.4 and add various patches Includes the following commits from 24-main/xrootd: 8061d7d40 24-main/xrootd: update to 5.8.4; update patches, bring spec file closer to upstream 13b858dce 24-main/xrootd: add some more patches 773f57f21 24-main/xrootd: Add TPC worker pool patch Keeping the Git hashes will make future archaeology easier.","title":"Building packages for multiple OSG release series"},{"location":"software/development-process/#testing-procedures","text":"Before promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing. If you are not doing your own development testing for a package , contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released. Testing should be performed for all distro versions the package is built for, and all supported release series. In addition to a fresh installation, it is also important to test upgrades from a previous version of the software.","title":"Testing Procedures"},{"location":"software/development-process/#vm-universe-vmu-tests","text":"These are OSG Software's automated tests, which run in VMs powered by HTCondor's VM universe. Each run tests installation and/or upgrades of the listed software, using the OSG-Test test suite of integration tests. Even if there are no functionality tests for your package in OSG-Test, you should run the VMU tests to validate the RPM installation and upgrade process. Make sure you meet the pre-requisites required to submit VM Universe jobs on osgsw-ap.chtc.wisc.edu . If you do not have permission to submit VMU runs, mention \"@Software and Release\" on the Jira ticket and a Software Team member will run the tests for you. After that's done, prepare the test suite with a comment describing the test run. For example, if you were testing a new htcondor-ce package: osg-run-tests 'Testing htcondor-ce-3.2.1-1 (SOFTWARE-####)' After you cd into the directory specified in the output of the previous command, you will need to edit the *.yaml files in parameters.d to reflect the tests that you will want to run, i.e. clean installs, upgrade installs and upgrade installs between OSG versions. Once you're satisfied with your list of parameters, submit the dag: ./master-run.sh","title":"VM Universe (VMU) tests"},{"location":"software/development-process/#promoting-a-package-to-testing","text":"Once development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle.","title":"Promoting a Package to Testing"},{"location":"software/development-process/#preparing-a-good-promotion-request","text":"Developers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected Jira ticket and will be answered there as well. Below are some tips for writing a good promotion request: Make sure that relevant information about goals, history, and resolution is in the associated ticket(s) List the NVRs of the builds to be promoted, as well as the tags they will be promoted into. If a version was not built for a particular platform, mention why not. Link to the results page of the VMU tests. Make sure the tests actually installed the version to be promoted. Explain any failures. If you ran manual tests, summarize your tests and findings, and explain any failures. For example (hypothetical promotion request for XRootD): VMU test results are at <LINK TO TEST RESULTS PAGE>; tests passed except for StashCache tests on EL10 because the package is not available on that platform. I tested turning on the new knob in a container and was able to pull a file from the server successfully. @Software and Release, permission to promote xrootd-5.9.9-9 to 24-main-testing and 25-main-testing ?","title":"Preparing a Good Promotion Request"},{"location":"software/development-process/#promoting","text":"Follow these steps to request promotion, promote a package, and note the promotion in JIRA: Make sure the package update has at least one associated Jira ticket; if there is no ticket, create one for releasing the package(s). Obtain permission to promote the package(s) from one of the Software Team members, by mentioning \"@Software and Release\" in the Jira ticket. Use osg-promote to promote the package(s) from development to testing Comment on the associated Jira ticket(s) with osg-promote's Jira-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing. Update the Jira ticket description with a short blurb about the important changes in the new version vs. the old version, so it can be used in the release notes. Mark the Jira ticket for the package as \u201cReady For Testing\u201d; tickets related to this version (e.g. for specific changes) should also be marked \u201cReady For Testing\u201d.","title":"Promoting"},{"location":"software/effort-tracking/","text":"Effort Tracking \u00b6 This page describes a simple plan for tracking effort in the OSG Technology teams. Basic Ideas \u00b6 At its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking. There are just a few simple ideas to keep in mind: Each week, report your effort on OSG Technology activities Update your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example: EFFORT External development: 63% Support: 12% Leave: 20% Outside: 5 Follow standard federal regulations for calculating effort (e.g., OMB Circular A-21) The main idea is that all of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%. Report 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.) If you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see. Effort is reported as integer percentages, no less accurate than 5% intervals So please do not report percentages like 43.21% and please do not round to the nearest 10%. Effort Categories \u00b6 Here are the categories in which to track effort: Investigations Work on the Investigations team External Software work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components Internal Software work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances Documentation Work on our TWiki or Markdown documentation Release Release team activities, primarily acceptance testing and cutting releases Support User support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support. Management This is mainly for team leads; e.g., managing team activities and tickets (generally); hiring; leading (not just attending) meetings Education Not for general learning or training activities The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities. Admin General administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014 use sparingly!! Outside For all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL) Leave This is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours A few thoughts about tricky situations: Meetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time. Administrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between Admin and Outside according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities. Outside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not. Learning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into Internal . But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as Admin , and maybe add a comment explaining the activity. Ultimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.","title":"Effort Tracking"},{"location":"software/effort-tracking/#effort-tracking","text":"This page describes a simple plan for tracking effort in the OSG Technology teams.","title":"Effort Tracking"},{"location":"software/effort-tracking/#basic-ideas","text":"At its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking. There are just a few simple ideas to keep in mind: Each week, report your effort on OSG Technology activities Update your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example: EFFORT External development: 63% Support: 12% Leave: 20% Outside: 5 Follow standard federal regulations for calculating effort (e.g., OMB Circular A-21) The main idea is that all of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%. Report 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.) If you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see. Effort is reported as integer percentages, no less accurate than 5% intervals So please do not report percentages like 43.21% and please do not round to the nearest 10%.","title":"Basic Ideas"},{"location":"software/effort-tracking/#effort-categories","text":"Here are the categories in which to track effort: Investigations Work on the Investigations team External Software work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components Internal Software work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances Documentation Work on our TWiki or Markdown documentation Release Release team activities, primarily acceptance testing and cutting releases Support User support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support. Management This is mainly for team leads; e.g., managing team activities and tickets (generally); hiring; leading (not just attending) meetings Education Not for general learning or training activities The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities. Admin General administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014 use sparingly!! Outside For all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL) Leave This is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours A few thoughts about tricky situations: Meetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time. Administrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between Admin and Outside according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities. Outside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not. Learning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into Internal . But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as Admin , and maybe add a comment explaining the activity. Ultimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.","title":"Effort Categories"},{"location":"software/git-software-development/","text":"Git software development workflow \u00b6 This document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software. Git and GitHub basics \u00b6 If you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at https://docs.github.com/en/get-started Getting shell access to GitHub \u00b6 There are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience. The instructions here are derived from GitHub's own instructions on using SSH keys . Creating a new SSH key (optional but recommended) \u00b6 If you already have an SSH keypair in your ~/.ssh directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub. The instructions below will create an SSH public/private key pair with the private key stored in ~/.ssh/id_github and public key stored in ~/.ssh/id_github.pub . Generating the key \u00b6 Use ssh-keygen to generate the SSH keypair. For <EMAIL_ADDRESS> , use the email address associated with your GitHub account. [user@client ~ ] $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_github -C <EMAIL_ADDRESS> Configuring SSH to use the key for GitHub \u00b6 Make sure SSH uses the new key by default to access GitHub. Create or edit ~/.ssh/config and append the following lines: Host github.com IdentityFile <YOUR_HOME_DIR>/.ssh/id_github Where is the output of the command: echo $HOME Adding the SSH public key to GitHub \u00b6 Using the GitHub web interface: On the upper right of the screen, click on your profile picture In the menu that pops up, click \"Settings\" On the left-hand sidebar, click \"SSH and GPG keys\" In the top right of the \"SSH keys\" box, click \"New SSH key\" In the \"Title\" field of the dialog that pops up, enter a descriptive name for the key Open the public key file (e.g. ~/.ssh/id_github.pub (don't forget the .pub )) in a text editor and copy its full contents to the clipboard In the \"Key\" field, paste the public key Below the \"Key\" field, click \"Add SSH key\" You should see your new key in the \"SSH keys\" list. Testing that shell access works \u00b6 To verify you can authenticate to GitHub using SSH, SSH to git@github.com . You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.' Contribution workflow \u00b6 We use the standard GitHub pull request workflow for making contributions to OSG software. If you've never contributed to this project on GitHub before, do the following steps first: Using the GitHub web interface, fork the repo you wish to contribute to. Make a clone of your forked repo on your local machine. [user@client ~ ] $ git clone git@github.com:<USERNAME/PROJECT> Where <USERNAME> is your github username and <PROJECT> is the name of the project you want to contribute to, e.g. in order to clone my local fork of the openscience/technology repository: [user@client ~ ] $ git clone https://github.com/ddavila0/technology.git Note If you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above. If you get some other error, the GitHub page on SSH may contain useful information on troubleshooting. Once you have your local repo, do the following: Create a branch to hold changes that are related to the issue you are working on. Give the <BRANCH> a name that will remind you of its purpose, including any relevant ticket numbers, such as SOFTWARE-2345.pathchange : [user@client ~ ] $ git checkout -b <BRANCH> Make your commits to this branch, then push the branch to your repo on GitHub. [user@client ~ ] $ git push origin <BRANCH> Select your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a Jira ticket number in the message to automatically link the pull request to the Jira ticket. Request a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member. If the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from. If your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request. Release workflow \u00b6 This section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by opensciencegrid . (If you can approve pull requests, you have write access). A release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by opensciencegrid (the OSG repo): If you haven't already, add the OSG repo as a \"remote\" to your repo: [user@client ~ ] $ git remote add upstream git@github.com:opensciencegrid/<PROJECT> Where <PROJECT> is the name of the project you are going to release, e.g. for openscience/technology repository it would be technology.git Fetch changes from the OSG repo: [user@client ~ ] $ git fetch upstream Compare your branch you are releasing from (probably master ) to its copy in the OSG repo: [user@client ~ ] $ git checkout master ; git diff upstream/master There should be no differences. Once this is done, release the software as you usually do. This process varies from one project to another, but often it involves running make upstream or similar. Check your project's README file for instructions. Test your software. Tag the commit that you made the release from. Git release tags are conventionally called VERSION , where VERSION is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the <TAG> v1.3.0 . Note Once a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from. Create the tag in your local repo: [user@client ~ ] $ git tag <TAG> Push the tag to your own GitHub repo: [user@client ~ ] $ git push origin <TAG> Push the tag to the OSG repo: [user@client ~ ] $ git push upstream <TAG> Best practices \u00b6 Making good pull requests (The Art of Good Commits) \u00b6 In addition to writing good code, it's important to organize your changes to make the task of reviewing them easier, both for the reviewer of the pull request, and even for yourself later. Here are some general guidelines and tips. Put logically separate changes into separate commits \u00b6 This becomes more relevant if there are a lot of changes in the pull request. Having a single commit with many different changes happening at the same time can make the changes harder to review. If possible, split up logically separate changes into separate commits. As a simple example, if you are renaming a variable in many places, and also refactoring the structure of some code, these changes can be split into two separate commits. This will make it easier when reviewing to see clearly what each commit is trying to accomplish. The process you went through to arrive at your final code may have been different, but you can clean up your commits after the fact. One method is to use git rebase -i to combine (squash) several commits into one, and then use git gui to amend the combined commit, staging the parts that represent each logical change into separate commits. Another example that occasionally comes up is when you want to copy or move a file AND make changes to that file. If you have a single commit that introduces a file to a new location with changes , it will not be obvious from the commit diff itself which parts are the same (moved or copied in) and which parts you are modifying. Instead, by putting the copy or move of the original file into its own commit, and then putting your changes in a separate commit, it will make it clear to the reviewer which parts are changing from the original. Avoid whitespace noise \u00b6 There are a few considerations to note when it comes to whitespace. Avoid adding spaces at the end of lines. These are generally considered \"noise\" that will get cleaned up later (sometimes automatically, depending on editor settings). It's not necessary to \"fix\" this kind of whitespace noise everywhere you happen to find it in existing files, but it's fine to remove trailing whitespace for lines that you are already modifying for your own changes. Do not strip the final newline at the end-of-file. Some text editors will automatically strip the final newline at the end of file, but this is a form of whitespace noise similar to trailing spaces. If that is the case for your editor, please configure it not to strip the newline at EOF. (GitHub will show the diff for files with a missing newline at EOF with a red circle-minus symbol with the mouseover text \"No newline at end of file\".) Avoid mixing tabs and spaces. With the exception of Makefiles and Go source code, indentation should be done with regular spaces, not tabs. Please configure your text editor accordingly. Mixing tabs and spaces in indentation is problematic because different editor settings can make tab stops appear at different widths. As with trailing whitespace, it's fine to convert stray tabs to spaces on lines you are modifying, but it is not necessary to fix them everywhere, if that is not the purpose of your pull request. Put large whitespace changes into a separate commit. If you do want to change a significant amount of whitespace (either converting tabs to spaces on many lines, or perhaps adjusting the amount of indentation, or wrapping text at a different width), make your whitespace-only changes as a separate commit. This will make it clear that, although many lines may be changing, there is no functional change for that particular commit. Then any functional changes to the text in a following commit will be easier to review. Don't commit large files \u00b6 Try to keep the repository small by not committing files that are too large. Avoid committing large binaries or data sets. Ask yourself if they need to be in the repository before committing. If, somehow, a large file has been committed and pushed, it's not enough to remove the file in another commit. The file must be removed from all of the history or else the repository will still contain the large file as Git keeps track of deleted files. This can be done using the git rebase -i and git filter-repo commands. If a large file must be added to the repository. Use Git LFS Verify that only the files intended are modified in each commit \u00b6 Sometimes you may have several files modified at once, but you only intend to commit a subset of the changes. In such cases you should be aware that git commit -a will include all the modified files in your commit. Likewise, if you have some untracked files in your working copy, that you do not intend to commit, be aware that git add . will introduce these as new files in the commit. Note If you do use the -a option for git commit , you may want to consider using the -v option along with it (i.e. git commit -av ), which will show you the diff to be committed in your editor while you are typing your commit message. After making a commit locally, you can verify that only the files you intended to modify were included in the commit by running git show --stat . Or to review the actual changes to those files, git show (without --stat ). If you find unintended files included in the commit, you can amend the commit so that it does not include changes to file-not-to-commit like so: [user@client ~ ] $ git reset HEAD^ file-not-to-commit [user@client ~ ] $ git commit --amend If you have multiple commits ready for a pull request, you can review the high-level changes for each commit with git log --stat origin/master.. (or origin/main.. , whichever is the name of the main origin branch). Tools like git gui also provide a way to review commits. If you find unintended files included in earlier commits, you can do a git rebase -i origin/master and edit the commits in question, which will give you a chance to amend a particular commit as shown above. Squash noisy work-in-progress commits \u00b6 Naturally in the trial-and-error-prone process of development, there will be many changes along the way that didn't make the final cut. This is good and healthy, and it's perfectly reasonable to be making many small work-in-progress commits locally while you are developing. However, for the reviewer, the relevant thing is what final changes are being introduced into the codebase. Having to review several ideas that were put into and then taken out of the changeset is a distraction, and makes it harder to see what the end result is for the new changes. If you have such work-in-progress commits, first combine them (this is also called \"squashing\" or \"rebasing\"), and then break them up into logically distinct commits as necessary, representing the final changeset. As mentioned above, one way to do this is with a combination of git rebase -i and git gui , though there are other third-party tools (e.g., magit) available also. Write a succinct subject to explain what each commit does \u00b6 The first line of a commit message is the \"subject\" (or sometimes also called the \"title\"). It should be short and sweet (at most 72 characters) and briefly state what the commit is designed to do. As a convention, the subject of the commit message should be written in the imperative - that is, it should be written as if it were a command. For instance, a subject should start with \"fix a bug\" rather than \"fixing\" or \"fixes\". Explain why a change was made in the commit message \u00b6 It is generally important also to explain why a change was made. If this is not covered by the succinct subject line of your commit, you should explain the rationale behind your change in the commit message body. (The commit message starts with the subject line, then is optionally followed by a blank line plus the message body.) You can also explain in the commit message body how this commit accomplishes the stated purpose in the subject - and you may find yourself needing to do this if there are some tricky details in the changes. But even if it is perfectly clear from the code and your commit message what you are changing and how you are going about it, it is not always clear why the change is needed or desired - so it is important to explain your reasons, in order to make this clear to the reviewer. For an example of \"explaining your reasons\", see this commit message body . Summarize your commits in the pull request title \u00b6 The title of a pull request is analogous to the subject of a commit. If you have only one commit in your pull request, GitHub will by default set the pull request title and body to match that commit's subject and body; and that default is acceptable for single-commit pull requests. But if you have multiple commits in your pull request, you should try to capture the overall goal of these commits in your pull request title. In the pull request body, you can also mention or discuss the high-level changes from each commit, and if relevant discuss how these changes work together for the overall goal of the pull request. Choose a separate, descriptive branch name for each pull request \u00b6 GitHub allows creating pull requests entirely on their web interface, and will automatically suggest a generic branch name like patch-42 . But this is boring and not especially helpful to the reviewer or to the one submitting the pull request. Instead, choose a short name for the branch that describes the topic of the changes or the feature being introduced. For instance, fix-memory-leak or scitokens-support . (As will be discussed more later, it is best to prefix the branch name with a ticket reference as well.) Note that each pull request should get its own branch name, even if two pull requests are for the same ticket and the topic is similar. New commits pushed to a branch for a pull request will automatically show up as part of that pull request; so a second pull request needs a separate branch to track the separate set of changes. Reference any relevant tickets \u00b6 Code changes often are related to a Jira ticket, for instance SOFTWARE-1234. By referencing the name of a ticket in your pull request, it provides a convenient way to look into the background context for the change; and later on down the road, it makes it easy to find which changes were made for a particular task, referenced by the ticket name. Ideally, you can include a ticket reference each of these three places: Your branch name. For example, name the branch in your fork of the GitHub repo for the pull request SOFTWARE-1234.fix-memory-leak . Your commit messages. For example, the subject of your commit message might read, fix a memory leak (SOFTWARE-1234) . If you have trouble squeezing the ticket name into the subject line, or if you have a number of related tickets that you want to reference, it is also OK to mention them later in the commit message body. The pull request title. If your pull request is just a single commit, and you have the ticket reference in the subject line of the commit message, GitHub will include this in the pull request title automatically. But if you have multiple commits, or you have only included the ticket reference in the body of the commit message, or more generally if you want to tweak the title of the pull request, you should in any case make a point to include the ticket name in the title of the pull request. (By convention, we include this at the end of the title, in parentheses.) If there is no ticket associated with your changes, consider creating one (or asking an OSG software team member to create one) before submitting your pull request. Further reading \u00b6 There are a number of articles and guides for making good git commits and good pull requests - a simple search will turn up plenty of material for the interested reader. See online guides such as this one for more details. Brownie points \u00b6 You will get brownie points from Carl, personally, if you strive to make your code (and other text files) fit within an 80-column terminal window. Reviewing pull requests \u00b6 There are a couple items to note about the review process for GitHub pull requests. Batch comments in a formal review \u00b6 When reviewing a pull request, GitHub allows you to comment on lines and presents the option to \"Add single comment\" or \"Start a review\". A single comment added will not be tied to your review, and a separate email notification will be sent for every time you click \"Add single comment\". Especially for reviewing larger pull requests, we generally prefer to \"Start a review\", and then \"Add review comment\" for subsequent comments. This will tie all of your comments and suggestions together as part of your review. When you complete your review, you will have the opportunity to make summary comments about the changes, when you select Approve/Comment/Request changes. By \"batching\" all of your review comments this way, a single email notification will be sent for your review, which contains all of your review comments together. Batch commits when accepting suggestions from a review \u00b6 When someone reviews your pull request, they may make suggestions that tweak your changes. Similar to review comments, suggestions from a review can either be applied one at a time (Commit suggestion), or they can be batched and applied together. To batch suggestions, first you need to open the \"Files changed\" tab; then for each suggestion you want to accept, click \"Add suggestion to batch\". Finally, click \"Commit suggestions\" to apply all batched suggestions as a single commit. Generally we prefer to batch related changes or miscellaneous tweaks rather than applying each one individually. But if there are a number of suggestions of a different nature, it is OK to group them such that you apply one batch for each set of related suggestions (consistent with the guideline to put logically separate changes into separate commits).","title":"Git Software Development Process"},{"location":"software/git-software-development/#git-software-development-workflow","text":"This document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software.","title":"Git software development workflow"},{"location":"software/git-software-development/#git-and-github-basics","text":"If you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at https://docs.github.com/en/get-started","title":"Git and GitHub basics"},{"location":"software/git-software-development/#getting-shell-access-to-github","text":"There are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience. The instructions here are derived from GitHub's own instructions on using SSH keys .","title":"Getting shell access to GitHub"},{"location":"software/git-software-development/#creating-a-new-ssh-key-optional-but-recommended","text":"If you already have an SSH keypair in your ~/.ssh directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub. The instructions below will create an SSH public/private key pair with the private key stored in ~/.ssh/id_github and public key stored in ~/.ssh/id_github.pub .","title":"Creating a new SSH key (optional but recommended)"},{"location":"software/git-software-development/#generating-the-key","text":"Use ssh-keygen to generate the SSH keypair. For <EMAIL_ADDRESS> , use the email address associated with your GitHub account. [user@client ~ ] $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_github -C <EMAIL_ADDRESS>","title":"Generating the key"},{"location":"software/git-software-development/#configuring-ssh-to-use-the-key-for-github","text":"Make sure SSH uses the new key by default to access GitHub. Create or edit ~/.ssh/config and append the following lines: Host github.com IdentityFile <YOUR_HOME_DIR>/.ssh/id_github Where is the output of the command: echo $HOME","title":"Configuring SSH to use the key for GitHub"},{"location":"software/git-software-development/#adding-the-ssh-public-key-to-github","text":"Using the GitHub web interface: On the upper right of the screen, click on your profile picture In the menu that pops up, click \"Settings\" On the left-hand sidebar, click \"SSH and GPG keys\" In the top right of the \"SSH keys\" box, click \"New SSH key\" In the \"Title\" field of the dialog that pops up, enter a descriptive name for the key Open the public key file (e.g. ~/.ssh/id_github.pub (don't forget the .pub )) in a text editor and copy its full contents to the clipboard In the \"Key\" field, paste the public key Below the \"Key\" field, click \"Add SSH key\" You should see your new key in the \"SSH keys\" list.","title":"Adding the SSH public key to GitHub"},{"location":"software/git-software-development/#testing-that-shell-access-works","text":"To verify you can authenticate to GitHub using SSH, SSH to git@github.com . You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.'","title":"Testing that shell access works"},{"location":"software/git-software-development/#contribution-workflow","text":"We use the standard GitHub pull request workflow for making contributions to OSG software. If you've never contributed to this project on GitHub before, do the following steps first: Using the GitHub web interface, fork the repo you wish to contribute to. Make a clone of your forked repo on your local machine. [user@client ~ ] $ git clone git@github.com:<USERNAME/PROJECT> Where <USERNAME> is your github username and <PROJECT> is the name of the project you want to contribute to, e.g. in order to clone my local fork of the openscience/technology repository: [user@client ~ ] $ git clone https://github.com/ddavila0/technology.git Note If you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above. If you get some other error, the GitHub page on SSH may contain useful information on troubleshooting. Once you have your local repo, do the following: Create a branch to hold changes that are related to the issue you are working on. Give the <BRANCH> a name that will remind you of its purpose, including any relevant ticket numbers, such as SOFTWARE-2345.pathchange : [user@client ~ ] $ git checkout -b <BRANCH> Make your commits to this branch, then push the branch to your repo on GitHub. [user@client ~ ] $ git push origin <BRANCH> Select your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a Jira ticket number in the message to automatically link the pull request to the Jira ticket. Request a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member. If the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from. If your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request.","title":"Contribution workflow"},{"location":"software/git-software-development/#release-workflow","text":"This section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by opensciencegrid . (If you can approve pull requests, you have write access). A release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by opensciencegrid (the OSG repo): If you haven't already, add the OSG repo as a \"remote\" to your repo: [user@client ~ ] $ git remote add upstream git@github.com:opensciencegrid/<PROJECT> Where <PROJECT> is the name of the project you are going to release, e.g. for openscience/technology repository it would be technology.git Fetch changes from the OSG repo: [user@client ~ ] $ git fetch upstream Compare your branch you are releasing from (probably master ) to its copy in the OSG repo: [user@client ~ ] $ git checkout master ; git diff upstream/master There should be no differences. Once this is done, release the software as you usually do. This process varies from one project to another, but often it involves running make upstream or similar. Check your project's README file for instructions. Test your software. Tag the commit that you made the release from. Git release tags are conventionally called VERSION , where VERSION is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the <TAG> v1.3.0 . Note Once a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from. Create the tag in your local repo: [user@client ~ ] $ git tag <TAG> Push the tag to your own GitHub repo: [user@client ~ ] $ git push origin <TAG> Push the tag to the OSG repo: [user@client ~ ] $ git push upstream <TAG>","title":"Release workflow"},{"location":"software/git-software-development/#best-practices","text":"","title":"Best practices"},{"location":"software/git-software-development/#making-good-pull-requests-the-art-of-good-commits","text":"In addition to writing good code, it's important to organize your changes to make the task of reviewing them easier, both for the reviewer of the pull request, and even for yourself later. Here are some general guidelines and tips.","title":"Making good pull requests (The Art of Good Commits)"},{"location":"software/git-software-development/#put-logically-separate-changes-into-separate-commits","text":"This becomes more relevant if there are a lot of changes in the pull request. Having a single commit with many different changes happening at the same time can make the changes harder to review. If possible, split up logically separate changes into separate commits. As a simple example, if you are renaming a variable in many places, and also refactoring the structure of some code, these changes can be split into two separate commits. This will make it easier when reviewing to see clearly what each commit is trying to accomplish. The process you went through to arrive at your final code may have been different, but you can clean up your commits after the fact. One method is to use git rebase -i to combine (squash) several commits into one, and then use git gui to amend the combined commit, staging the parts that represent each logical change into separate commits. Another example that occasionally comes up is when you want to copy or move a file AND make changes to that file. If you have a single commit that introduces a file to a new location with changes , it will not be obvious from the commit diff itself which parts are the same (moved or copied in) and which parts you are modifying. Instead, by putting the copy or move of the original file into its own commit, and then putting your changes in a separate commit, it will make it clear to the reviewer which parts are changing from the original.","title":"Put logically separate changes into separate commits"},{"location":"software/git-software-development/#avoid-whitespace-noise","text":"There are a few considerations to note when it comes to whitespace. Avoid adding spaces at the end of lines. These are generally considered \"noise\" that will get cleaned up later (sometimes automatically, depending on editor settings). It's not necessary to \"fix\" this kind of whitespace noise everywhere you happen to find it in existing files, but it's fine to remove trailing whitespace for lines that you are already modifying for your own changes. Do not strip the final newline at the end-of-file. Some text editors will automatically strip the final newline at the end of file, but this is a form of whitespace noise similar to trailing spaces. If that is the case for your editor, please configure it not to strip the newline at EOF. (GitHub will show the diff for files with a missing newline at EOF with a red circle-minus symbol with the mouseover text \"No newline at end of file\".) Avoid mixing tabs and spaces. With the exception of Makefiles and Go source code, indentation should be done with regular spaces, not tabs. Please configure your text editor accordingly. Mixing tabs and spaces in indentation is problematic because different editor settings can make tab stops appear at different widths. As with trailing whitespace, it's fine to convert stray tabs to spaces on lines you are modifying, but it is not necessary to fix them everywhere, if that is not the purpose of your pull request. Put large whitespace changes into a separate commit. If you do want to change a significant amount of whitespace (either converting tabs to spaces on many lines, or perhaps adjusting the amount of indentation, or wrapping text at a different width), make your whitespace-only changes as a separate commit. This will make it clear that, although many lines may be changing, there is no functional change for that particular commit. Then any functional changes to the text in a following commit will be easier to review.","title":"Avoid whitespace noise"},{"location":"software/git-software-development/#dont-commit-large-files","text":"Try to keep the repository small by not committing files that are too large. Avoid committing large binaries or data sets. Ask yourself if they need to be in the repository before committing. If, somehow, a large file has been committed and pushed, it's not enough to remove the file in another commit. The file must be removed from all of the history or else the repository will still contain the large file as Git keeps track of deleted files. This can be done using the git rebase -i and git filter-repo commands. If a large file must be added to the repository. Use Git LFS","title":"Don't commit large files"},{"location":"software/git-software-development/#verify-that-only-the-files-intended-are-modified-in-each-commit","text":"Sometimes you may have several files modified at once, but you only intend to commit a subset of the changes. In such cases you should be aware that git commit -a will include all the modified files in your commit. Likewise, if you have some untracked files in your working copy, that you do not intend to commit, be aware that git add . will introduce these as new files in the commit. Note If you do use the -a option for git commit , you may want to consider using the -v option along with it (i.e. git commit -av ), which will show you the diff to be committed in your editor while you are typing your commit message. After making a commit locally, you can verify that only the files you intended to modify were included in the commit by running git show --stat . Or to review the actual changes to those files, git show (without --stat ). If you find unintended files included in the commit, you can amend the commit so that it does not include changes to file-not-to-commit like so: [user@client ~ ] $ git reset HEAD^ file-not-to-commit [user@client ~ ] $ git commit --amend If you have multiple commits ready for a pull request, you can review the high-level changes for each commit with git log --stat origin/master.. (or origin/main.. , whichever is the name of the main origin branch). Tools like git gui also provide a way to review commits. If you find unintended files included in earlier commits, you can do a git rebase -i origin/master and edit the commits in question, which will give you a chance to amend a particular commit as shown above.","title":"Verify that only the files intended are modified in each commit"},{"location":"software/git-software-development/#squash-noisy-work-in-progress-commits","text":"Naturally in the trial-and-error-prone process of development, there will be many changes along the way that didn't make the final cut. This is good and healthy, and it's perfectly reasonable to be making many small work-in-progress commits locally while you are developing. However, for the reviewer, the relevant thing is what final changes are being introduced into the codebase. Having to review several ideas that were put into and then taken out of the changeset is a distraction, and makes it harder to see what the end result is for the new changes. If you have such work-in-progress commits, first combine them (this is also called \"squashing\" or \"rebasing\"), and then break them up into logically distinct commits as necessary, representing the final changeset. As mentioned above, one way to do this is with a combination of git rebase -i and git gui , though there are other third-party tools (e.g., magit) available also.","title":"Squash noisy work-in-progress commits"},{"location":"software/git-software-development/#write-a-succinct-subject-to-explain-what-each-commit-does","text":"The first line of a commit message is the \"subject\" (or sometimes also called the \"title\"). It should be short and sweet (at most 72 characters) and briefly state what the commit is designed to do. As a convention, the subject of the commit message should be written in the imperative - that is, it should be written as if it were a command. For instance, a subject should start with \"fix a bug\" rather than \"fixing\" or \"fixes\".","title":"Write a succinct subject to explain what each commit does"},{"location":"software/git-software-development/#explain-why-a-change-was-made-in-the-commit-message","text":"It is generally important also to explain why a change was made. If this is not covered by the succinct subject line of your commit, you should explain the rationale behind your change in the commit message body. (The commit message starts with the subject line, then is optionally followed by a blank line plus the message body.) You can also explain in the commit message body how this commit accomplishes the stated purpose in the subject - and you may find yourself needing to do this if there are some tricky details in the changes. But even if it is perfectly clear from the code and your commit message what you are changing and how you are going about it, it is not always clear why the change is needed or desired - so it is important to explain your reasons, in order to make this clear to the reviewer. For an example of \"explaining your reasons\", see this commit message body .","title":"Explain why a change was made in the commit message"},{"location":"software/git-software-development/#summarize-your-commits-in-the-pull-request-title","text":"The title of a pull request is analogous to the subject of a commit. If you have only one commit in your pull request, GitHub will by default set the pull request title and body to match that commit's subject and body; and that default is acceptable for single-commit pull requests. But if you have multiple commits in your pull request, you should try to capture the overall goal of these commits in your pull request title. In the pull request body, you can also mention or discuss the high-level changes from each commit, and if relevant discuss how these changes work together for the overall goal of the pull request.","title":"Summarize your commits in the pull request title"},{"location":"software/git-software-development/#choose-a-separate-descriptive-branch-name-for-each-pull-request","text":"GitHub allows creating pull requests entirely on their web interface, and will automatically suggest a generic branch name like patch-42 . But this is boring and not especially helpful to the reviewer or to the one submitting the pull request. Instead, choose a short name for the branch that describes the topic of the changes or the feature being introduced. For instance, fix-memory-leak or scitokens-support . (As will be discussed more later, it is best to prefix the branch name with a ticket reference as well.) Note that each pull request should get its own branch name, even if two pull requests are for the same ticket and the topic is similar. New commits pushed to a branch for a pull request will automatically show up as part of that pull request; so a second pull request needs a separate branch to track the separate set of changes.","title":"Choose a separate, descriptive branch name for each pull request"},{"location":"software/git-software-development/#reference-any-relevant-tickets","text":"Code changes often are related to a Jira ticket, for instance SOFTWARE-1234. By referencing the name of a ticket in your pull request, it provides a convenient way to look into the background context for the change; and later on down the road, it makes it easy to find which changes were made for a particular task, referenced by the ticket name. Ideally, you can include a ticket reference each of these three places: Your branch name. For example, name the branch in your fork of the GitHub repo for the pull request SOFTWARE-1234.fix-memory-leak . Your commit messages. For example, the subject of your commit message might read, fix a memory leak (SOFTWARE-1234) . If you have trouble squeezing the ticket name into the subject line, or if you have a number of related tickets that you want to reference, it is also OK to mention them later in the commit message body. The pull request title. If your pull request is just a single commit, and you have the ticket reference in the subject line of the commit message, GitHub will include this in the pull request title automatically. But if you have multiple commits, or you have only included the ticket reference in the body of the commit message, or more generally if you want to tweak the title of the pull request, you should in any case make a point to include the ticket name in the title of the pull request. (By convention, we include this at the end of the title, in parentheses.) If there is no ticket associated with your changes, consider creating one (or asking an OSG software team member to create one) before submitting your pull request.","title":"Reference any relevant tickets"},{"location":"software/git-software-development/#further-reading","text":"There are a number of articles and guides for making good git commits and good pull requests - a simple search will turn up plenty of material for the interested reader. See online guides such as this one for more details.","title":"Further reading"},{"location":"software/git-software-development/#brownie-points","text":"You will get brownie points from Carl, personally, if you strive to make your code (and other text files) fit within an 80-column terminal window.","title":"Brownie points"},{"location":"software/git-software-development/#reviewing-pull-requests","text":"There are a couple items to note about the review process for GitHub pull requests.","title":"Reviewing pull requests"},{"location":"software/git-software-development/#batch-comments-in-a-formal-review","text":"When reviewing a pull request, GitHub allows you to comment on lines and presents the option to \"Add single comment\" or \"Start a review\". A single comment added will not be tied to your review, and a separate email notification will be sent for every time you click \"Add single comment\". Especially for reviewing larger pull requests, we generally prefer to \"Start a review\", and then \"Add review comment\" for subsequent comments. This will tie all of your comments and suggestions together as part of your review. When you complete your review, you will have the opportunity to make summary comments about the changes, when you select Approve/Comment/Request changes. By \"batching\" all of your review comments this way, a single email notification will be sent for your review, which contains all of your review comments together.","title":"Batch comments in a formal review"},{"location":"software/git-software-development/#batch-commits-when-accepting-suggestions-from-a-review","text":"When someone reviews your pull request, they may make suggestions that tweak your changes. Similar to review comments, suggestions from a review can either be applied one at a time (Commit suggestion), or they can be batched and applied together. To batch suggestions, first you need to open the \"Files changed\" tab; then for each suggestion you want to accept, click \"Add suggestion to batch\". Finally, click \"Commit suggestions\" to apply all batched suggestions as a single commit. Generally we prefer to batch related changes or miscellaneous tweaks rather than applying each one individually. But if there are a number of suggestions of a different nature, it is OK to group them such that you apply one batch for each set of related suggestions (consistent with the guideline to put logically separate changes into separate commits).","title":"Batch commits when accepting suggestions from a review"},{"location":"software/ipv6-testing/","text":"Testing Software with IPv6 \u00b6 About this Document \u00b6 This document provides instructions on setting up a host with an IPv6 address for testing the OSG software stack. The plan is to be able to spin up special Fermicloud VM\u2019s that have corresponding public IPv6 addresses meaning that there will be a limit of ~15 VM\u2019s at one time. For more information on IPv6, consult Wikipedia . Requirements \u00b6 Be familiar with your institute's network policy and firewall configuration 1 Root access is required to configure iptables Enabling IPV6 \u00b6 Determine the public IPv6 address of your host. In the example below that would be 2001:400:2410:29::182 : user@host $ nslookup -type = aaaa <HOSTNAME> Server: 132.239.0.252 Address: 132.239.0.252#53 Non-authoritative answer: ipv6vm001.fnal.gov has AAAA address 2001:400:2410:29::182 Replacing <HOSTNAME> with your machine's hostname. Ask your network administrator for your IPv6 default gateway Modify /etc/sysconfig/network-scripts/ifcfg-eth0 and be sure these lines exist, and : IPV6INIT=yes IPV6_AUTOCONF=no IPV6ADDR=<IPV6 ADDRESS>\" IPV6_DEFAULTGW=\"The IPV6 Default Gateway\" Replace <IPV6 ADDRESS> with the address found in step 1. Restart the network devices: root@host # service network restart Shutting down interface eth0: [ OK ] Shutting down loopback interface: [ OK ] Bringing up loopback interface: [ OK ] Bringing up interface eth0: [ OK ] Testing IPv6 connectivity \u00b6 To verify that the VM is capable of IPv6 we will be using the ping6 command between the test VM and another IPv6 capable machine From another IPv6 capable machine, ping your VM: user@host $ ping6 ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov(ipv6vm001.fnal.gov) 56 data bytes 64 bytes from ipv6vm001.fnal.gov: icmp_seq=1 ttl=51 time=68.1 ms 64 bytes from ipv6vm001.fnal.gov: icmp_seq=2 ttl=51 time=57.6 ms From your test VM, ping another IPv6 capable machine: [efajardo@ipv6vm001 ~]# ping6 uaf-4.t2.ucsd.edu PING uaf-4.t2.ucsd.edu(uaf-4.t2.ucsd.edu) 56 data bytes 64 bytes from uaf-4.t2.ucsd.edu: icmp_seq=1 ttl=51 time=57.6 ms Verifying SSH over IPv6 \u00b6 Make sure you can login to your VM over IPv6. Currently, Fermilab's kerberos does not support SSH over IPv6. Add your ssh_key to your machine and make sure /etc/ssh/sshd_config has the following lines: RSAAuthentication yes PubkeyAuthentication yes Try connecting to you IPv6 enabled machine over SSH: efajardo@uaf-4 ~$ ssh -6 root@ipv6vm001.fnal.gov Last login: Wed Jun 11 14:51:47 2014 from 2607:f720:1700:1b30:21f:c6ff:feeb:2631 [root@ipv6vm001 ~]# Disabling IPv4 \u00b6 If you were able to log into your VM over IPv6, you can disable IPv4 and try to communicate exclusively over IPv6. Comment the IPADDR line in /etc/sysconfig/network-scripts/ifcfg-eth0 : #IPADDR=131.225.41.182 IPV6ADDR=\"2607:f720:1700:1b30::9b\" Note Ensure that your IPV6ADDR is uncommented otherwise you will not be able to connect to the host again Restart the network services: root@host # service network restart The ping command should no longer work: root@host # ping ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov (131.225.41.182): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Disabling IPv6 \u00b6 In your testing, you may find the need to disable IPv6. root@host # sysctl -w net.ipv6.conf.all.disable_ipv6 = 1 root@host # service network restart The ping6 command should no longer work: root@host # ping6 ipv6vm001.fnal.gov Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Testing in mixed mode \u00b6 To test IPv6 in mixed mode, you can use the ntop tool to monitor traffic over IPv6. ntop is installed on all the test machines and you can access the web interface at hostname:3000 . To see a table that displays network traffic between your VM and another host by going to All Protocols -> Traffic and looking at the IPv6 column. Enforcing communication over IPv6 \u00b6 If you want to enforce IPv6 over mixed mode you can try using the IPv6 address for whatever software that you are testing. For example with xrdcp: root@host # xrdcp -d 1 /tmp/first_test root:// [ 2607 :f720:1700:1b30::a4 ] :1094//tmp/first_test_8 [19B/19B][100%][==================================================][0B/s] Notice that the IPv6 address follows RFC2732 .","title":"IPv6 Testing"},{"location":"software/ipv6-testing/#testing-software-with-ipv6","text":"","title":"Testing Software with IPv6"},{"location":"software/ipv6-testing/#about-this-document","text":"This document provides instructions on setting up a host with an IPv6 address for testing the OSG software stack. The plan is to be able to spin up special Fermicloud VM\u2019s that have corresponding public IPv6 addresses meaning that there will be a limit of ~15 VM\u2019s at one time. For more information on IPv6, consult Wikipedia .","title":"About this Document"},{"location":"software/ipv6-testing/#requirements","text":"Be familiar with your institute's network policy and firewall configuration 1 Root access is required to configure iptables","title":"Requirements"},{"location":"software/ipv6-testing/#enabling-ipv6","text":"Determine the public IPv6 address of your host. In the example below that would be 2001:400:2410:29::182 : user@host $ nslookup -type = aaaa <HOSTNAME> Server: 132.239.0.252 Address: 132.239.0.252#53 Non-authoritative answer: ipv6vm001.fnal.gov has AAAA address 2001:400:2410:29::182 Replacing <HOSTNAME> with your machine's hostname. Ask your network administrator for your IPv6 default gateway Modify /etc/sysconfig/network-scripts/ifcfg-eth0 and be sure these lines exist, and : IPV6INIT=yes IPV6_AUTOCONF=no IPV6ADDR=<IPV6 ADDRESS>\" IPV6_DEFAULTGW=\"The IPV6 Default Gateway\" Replace <IPV6 ADDRESS> with the address found in step 1. Restart the network devices: root@host # service network restart Shutting down interface eth0: [ OK ] Shutting down loopback interface: [ OK ] Bringing up loopback interface: [ OK ] Bringing up interface eth0: [ OK ]","title":"Enabling IPV6"},{"location":"software/ipv6-testing/#testing-ipv6-connectivity","text":"To verify that the VM is capable of IPv6 we will be using the ping6 command between the test VM and another IPv6 capable machine From another IPv6 capable machine, ping your VM: user@host $ ping6 ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov(ipv6vm001.fnal.gov) 56 data bytes 64 bytes from ipv6vm001.fnal.gov: icmp_seq=1 ttl=51 time=68.1 ms 64 bytes from ipv6vm001.fnal.gov: icmp_seq=2 ttl=51 time=57.6 ms From your test VM, ping another IPv6 capable machine: [efajardo@ipv6vm001 ~]# ping6 uaf-4.t2.ucsd.edu PING uaf-4.t2.ucsd.edu(uaf-4.t2.ucsd.edu) 56 data bytes 64 bytes from uaf-4.t2.ucsd.edu: icmp_seq=1 ttl=51 time=57.6 ms","title":"Testing IPv6 connectivity"},{"location":"software/ipv6-testing/#verifying-ssh-over-ipv6","text":"Make sure you can login to your VM over IPv6. Currently, Fermilab's kerberos does not support SSH over IPv6. Add your ssh_key to your machine and make sure /etc/ssh/sshd_config has the following lines: RSAAuthentication yes PubkeyAuthentication yes Try connecting to you IPv6 enabled machine over SSH: efajardo@uaf-4 ~$ ssh -6 root@ipv6vm001.fnal.gov Last login: Wed Jun 11 14:51:47 2014 from 2607:f720:1700:1b30:21f:c6ff:feeb:2631 [root@ipv6vm001 ~]#","title":"Verifying SSH over IPv6"},{"location":"software/ipv6-testing/#disabling-ipv4","text":"If you were able to log into your VM over IPv6, you can disable IPv4 and try to communicate exclusively over IPv6. Comment the IPADDR line in /etc/sysconfig/network-scripts/ifcfg-eth0 : #IPADDR=131.225.41.182 IPV6ADDR=\"2607:f720:1700:1b30::9b\" Note Ensure that your IPV6ADDR is uncommented otherwise you will not be able to connect to the host again Restart the network services: root@host # service network restart The ping command should no longer work: root@host # ping ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov (131.225.41.182): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1","title":"Disabling IPv4"},{"location":"software/ipv6-testing/#disabling-ipv6","text":"In your testing, you may find the need to disable IPv6. root@host # sysctl -w net.ipv6.conf.all.disable_ipv6 = 1 root@host # service network restart The ping6 command should no longer work: root@host # ping6 ipv6vm001.fnal.gov Request timeout for icmp_seq 0 Request timeout for icmp_seq 1","title":"Disabling IPv6"},{"location":"software/ipv6-testing/#testing-in-mixed-mode","text":"To test IPv6 in mixed mode, you can use the ntop tool to monitor traffic over IPv6. ntop is installed on all the test machines and you can access the web interface at hostname:3000 . To see a table that displays network traffic between your VM and another host by going to All Protocols -> Traffic and looking at the IPv6 column.","title":"Testing in mixed mode"},{"location":"software/ipv6-testing/#enforcing-communication-over-ipv6","text":"If you want to enforce IPv6 over mixed mode you can try using the IPv6 address for whatever software that you are testing. For example with xrdcp: root@host # xrdcp -d 1 /tmp/first_test root:// [ 2607 :f720:1700:1b30::a4 ] :1094//tmp/first_test_8 [19B/19B][100%][==================================================][0B/s] Notice that the IPv6 address follows RFC2732 .","title":"Enforcing communication over IPv6"},{"location":"software/koji-mass-rebuilds/","text":"Mass RPM Rebuilds for a new Build Target in Koji \u00b6 Whenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner. This document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds. New RHEL version vs new OSG series \u00b6 New RHEL version \u00b6 For a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later. New OSG series \u00b6 For a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series. A prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *). After some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort. Doing scratch builds of everything first \u00b6 Before doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target. Doing mass scratch rebuilds for an existing series is easy, as they can all be done at once. Relatedly, doing a round of scratch builds after successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues. Options for calculating build dependencies \u00b6 We can get dependency information from a number of places: scraping .spec files for Requires/BuildRequires/Provides and %package names querying existing rpms directly on koji-hub and our OS/EPEL mirrors ( rpm -q ) querying srpms from osg-build prebuild directly for build requirements inspecting previous buildroots to determine resolved build dependencies use repoquery to determine whatrequires/whatprovides for packages use yum-builddep to find packages with all build requirements available using the repodata (primary+filelists) from rpm repositories, including: upcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR osg-upcoming-elX-build, which includes them all One important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build. That is, (A BuildRequires B) and (B Requires C) implies A BuildRequires C . Combined with the fact that runtime requirements are transitive, that is, (A Requires B) and (B Requires C) implies A Requires C , computing build requirements is a recursive operation, which can be many levels deep. Another question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz >= 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.) Note also that the dependencies/capabilities for a given package often varies between different rhel versions. Pre-computing (predictive) vs just-in-time \u00b6 Two different approaches to determining dependency order for building are: pre compute all dependencies based on an existing series/rhel version, OR compute which remaining packages have all build reqs satisfied now The first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target. The second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets. It may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first. Package list closure, pruning \u00b6 At some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts). But if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target. Proposal / Recommendations \u00b6 As mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version. For a new OSG series: \u00b6 update native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages tag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development) build all packages in new packaging area into new build target at once for all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development) For a new RHEL version: \u00b6 pull the repodata from the relevant *-build repo from koji: for pre-computing, use a build repo from an existing rhel version: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/ for just-in-time, use the new build repo: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/ the primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.) generate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo. the primary (sqlite) file can be used to get build-requires. use sql to resolve direct dependencies at the package name level: src-pkg: bin-pkg (BuildRequires) bin-pkg: bin-pkg (Requires) bin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies) resolve this list into a full list of recursive build dependencies. Since this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python. build ready-to-be-built packages update our copy of the repodata from the regen'ed *-build repo, as often as new versions become available update our dependency lists repeat until all packages are built","title":"Koji Mass Rebuilds"},{"location":"software/koji-mass-rebuilds/#mass-rpm-rebuilds-for-a-new-build-target-in-koji","text":"Whenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner. This document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds.","title":"Mass RPM Rebuilds for a new Build Target in Koji"},{"location":"software/koji-mass-rebuilds/#new-rhel-version-vs-new-osg-series","text":"","title":"New RHEL version vs new OSG series"},{"location":"software/koji-mass-rebuilds/#new-rhel-version","text":"For a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later.","title":"New RHEL version"},{"location":"software/koji-mass-rebuilds/#new-osg-series","text":"For a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series. A prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *). After some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort.","title":"New OSG series"},{"location":"software/koji-mass-rebuilds/#doing-scratch-builds-of-everything-first","text":"Before doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target. Doing mass scratch rebuilds for an existing series is easy, as they can all be done at once. Relatedly, doing a round of scratch builds after successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues.","title":"Doing scratch builds of everything first"},{"location":"software/koji-mass-rebuilds/#options-for-calculating-build-dependencies","text":"We can get dependency information from a number of places: scraping .spec files for Requires/BuildRequires/Provides and %package names querying existing rpms directly on koji-hub and our OS/EPEL mirrors ( rpm -q ) querying srpms from osg-build prebuild directly for build requirements inspecting previous buildroots to determine resolved build dependencies use repoquery to determine whatrequires/whatprovides for packages use yum-builddep to find packages with all build requirements available using the repodata (primary+filelists) from rpm repositories, including: upcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR osg-upcoming-elX-build, which includes them all One important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build. That is, (A BuildRequires B) and (B Requires C) implies A BuildRequires C . Combined with the fact that runtime requirements are transitive, that is, (A Requires B) and (B Requires C) implies A Requires C , computing build requirements is a recursive operation, which can be many levels deep. Another question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz >= 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.) Note also that the dependencies/capabilities for a given package often varies between different rhel versions.","title":"Options for calculating build dependencies"},{"location":"software/koji-mass-rebuilds/#pre-computing-predictive-vs-just-in-time","text":"Two different approaches to determining dependency order for building are: pre compute all dependencies based on an existing series/rhel version, OR compute which remaining packages have all build reqs satisfied now The first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target. The second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets. It may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first.","title":"Pre-computing (predictive) vs just-in-time"},{"location":"software/koji-mass-rebuilds/#package-list-closure-pruning","text":"At some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts). But if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target.","title":"Package list closure, pruning"},{"location":"software/koji-mass-rebuilds/#proposal-recommendations","text":"As mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version.","title":"Proposal / Recommendations"},{"location":"software/koji-mass-rebuilds/#for-a-new-osg-series","text":"update native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages tag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development) build all packages in new packaging area into new build target at once for all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development)","title":"For a new OSG series:"},{"location":"software/koji-mass-rebuilds/#for-a-new-rhel-version","text":"pull the repodata from the relevant *-build repo from koji: for pre-computing, use a build repo from an existing rhel version: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/ for just-in-time, use the new build repo: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/ the primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.) generate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo. the primary (sqlite) file can be used to get build-requires. use sql to resolve direct dependencies at the package name level: src-pkg: bin-pkg (BuildRequires) bin-pkg: bin-pkg (Requires) bin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies) resolve this list into a full list of recursive build dependencies. Since this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python. build ready-to-be-built packages update our copy of the repodata from the regen'ed *-build repo, as often as new versions become available update our dependency lists repeat until all packages are built","title":"For a new RHEL version:"},{"location":"software/koji-workflow/","text":"Koji Workflow \u00b6 This covers the basics of using and understanding the OSG Koji instance. It is meant primarily for OSG Software team members who need to interact with the service. Terminology \u00b6 Using and understanding the following terminology correctly will help in the reading of this document: Package This refers to a named piece of software in the Koji database. An example would be \"xrootd\". Build A specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package. Tag A named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-25-el9-development\" tag, which contains (among others) the \"xrootd\" package and the \"xrootd-5.9.1-1.2.osg25-el9\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include AlmaLinux base or EPEL. Note A tag is NOT a yum repository. Target A target consists of a build tag and a destination tag. An example is \"osg-25-main-el9\", where the build tag is \"osg-25-main-el9\" and the destination tag is \"osg-25-main-el9-development\". A target is used by the build task to know what repository to build from and tag to build into. Task A unit of work for Koji. Several common tasks are: build This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask. Note If the build task is marked as \"scratch\", then it won't result in a saved Build. buildArch This task takes a SRPM, architecture name, and a Koji repository as an input, and runs mock to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful. tagBuild This adds a successful build to a given tag. newRepo This creates a new repository from a given tag. Build artifacts The results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs. Repository A yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag. Obtaining Access \u00b6 Building OSG packages in Koji requires these privileges: access to the OSG Software Packaging repository at https://github.com/osg-htc/software-packaging access to osgsw-ap.chtc.wisc.edu for uploading to the upstream sources directory access to the Koji service via a Kerberos credential If you are not already registered as an OSG Contact, follow the registration instructions at this page: https://osg-htc.org/docs/common/contact-registration/ Once you are registered, open a Freshdesk ticket or send email to help@osg-htc.org requesting access to OSG build services. Include the following information: your GitHub username your Kerberos principal if you already have one; for people with a UW NetID account, this is <netid>@AD.WISC.EDU and for people with a Fermilab account, this is <username>@FNAL.GOV Initial Setup \u00b6 You will be using the OSG Build Tools to interact with Koji. See the installation guide on that page for information on getting started. Using Koji \u00b6 Creating a test build \u00b6 Before pushing package changes to the OSG Software Packaging repository, you should create a \"scratch build\". This builds an RPM from the current directory using Koji, but does not tag the resulting package. To make a scratch build and download the resulting files to your machine, run: $ osg-build koji --scratch --getfiles <PACKAGE DIRECTORY> If you do not want to download the files, omit the --getfiles flag. You may also visit the links that osg-build printed to download the files later. Creating a new build \u00b6 We create a new build in Koji from the package's directory in OSG Software Packaging repository . If a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You can do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji. To do a build, execute the following command from within an up-to-date clone of the repository: [you@host]$ osg-build koji <PACKAGE NAME> Build task Results \u00b6 How to find build results \u00b6 The most recent build results are always shown on the home page of Koji: https://koji.osg-htc.org/koji/index Clicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM. If your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a glob, e.g. osdf* ), and it will bring up a list of all builds for that package. You can find your build from there. For example, the xrootd package page is here: https://koji.osg-htc.org/koji/packageinfo?packageID=89 And the xrootd-5.9.1-1.2.osg25.el9 build is here: https://koji.osg-htc.org/koji/buildinfo?buildID=18884 How to get the resulting RPM into a repository \u00b6 Once a package has been built, it will be signed and added to a tag, typically one of the -development tags. The build will eventually be copied and made available for installation at repo.osg-htc.org , in typically less than half an hour. The build will be available for use as a build dependency in under five minutes; Koji will wait for the repo to be updated (\"regenerated\") before doing another build. The OSG repos with names ending in -minefield are built from the -development repos; installing RPMs from those repos pulls directly from the Koji build system. You may update the -minefield repos by running the osg-koji regen-repo command on the corresponding -development repo. For example, to update the osg-minefield repo for OSG series 25-main on distro el9 , run $ osg-koji regen-repo osg-25-main-el9-development A regen-repo task will be created -- you can watch its progress in the web interface. Note If the output you get is something like Repo info: https://koji.osg-htc.org/koji/repoinfo?repoID=153995 , it means that the build repo is already current. If you still want to force repo creation, add the --make-task argument to the above regen-repo command. To do this for multiple distro versions, use a loop. For example: $ for el in el8 el9 el10 ; do osg-koji regen-repo osg-25-main- ${ el } -development ; done Note Due to SOFTWARE-6069 , you may need to run osg-koji regen-repo by hand to update the minefield repos. You may check the status of regen-repo tasks in the Koji web interface . Debugging build issues \u00b6 Failed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include: root.log This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here. 404 Errors If you see Error downloading packages and HTTP Error 404 - Not Found errors in your root.log , this commonly indicates that an rpm repo mirror was updated and our build repo is out-of-date. This can be fixed by regenerating the relevant build repos for your builds. This is usually something like osg-25-main-el8-build or osg-24-upcoming-el9-build ; but you can find the exact build tag by clicking the Build Target link for the koji task, and whatever is listed for the Build Tag is the name of the repo to regen. Regenerate each repo that failed with 404 errors: $ osg-koji regen-repo <BUILD TAG> build.log The output of the rpmbuild executable. If your package fails to compile, the reason will show up here. Other errors package <PACKAGE NAME> not in list for tag <TAG> This happens when the name of the directory your package is in does not match the name of the package. You must rename one or the other and commit your changes before trying again. Promoting Builds from Development -> Testing \u00b6 Software team members can promote any package to testing. To promote from development to testing: Using osg-promote \u00b6 Before using osg-promote , authenticate to Koji using kinit and your Kerberos principal (same as for osg-build ). If you want to promote the latest version: [you@host]$ osg-promote -r <OSGVER>-testing <PACKAGE NAME> <PACKAGE NAME> is the bare package name without version, e.g. gratia-probe . If you want to promote a specific version: [you@host]$ osg-promote -r <OSGVER>-testing <BUILD NAME> <BUILD NAME> is a full name-version-revision.disttag such as gratia-probe-1.17.0-2.osg33.el6 . <OSGVER> is the OSG major version that you are promoting for (e.g. 24 ). osg-promote will promote the builds of a package for all distro versions (el8, el9, etc.). After promoting, copy and paste the Jira code osg-promote produces into the Jira ticket that you are working on. For osg-promote , you may omit the dist tag (e.g. .osg25.el9 ); the script will add the appropriate disttag on. See OSG Build Tools for full details on osg-promote . Further reading \u00b6 Official Koji documentation: https://docs.pagure.org/koji/ Fedora's koji documentation: https://fedoraproject.org/wiki/Koji Fedora's \"Using Koji\" page: https://docs.fedoraproject.org/en-US/package-maintainers/Using_the_Koji_Build_System/ Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.","title":"Koji Workflow"},{"location":"software/koji-workflow/#koji-workflow","text":"This covers the basics of using and understanding the OSG Koji instance. It is meant primarily for OSG Software team members who need to interact with the service.","title":"Koji Workflow"},{"location":"software/koji-workflow/#terminology","text":"Using and understanding the following terminology correctly will help in the reading of this document: Package This refers to a named piece of software in the Koji database. An example would be \"xrootd\". Build A specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package. Tag A named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-25-el9-development\" tag, which contains (among others) the \"xrootd\" package and the \"xrootd-5.9.1-1.2.osg25-el9\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include AlmaLinux base or EPEL. Note A tag is NOT a yum repository. Target A target consists of a build tag and a destination tag. An example is \"osg-25-main-el9\", where the build tag is \"osg-25-main-el9\" and the destination tag is \"osg-25-main-el9-development\". A target is used by the build task to know what repository to build from and tag to build into. Task A unit of work for Koji. Several common tasks are: build This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask. Note If the build task is marked as \"scratch\", then it won't result in a saved Build. buildArch This task takes a SRPM, architecture name, and a Koji repository as an input, and runs mock to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful. tagBuild This adds a successful build to a given tag. newRepo This creates a new repository from a given tag. Build artifacts The results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs. Repository A yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag.","title":"Terminology"},{"location":"software/koji-workflow/#obtaining-access","text":"Building OSG packages in Koji requires these privileges: access to the OSG Software Packaging repository at https://github.com/osg-htc/software-packaging access to osgsw-ap.chtc.wisc.edu for uploading to the upstream sources directory access to the Koji service via a Kerberos credential If you are not already registered as an OSG Contact, follow the registration instructions at this page: https://osg-htc.org/docs/common/contact-registration/ Once you are registered, open a Freshdesk ticket or send email to help@osg-htc.org requesting access to OSG build services. Include the following information: your GitHub username your Kerberos principal if you already have one; for people with a UW NetID account, this is <netid>@AD.WISC.EDU and for people with a Fermilab account, this is <username>@FNAL.GOV","title":"Obtaining Access"},{"location":"software/koji-workflow/#initial-setup","text":"You will be using the OSG Build Tools to interact with Koji. See the installation guide on that page for information on getting started.","title":"Initial Setup"},{"location":"software/koji-workflow/#using-koji","text":"","title":"Using Koji"},{"location":"software/koji-workflow/#creating-a-test-build","text":"Before pushing package changes to the OSG Software Packaging repository, you should create a \"scratch build\". This builds an RPM from the current directory using Koji, but does not tag the resulting package. To make a scratch build and download the resulting files to your machine, run: $ osg-build koji --scratch --getfiles <PACKAGE DIRECTORY> If you do not want to download the files, omit the --getfiles flag. You may also visit the links that osg-build printed to download the files later.","title":"Creating a test build"},{"location":"software/koji-workflow/#creating-a-new-build","text":"We create a new build in Koji from the package's directory in OSG Software Packaging repository . If a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You can do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji. To do a build, execute the following command from within an up-to-date clone of the repository: [you@host]$ osg-build koji <PACKAGE NAME>","title":"Creating a new build"},{"location":"software/koji-workflow/#build-task-results","text":"","title":"Build task Results"},{"location":"software/koji-workflow/#how-to-find-build-results","text":"The most recent build results are always shown on the home page of Koji: https://koji.osg-htc.org/koji/index Clicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM. If your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a glob, e.g. osdf* ), and it will bring up a list of all builds for that package. You can find your build from there. For example, the xrootd package page is here: https://koji.osg-htc.org/koji/packageinfo?packageID=89 And the xrootd-5.9.1-1.2.osg25.el9 build is here: https://koji.osg-htc.org/koji/buildinfo?buildID=18884","title":"How to find build results"},{"location":"software/koji-workflow/#how-to-get-the-resulting-rpm-into-a-repository","text":"Once a package has been built, it will be signed and added to a tag, typically one of the -development tags. The build will eventually be copied and made available for installation at repo.osg-htc.org , in typically less than half an hour. The build will be available for use as a build dependency in under five minutes; Koji will wait for the repo to be updated (\"regenerated\") before doing another build. The OSG repos with names ending in -minefield are built from the -development repos; installing RPMs from those repos pulls directly from the Koji build system. You may update the -minefield repos by running the osg-koji regen-repo command on the corresponding -development repo. For example, to update the osg-minefield repo for OSG series 25-main on distro el9 , run $ osg-koji regen-repo osg-25-main-el9-development A regen-repo task will be created -- you can watch its progress in the web interface. Note If the output you get is something like Repo info: https://koji.osg-htc.org/koji/repoinfo?repoID=153995 , it means that the build repo is already current. If you still want to force repo creation, add the --make-task argument to the above regen-repo command. To do this for multiple distro versions, use a loop. For example: $ for el in el8 el9 el10 ; do osg-koji regen-repo osg-25-main- ${ el } -development ; done Note Due to SOFTWARE-6069 , you may need to run osg-koji regen-repo by hand to update the minefield repos. You may check the status of regen-repo tasks in the Koji web interface .","title":"How to get the resulting RPM into a repository"},{"location":"software/koji-workflow/#debugging-build-issues","text":"Failed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include: root.log This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here. 404 Errors If you see Error downloading packages and HTTP Error 404 - Not Found errors in your root.log , this commonly indicates that an rpm repo mirror was updated and our build repo is out-of-date. This can be fixed by regenerating the relevant build repos for your builds. This is usually something like osg-25-main-el8-build or osg-24-upcoming-el9-build ; but you can find the exact build tag by clicking the Build Target link for the koji task, and whatever is listed for the Build Tag is the name of the repo to regen. Regenerate each repo that failed with 404 errors: $ osg-koji regen-repo <BUILD TAG> build.log The output of the rpmbuild executable. If your package fails to compile, the reason will show up here. Other errors package <PACKAGE NAME> not in list for tag <TAG> This happens when the name of the directory your package is in does not match the name of the package. You must rename one or the other and commit your changes before trying again.","title":"Debugging build issues"},{"location":"software/koji-workflow/#promoting-builds-from-development-testing","text":"Software team members can promote any package to testing. To promote from development to testing:","title":"Promoting Builds from Development -&gt; Testing"},{"location":"software/koji-workflow/#using-osg-promote","text":"Before using osg-promote , authenticate to Koji using kinit and your Kerberos principal (same as for osg-build ). If you want to promote the latest version: [you@host]$ osg-promote -r <OSGVER>-testing <PACKAGE NAME> <PACKAGE NAME> is the bare package name without version, e.g. gratia-probe . If you want to promote a specific version: [you@host]$ osg-promote -r <OSGVER>-testing <BUILD NAME> <BUILD NAME> is a full name-version-revision.disttag such as gratia-probe-1.17.0-2.osg33.el6 . <OSGVER> is the OSG major version that you are promoting for (e.g. 24 ). osg-promote will promote the builds of a package for all distro versions (el8, el9, etc.). After promoting, copy and paste the Jira code osg-promote produces into the Jira ticket that you are working on. For osg-promote , you may omit the dist tag (e.g. .osg25.el9 ); the script will add the appropriate disttag on. See OSG Build Tools for full details on osg-promote .","title":"Using osg-promote"},{"location":"software/koji-workflow/#further-reading","text":"Official Koji documentation: https://docs.pagure.org/koji/ Fedora's koji documentation: https://fedoraproject.org/wiki/Koji Fedora's \"Using Koji\" page: https://docs.fedoraproject.org/en-US/package-maintainers/Using_the_Koji_Build_System/ Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.","title":"Further reading"},{"location":"software/new-team-member/","text":"Setup Instructions for New Team Members \u00b6 ssh access to a UW CompSci account, including AFS access Ask CHTC infrastructure Read/write access to the UW Subversion repository; Send email to Mat or Brian L after having obtained UW CompSci account User certificate Follow instructions here Import the certificate into your browser of choice Register for a GGUS account with the following information: Your certificate's subject DN Select none from the \"Virtual Organization\" drop-down Select yes for \"Do you want to have support access?\" and answer \"Why?\" with the following: Yes, I need to comment on tickets as a member of the OSG Software & Release Team (https://www.opensciencegrid.org/technology/#the-team) Access to Koji After obtaining certificate, follow the instructions on the Koji User Management doc Sign up for mailing lists software-discuss@osg-htc.org technology-team@osg-htc.org osg-commits@cs.wisc.edu GitHub team membership https://github.com/orgs/opensciencegrid/teams/software-and-release/members If > 50% S&R, add them to the triage schedule","title":"New Team Member"},{"location":"software/new-team-member/#setup-instructions-for-new-team-members","text":"ssh access to a UW CompSci account, including AFS access Ask CHTC infrastructure Read/write access to the UW Subversion repository; Send email to Mat or Brian L after having obtained UW CompSci account User certificate Follow instructions here Import the certificate into your browser of choice Register for a GGUS account with the following information: Your certificate's subject DN Select none from the \"Virtual Organization\" drop-down Select yes for \"Do you want to have support access?\" and answer \"Why?\" with the following: Yes, I need to comment on tickets as a member of the OSG Software & Release Team (https://www.opensciencegrid.org/technology/#the-team) Access to Koji After obtaining certificate, follow the instructions on the Koji User Management doc Sign up for mailing lists software-discuss@osg-htc.org technology-team@osg-htc.org osg-commits@cs.wisc.edu GitHub team membership https://github.com/orgs/opensciencegrid/teams/software-and-release/members If > 50% S&R, add them to the triage schedule","title":"Setup Instructions for New Team Members"},{"location":"software/osg-build-tools/","text":"OSG Build Tools \u00b6 This page documents the tools used for RPM development for the OSG Software Stack. See the RPM development guide for the principles on which these tools are based. The tools are available in Git in osg-htc/osg-build on GitHub . First, acquire the tools via one of the following methods: Apptainer/Singularity Docker/Podman Local install Rootly install with Make Install via Pip Quick start with Apptainer \u00b6 This quick start guide shows how to use the OSG build tools via Apptainer. (Singularity should work too.) This assumes you will use Kerberos for authentication with UW-Madison or Fermilab credentials. To get UW-Madison Kerberos credentials, you will need to be inside the UW Campus network, either via campus wifi or VPN. In addition, this assumes that the repository you want to build packages from is from a subdirectory under your home directory. If not, pass the appropriate --bind ( -B ) argument to the apptainer run command. The image is available from OSG Harbor . Pull the image and run it: apptainer pull oras://hub.osg-htc.org/osg-htc/osg-build:v2-sif osg-build.sif apptainer run osg-build.sif you will be in a shell inside the image. Set up your configuration for accessing the OSG Koji environment. osg-koji setup Select Kerberos as your auth method. Use kinit to get a credential. For UW-Madison: kinit <username>@AD.WISC.EDU replacing <username> with the name of your UW NetID. For Fermilab: kinit <username>@FNAL.GOV replacing <username> with your Fermilab user name. Verify that you can successfully authenticate to Koji: osg-koji hello If that succeeds, you will be ready to use the rest of the osg-build and osg-koji commands. Note: the mock command does not work in Apptainer/Singularity due to permissions issues. For testing builds, use osg-build koji --scratch instead. Quick start with Docker \u00b6 Note The Docker image and helper scripts are a work in progress. This quick start guide shows how to use the OSG build tools via Docker. (Podman should work too.) This assumes you will use Kerberos for authentication with UW-Madison or Fermilab credentials. To get UW-Madison Kerberos credentials, you will need to be inside the UW Campus network, either via campus wifi or VPN. First, pull the GitHub repo hosting the image and helper scripts. git clone https://github.com/osg-htc/docker-osg-build Next, run the initbuilder script, giving it the directory under which the repo containing the software packaging is. For example, if you have your checkout of the software-packaging repo under ~/software-packaging , run cd docker-osg-build ./initbuilder ~/software-packaging Answer the configuration questions from initbuilder. From then on, use the osg-build-shell command to enter into a shell inside the image. Use kinit to get a credential. For UW-Madison: kinit <username>@AD.WISC.EDU replacing <username> with the name of your UW NetID. For Fermilab: kinit <username>@FNAL.GOV replacing <username> with your Fermilab user name. Verify that you can successfully authenticate to Koji: osg-koji hello If that succeeds, you will be ready to use the rest of the osg-build and osg-koji commands. Installation into a Linux system \u00b6 You may install the software locally, either via pip or via a script distributed by an RPM. If using pip , you will need to install some dependencies by hand. Install as root via OSG RPM (EL8, EL9, EL10) \u00b6 The OSG 24-internal and 25-internal repositories contain a package called \"osg-build-deps\". Install the OSG 25 repositories . Then, install osg-build-deps: yum install --enablerepo=osg-internal-development osg-build-deps This will pull in the dependencies necessary to run the OSG Build Tools. Then, to install the OSG Build Tools themselves into /usr/local , run install-osg-build.sh This will clone osg-build into /usr/local/src/osg-build and install the software under the /usr/local tree. Install via pip (EL8, EL9, EL10, Ubuntu, others) \u00b6 To use Kerberos authentication you will need the client tools to run kinit . On Enterprise Linux variants and Feodra, run: yum install krb5-workstation On Ubuntu, run: apt install krb5-user Before installing via pip, you will need to install the requests-gssapi library by hand because it contains compiled binaries. On EL8/EL9/EL10, run yum install python3-requests-gssapi on Ubuntu 24.04, run apt install python3-requests-gssapi If requests-gssapi is not available, you will have to compile it by hand; see below. To install the OSG Build Tools themselves, run: pip install --user git+https://github.com/osg-htc/osg-build@V2-branch Note If you installed requests-gssapi via RPM/Deb, and you want to run osg-build in a virtualenv or via pipx, pass --system-site-packages when creating the virtualenv or running pipx install . Building requests-gssapi from source \u00b6 If the requests-gssapi library is not available or you don't want to install it via a package manager, you must install the dependencies that will allow pip to build it from source. On EL8/EL9/EL10 run: yum install gcc make python3-devel krb5-devel On Ubuntu run apt install gcc make libpython-dev libkrb5-dev Afterwards, pip install the OSG Build Tools as previously: pip install --user git+https://github.com/osg-htc/osg-build@V2-branch Configuring Koji access and OSG Build \u00b6 Access to Koji is performed using Kerberos. Set up your configuration for accessing the OSG Koji environment. osg-koji setup You will be prompted for your authentication mehtod; select Kerberos and enter your Kerberos principal at the prompt. Use kinit to get a credential. For UW-Madison: kinit <username>@AD.WISC.EDU replacing <username> with the name of your UW NetID. For Fermilab: kinit <username>@FNAL.GOV replacing <username> with your Fermilab user name. Verify that you can successfully authenticate to Koji: osg-koji hello If authentication is successful, you are ready to perform Koji build tasks. Commonly used tools \u00b6 These are the tools you will be using for day-to-day builds. osg-build \u00b6 Overview \u00b6 This is the primary tool used in building source and binary RPMs. osg-build <TASK> [options] <PACKAGE DIRECTORY> [...] package_directory is a directory containing an osg/ and/or an upstream/ subdirectory. See the RPM development guide for how these directories are organized. Tasks \u00b6 koji \u00b6 Builds the package on the Koji build service hosted at UW-Madison. https://koji.osg-htc.org . lint \u00b6 Runs rpmlint the package(s) to check for various problems. You will need to have rpmlint installed. mock \u00b6 Prebuilds the final source package, then builds it locally using mock , and stores the resulting source and binary RPMs in the package-specific _build_results directory. prebuild \u00b6 Prebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the _upstream_srpm_contents and _final_srpm_contents directories. prepare \u00b6 Prebuilds the final source package, then calls rpmbuild -bp on the result, extracting and patching the source files (and performing any other steps defined in the %prep section of the spec file. The resulting sources will be under _build_results/BUILD . rpmbuild \u00b6 Prebuilds the final source package, then builds it locally using rpmbuild , and stores the resulting source and binary RPMs in the package-specific _build_results directory. quilt \u00b6 Collects the upstream local sources and spec file, then calls quilt setup on the spec file, extracting the source files and adding the patches to a quilt series file. See Quilt documentation (PDF link) for more information on quilt; also look at the example in the Usage Patterns section below. The sources are unpacked into _quilt ; they will be in pre-patch state and the various quilt commands can be used to apply and modify patches. Requires quilt . Options \u00b6 This section lists the common command-line options. --help \u00b6 Prints the built-in usage information and exits without doing anything else. --version \u00b6 Prints the version of osg-build and exits without doing anything else. Options common to all tasks \u00b6 --el8, --el9, etc. \u00b6 Build for these specific distro versions. -q, --quiet, -v, --verbose \u00b6 Decrease or increase the amount of information printed. Options specific to prebuild task \u00b6 --full-extract \u00b6 If set, all upstream tarballs will be extracted into _upstream_tarball_contents/ during the prebuild step. This flag is now mostly redundant with the prepare and quilt tasks. Options specific to rpmbuild and mock tasks \u00b6 --distro-tag dist \u00b6 Sets the %dist tag added on to the end of the release in the RPM ( rpmbuild and mock tasks only ). -t, --target-arch arch \u00b6 Specify an architecture to build packages for ( rpmbuild , mock , and scratch koji builds only ). Options specific to mock task \u00b6 --mock-clean, --no-mock-clean \u00b6 Enable/disable deletion of the mock buildroot after a successful build. Default is true . -m, --mock-config path \u00b6 Specifies the mock configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag %dist ). See also --mock-config-from-koji . --mock-config-from-koji build tag \u00b6 Creates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is: osg-3.4-el6-build Also requires the Koji command-line tools (package koji ), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely: --koji-backend , --koji-login , and --koji-wrapper . Options specific to koji task \u00b6 --dry-run \u00b6 Do not actually run koji, merely show the command(s) that will be run. For debugging purposes. --getfiles, --get-files \u00b6 For scratch builds without --vcs only. Download the resulting RPMs and logs from the build into the _build_results directory. --koji-target target \u00b6 The koji target to use for building. --koji-tag tag \u00b6 The koji tag to add packages to. See the Koji Workflow guide for more information on the terminology. The special value TARGET uses the destination tag defined in the koji target. Default is osg-el6 or osg-el7 . --ktt, --koji-tag-and-target arg \u00b6 Shorthand for setting both --koji-tag and --koji-target to arg . --regen-repos \u00b6 Start a regen-repo koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with --no-wait , since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete. --scratch, --no-scratch \u00b6 Perform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient. --vcs, --git, --svn \u00b6 Have Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. This is always true for non-scratch builds. If this flag is specified, you may use Git URLs to specify the packages to builds. A Git URL looks like git+REPO?DIRECTORY#BRANCH , e.g. git+https://github.com/osg-htc/software-packaging.git?24-main/xrootd#main` (the #main at the end, specifying the main branch, is optional). You can also use SVN URLs to build from the old SVN repository, such as svn+https://vdt.cs.wisc.edu/svn/native/redhat/branches/24-main/xrootd#28000 (the #28000 at the end, specifying commit 28000, is optional). --repo= destination repository \u00b6 Selects the repository (24-main, 24-upcoming, etc.) to build packages for. See --repo-list for a list of repositories. --repo-list \u00b6 Lists the available repositories for the --repo argument. osg-koji \u00b6 This is a wrapper script around the koji command line tool. It automatically specifies parameters to access the OSG's koji instance. An additional command, osg-koji setup exists, which sets up koji configuration in ~/.osg-koji . osg-promote \u00b6 Overview \u00b6 Run this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines. Once the packages are promoted, the script will generate code to cut and paste into a JIRA comment. Synopsis \u00b6 osg-promote [-r|--route <ROUTE>]... [options] <PACKAGE OR BUILD> [...] Examples \u00b6 Promote the latest build of osg-version to testing for the current release series osg-promote -r testing osg-version Promote the latest builds of osg-ce to testing for the 3.3 and 3.4 release series osg-promote -r 3.3-testing -r 3.4-testing osg-ce Promote osg-build-1.5.0-1 to testing for the current release series osg-promote -r testing osg-build-1.5.0-1 Arguments \u00b6 -h \u00b6 Display help and a list of valid routes. package or build \u00b6 A package (e.g. osg-version ) or build (e.g. osg-version-3.3.0-1.osg33.el6 ) to promote. You may omit the dist tag (the .osg33.el6 part). If a package is specified, the most recent version of that package will be promoted. If a build is specified, that build and the build that has the same version - release for the other distro version(s) will be promoted. That is, if you specify the route 3.3-testing and the build foo-1-1 , then foo-1-1.osg33.el6 and foo-1-1.osg33.el7 will be promoted. This may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package must match. -r ROUTE , --route ROUTE \u00b6 The promotion route to use. Use osg-promote -h to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass -r 3.4-testing -r 3.3-testing . If not specified, the testing route is used, which corresponds to the testing route for the latest release series. -n, --dry-run \u00b6 Do not promote, just show what would be done. --el6-only / --el7-only \u00b6 Only promote packages for el6 / el7. --no-el6 / --no-el7 \u00b6 Do not promote packages for el6 / el7. --ignore-rejects \u00b6 Ignore rejections due to version mismatch between dvers or missing package for one dver. --regen \u00b6 Regenerate the destination repos after promoting. -y, --assume-yes \u00b6 Do not prompt before promotion. Minor tools \u00b6 These tools are used less frequently or are for specialized purposes. koji-tag-diff \u00b6 This script displays the differences between the latest packages in two koji tags. Example invocation: koji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing This prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa. osg-build-test \u00b6 This script runs automated tests for osg-build . Only a few tests have been implemented so far. osg-import-srpm \u00b6 This is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an upstream/*.source file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass --upstream-root=<UPSTREAM DIR> to put them somewhere else. If called with the --extract-spec or -e argument, it will extract the spec file from the SRPM and place it into the osg subdir in SVN. If called with the --diff-spec or -d argument, it will extract the spec file and compare it to the existing spec file in the osg subdir. The script hasn't been touched in a while and needs a good deal of cleanup. A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications. Common Usage Patterns \u00b6 Verify that all files necessary to build the package are in the right place \u00b6 Run osg-build prebuild <PACKAGEDIR> . Fetch and extract all source files for examination \u00b6 Run osg-build prebuild --full-extract <PACKAGEDIR> . Look inside the _upstream_tarball_contents directory. Get a post-patch version of the upstream sources for examination \u00b6 Run osg-build prepare <PACKAGEDIR> . Look inside the _build_results directory. See which patches work with a new version of a package, update or remove them \u00b6 Place the new source tarball into the upstream cache, edit the version in the spec file and *.source files as necessary Run osg-build quilt <PACKAGEDIR> Enter the extracted sources inside the _quilt directory. You should see a file called series and a symlink called patches Run quilt series to get a list of patches in order of application Run quilt push to apply the next patch; If the patch applies cleanly, continue. If the patch applies with some fuzz, run quilt refresh to update the offsets in the patch. If the patch does not apply and you wish to remove it, run quilt delete <PATCH NAME> (delete only removes it from the series file, not the disk) If the patch does not apply and you wish to fix it, your options are: 1. Force-apply the patch by running quilt push -f . Then, manually examine the rejected changes (*.rej files) and make the changes to the corresponding original files. Then, run quilt refresh to edit the patch and incorporate your changes. 2. Alternatively, delete the patch from the series file by running quilt delete <PATCH NAME> . Then, use quilt new , quilt edit , and quilt refresh and make a new patch. Consult the quilt(1) manpage for more info. If you have a new patch, run quilt import <PATCHFILE> to add the patch to the series file, and run quilt push to apply it. If you have changes to make to the source code that you want to save as a patch, run quilt new <PATCHNAME> , edit the files, run quilt add <FILE> on each file you edited, then run quilt refresh to recreate the patch. Once you're all done, copy the patches in the patches/ directory to the osg/ dir in the packaging repository, run quilt series to get the application order and update the spec file accordingly. See if a package builds successfully for OSG 24 main \u00b6 Run osg-build koji --scratch <PACKAGEDIR> --repo 24-main . You may download the resulting RPMs from kojiweb https://koji.osg-htc.org/koji or pass --getfiles to osg-build koji and they will get downloaded to the _build_results directory. Check for potential errors in a package \u00b6 Run osg-build lint <PACKAGEDIR> . Promote the latest build of a package to testing for the OSG 24 release series \u00b6 Run osg-promote -r 24-main <PACKAGE> Promote the latest build of a package to testing for both the OSG 24 and 25 release series \u00b6 Run osg-promote -r 24-main -r 25-main <PACKAGE>","title":"OSG Build Tools"},{"location":"software/osg-build-tools/#osg-build-tools","text":"This page documents the tools used for RPM development for the OSG Software Stack. See the RPM development guide for the principles on which these tools are based. The tools are available in Git in osg-htc/osg-build on GitHub . First, acquire the tools via one of the following methods: Apptainer/Singularity Docker/Podman Local install Rootly install with Make Install via Pip","title":"OSG Build Tools"},{"location":"software/osg-build-tools/#quick-start-with-apptainer","text":"This quick start guide shows how to use the OSG build tools via Apptainer. (Singularity should work too.) This assumes you will use Kerberos for authentication with UW-Madison or Fermilab credentials. To get UW-Madison Kerberos credentials, you will need to be inside the UW Campus network, either via campus wifi or VPN. In addition, this assumes that the repository you want to build packages from is from a subdirectory under your home directory. If not, pass the appropriate --bind ( -B ) argument to the apptainer run command. The image is available from OSG Harbor . Pull the image and run it: apptainer pull oras://hub.osg-htc.org/osg-htc/osg-build:v2-sif osg-build.sif apptainer run osg-build.sif you will be in a shell inside the image. Set up your configuration for accessing the OSG Koji environment. osg-koji setup Select Kerberos as your auth method. Use kinit to get a credential. For UW-Madison: kinit <username>@AD.WISC.EDU replacing <username> with the name of your UW NetID. For Fermilab: kinit <username>@FNAL.GOV replacing <username> with your Fermilab user name. Verify that you can successfully authenticate to Koji: osg-koji hello If that succeeds, you will be ready to use the rest of the osg-build and osg-koji commands. Note: the mock command does not work in Apptainer/Singularity due to permissions issues. For testing builds, use osg-build koji --scratch instead.","title":"Quick start with Apptainer"},{"location":"software/osg-build-tools/#quick-start-with-docker","text":"Note The Docker image and helper scripts are a work in progress. This quick start guide shows how to use the OSG build tools via Docker. (Podman should work too.) This assumes you will use Kerberos for authentication with UW-Madison or Fermilab credentials. To get UW-Madison Kerberos credentials, you will need to be inside the UW Campus network, either via campus wifi or VPN. First, pull the GitHub repo hosting the image and helper scripts. git clone https://github.com/osg-htc/docker-osg-build Next, run the initbuilder script, giving it the directory under which the repo containing the software packaging is. For example, if you have your checkout of the software-packaging repo under ~/software-packaging , run cd docker-osg-build ./initbuilder ~/software-packaging Answer the configuration questions from initbuilder. From then on, use the osg-build-shell command to enter into a shell inside the image. Use kinit to get a credential. For UW-Madison: kinit <username>@AD.WISC.EDU replacing <username> with the name of your UW NetID. For Fermilab: kinit <username>@FNAL.GOV replacing <username> with your Fermilab user name. Verify that you can successfully authenticate to Koji: osg-koji hello If that succeeds, you will be ready to use the rest of the osg-build and osg-koji commands.","title":"Quick start with Docker"},{"location":"software/osg-build-tools/#installation-into-a-linux-system","text":"You may install the software locally, either via pip or via a script distributed by an RPM. If using pip , you will need to install some dependencies by hand.","title":"Installation into a Linux system"},{"location":"software/osg-build-tools/#install-as-root-via-osg-rpm-el8-el9-el10","text":"The OSG 24-internal and 25-internal repositories contain a package called \"osg-build-deps\". Install the OSG 25 repositories . Then, install osg-build-deps: yum install --enablerepo=osg-internal-development osg-build-deps This will pull in the dependencies necessary to run the OSG Build Tools. Then, to install the OSG Build Tools themselves into /usr/local , run install-osg-build.sh This will clone osg-build into /usr/local/src/osg-build and install the software under the /usr/local tree.","title":"Install as root via OSG RPM (EL8, EL9, EL10)"},{"location":"software/osg-build-tools/#install-via-pip-el8-el9-el10-ubuntu-others","text":"To use Kerberos authentication you will need the client tools to run kinit . On Enterprise Linux variants and Feodra, run: yum install krb5-workstation On Ubuntu, run: apt install krb5-user Before installing via pip, you will need to install the requests-gssapi library by hand because it contains compiled binaries. On EL8/EL9/EL10, run yum install python3-requests-gssapi on Ubuntu 24.04, run apt install python3-requests-gssapi If requests-gssapi is not available, you will have to compile it by hand; see below. To install the OSG Build Tools themselves, run: pip install --user git+https://github.com/osg-htc/osg-build@V2-branch Note If you installed requests-gssapi via RPM/Deb, and you want to run osg-build in a virtualenv or via pipx, pass --system-site-packages when creating the virtualenv or running pipx install .","title":"Install via pip (EL8, EL9, EL10, Ubuntu, others)"},{"location":"software/osg-build-tools/#building-requests-gssapi-from-source","text":"If the requests-gssapi library is not available or you don't want to install it via a package manager, you must install the dependencies that will allow pip to build it from source. On EL8/EL9/EL10 run: yum install gcc make python3-devel krb5-devel On Ubuntu run apt install gcc make libpython-dev libkrb5-dev Afterwards, pip install the OSG Build Tools as previously: pip install --user git+https://github.com/osg-htc/osg-build@V2-branch","title":"Building requests-gssapi from source"},{"location":"software/osg-build-tools/#configuring-koji-access-and-osg-build","text":"Access to Koji is performed using Kerberos. Set up your configuration for accessing the OSG Koji environment. osg-koji setup You will be prompted for your authentication mehtod; select Kerberos and enter your Kerberos principal at the prompt. Use kinit to get a credential. For UW-Madison: kinit <username>@AD.WISC.EDU replacing <username> with the name of your UW NetID. For Fermilab: kinit <username>@FNAL.GOV replacing <username> with your Fermilab user name. Verify that you can successfully authenticate to Koji: osg-koji hello If authentication is successful, you are ready to perform Koji build tasks.","title":"Configuring Koji access and OSG Build"},{"location":"software/osg-build-tools/#commonly-used-tools","text":"These are the tools you will be using for day-to-day builds.","title":"Commonly used tools"},{"location":"software/osg-build-tools/#osg-build","text":"","title":"osg-build"},{"location":"software/osg-build-tools/#overview","text":"This is the primary tool used in building source and binary RPMs. osg-build <TASK> [options] <PACKAGE DIRECTORY> [...] package_directory is a directory containing an osg/ and/or an upstream/ subdirectory. See the RPM development guide for how these directories are organized.","title":"Overview"},{"location":"software/osg-build-tools/#tasks","text":"","title":"Tasks"},{"location":"software/osg-build-tools/#koji","text":"Builds the package on the Koji build service hosted at UW-Madison. https://koji.osg-htc.org .","title":"koji"},{"location":"software/osg-build-tools/#lint","text":"Runs rpmlint the package(s) to check for various problems. You will need to have rpmlint installed.","title":"lint"},{"location":"software/osg-build-tools/#mock","text":"Prebuilds the final source package, then builds it locally using mock , and stores the resulting source and binary RPMs in the package-specific _build_results directory.","title":"mock"},{"location":"software/osg-build-tools/#prebuild","text":"Prebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the _upstream_srpm_contents and _final_srpm_contents directories.","title":"prebuild"},{"location":"software/osg-build-tools/#prepare","text":"Prebuilds the final source package, then calls rpmbuild -bp on the result, extracting and patching the source files (and performing any other steps defined in the %prep section of the spec file. The resulting sources will be under _build_results/BUILD .","title":"prepare"},{"location":"software/osg-build-tools/#rpmbuild","text":"Prebuilds the final source package, then builds it locally using rpmbuild , and stores the resulting source and binary RPMs in the package-specific _build_results directory.","title":"rpmbuild"},{"location":"software/osg-build-tools/#quilt","text":"Collects the upstream local sources and spec file, then calls quilt setup on the spec file, extracting the source files and adding the patches to a quilt series file. See Quilt documentation (PDF link) for more information on quilt; also look at the example in the Usage Patterns section below. The sources are unpacked into _quilt ; they will be in pre-patch state and the various quilt commands can be used to apply and modify patches. Requires quilt .","title":"quilt"},{"location":"software/osg-build-tools/#options","text":"This section lists the common command-line options.","title":"Options"},{"location":"software/osg-build-tools/#-help","text":"Prints the built-in usage information and exits without doing anything else.","title":"--help"},{"location":"software/osg-build-tools/#-version","text":"Prints the version of osg-build and exits without doing anything else.","title":"--version"},{"location":"software/osg-build-tools/#options-common-to-all-tasks","text":"","title":"Options common to all tasks"},{"location":"software/osg-build-tools/#-el8-el9-etc","text":"Build for these specific distro versions.","title":"--el8, --el9, etc."},{"location":"software/osg-build-tools/#-q-quiet-v-verbose","text":"Decrease or increase the amount of information printed.","title":"-q, --quiet, -v, --verbose"},{"location":"software/osg-build-tools/#options-specific-to-prebuild-task","text":"","title":"Options specific to prebuild task"},{"location":"software/osg-build-tools/#-full-extract","text":"If set, all upstream tarballs will be extracted into _upstream_tarball_contents/ during the prebuild step. This flag is now mostly redundant with the prepare and quilt tasks.","title":"--full-extract"},{"location":"software/osg-build-tools/#options-specific-to-rpmbuild-and-mock-tasks","text":"","title":"Options specific to rpmbuild and mock tasks"},{"location":"software/osg-build-tools/#-distro-tag-dist","text":"Sets the %dist tag added on to the end of the release in the RPM ( rpmbuild and mock tasks only ).","title":"--distro-tag dist"},{"location":"software/osg-build-tools/#-t-target-arch-arch","text":"Specify an architecture to build packages for ( rpmbuild , mock , and scratch koji builds only ).","title":"-t, --target-arch arch"},{"location":"software/osg-build-tools/#options-specific-to-mock-task","text":"","title":"Options specific to mock task"},{"location":"software/osg-build-tools/#-mock-clean-no-mock-clean","text":"Enable/disable deletion of the mock buildroot after a successful build. Default is true .","title":"--mock-clean, --no-mock-clean"},{"location":"software/osg-build-tools/#-m-mock-config-path","text":"Specifies the mock configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag %dist ). See also --mock-config-from-koji .","title":"-m, --mock-config path"},{"location":"software/osg-build-tools/#-mock-config-from-koji-build-tag","text":"Creates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is: osg-3.4-el6-build Also requires the Koji command-line tools (package koji ), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely: --koji-backend , --koji-login , and --koji-wrapper .","title":"--mock-config-from-koji build tag"},{"location":"software/osg-build-tools/#options-specific-to-koji-task","text":"","title":"Options specific to koji task"},{"location":"software/osg-build-tools/#-dry-run","text":"Do not actually run koji, merely show the command(s) that will be run. For debugging purposes.","title":"--dry-run"},{"location":"software/osg-build-tools/#-getfiles-get-files","text":"For scratch builds without --vcs only. Download the resulting RPMs and logs from the build into the _build_results directory.","title":"--getfiles, --get-files"},{"location":"software/osg-build-tools/#-koji-target-target","text":"The koji target to use for building.","title":"--koji-target target"},{"location":"software/osg-build-tools/#-koji-tag-tag","text":"The koji tag to add packages to. See the Koji Workflow guide for more information on the terminology. The special value TARGET uses the destination tag defined in the koji target. Default is osg-el6 or osg-el7 .","title":"--koji-tag tag"},{"location":"software/osg-build-tools/#-ktt-koji-tag-and-target-arg","text":"Shorthand for setting both --koji-tag and --koji-target to arg .","title":"--ktt, --koji-tag-and-target arg"},{"location":"software/osg-build-tools/#-regen-repos","text":"Start a regen-repo koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with --no-wait , since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete.","title":"--regen-repos"},{"location":"software/osg-build-tools/#-scratch-no-scratch","text":"Perform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient.","title":"--scratch, --no-scratch"},{"location":"software/osg-build-tools/#-vcs-git-svn","text":"Have Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. This is always true for non-scratch builds. If this flag is specified, you may use Git URLs to specify the packages to builds. A Git URL looks like git+REPO?DIRECTORY#BRANCH , e.g. git+https://github.com/osg-htc/software-packaging.git?24-main/xrootd#main` (the #main at the end, specifying the main branch, is optional). You can also use SVN URLs to build from the old SVN repository, such as svn+https://vdt.cs.wisc.edu/svn/native/redhat/branches/24-main/xrootd#28000 (the #28000 at the end, specifying commit 28000, is optional).","title":"--vcs, --git, --svn"},{"location":"software/osg-build-tools/#-repodestination-repository","text":"Selects the repository (24-main, 24-upcoming, etc.) to build packages for. See --repo-list for a list of repositories.","title":"--repo=destination repository"},{"location":"software/osg-build-tools/#-repo-list","text":"Lists the available repositories for the --repo argument.","title":"--repo-list"},{"location":"software/osg-build-tools/#osg-koji","text":"This is a wrapper script around the koji command line tool. It automatically specifies parameters to access the OSG's koji instance. An additional command, osg-koji setup exists, which sets up koji configuration in ~/.osg-koji .","title":"osg-koji"},{"location":"software/osg-build-tools/#osg-promote","text":"","title":"osg-promote"},{"location":"software/osg-build-tools/#overview_1","text":"Run this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines. Once the packages are promoted, the script will generate code to cut and paste into a JIRA comment.","title":"Overview"},{"location":"software/osg-build-tools/#synopsis","text":"osg-promote [-r|--route <ROUTE>]... [options] <PACKAGE OR BUILD> [...]","title":"Synopsis"},{"location":"software/osg-build-tools/#examples","text":"Promote the latest build of osg-version to testing for the current release series osg-promote -r testing osg-version Promote the latest builds of osg-ce to testing for the 3.3 and 3.4 release series osg-promote -r 3.3-testing -r 3.4-testing osg-ce Promote osg-build-1.5.0-1 to testing for the current release series osg-promote -r testing osg-build-1.5.0-1","title":"Examples"},{"location":"software/osg-build-tools/#arguments","text":"","title":"Arguments"},{"location":"software/osg-build-tools/#-h","text":"Display help and a list of valid routes.","title":"-h"},{"location":"software/osg-build-tools/#package-or-build","text":"A package (e.g. osg-version ) or build (e.g. osg-version-3.3.0-1.osg33.el6 ) to promote. You may omit the dist tag (the .osg33.el6 part). If a package is specified, the most recent version of that package will be promoted. If a build is specified, that build and the build that has the same version - release for the other distro version(s) will be promoted. That is, if you specify the route 3.3-testing and the build foo-1-1 , then foo-1-1.osg33.el6 and foo-1-1.osg33.el7 will be promoted. This may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package must match.","title":"package or build"},{"location":"software/osg-build-tools/#-r-route-route-route","text":"The promotion route to use. Use osg-promote -h to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass -r 3.4-testing -r 3.3-testing . If not specified, the testing route is used, which corresponds to the testing route for the latest release series.","title":"-r ROUTE, --route ROUTE"},{"location":"software/osg-build-tools/#-n-dry-run","text":"Do not promote, just show what would be done.","title":"-n, --dry-run"},{"location":"software/osg-build-tools/#-el6-only-el7-only","text":"Only promote packages for el6 / el7.","title":"--el6-only / --el7-only"},{"location":"software/osg-build-tools/#-no-el6-no-el7","text":"Do not promote packages for el6 / el7.","title":"--no-el6 / --no-el7"},{"location":"software/osg-build-tools/#-ignore-rejects","text":"Ignore rejections due to version mismatch between dvers or missing package for one dver.","title":"--ignore-rejects"},{"location":"software/osg-build-tools/#-regen","text":"Regenerate the destination repos after promoting.","title":"--regen"},{"location":"software/osg-build-tools/#-y-assume-yes","text":"Do not prompt before promotion.","title":"-y, --assume-yes"},{"location":"software/osg-build-tools/#minor-tools","text":"These tools are used less frequently or are for specialized purposes.","title":"Minor tools"},{"location":"software/osg-build-tools/#koji-tag-diff","text":"This script displays the differences between the latest packages in two koji tags. Example invocation: koji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing This prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa.","title":"koji-tag-diff"},{"location":"software/osg-build-tools/#osg-build-test","text":"This script runs automated tests for osg-build . Only a few tests have been implemented so far.","title":"osg-build-test"},{"location":"software/osg-build-tools/#osg-import-srpm","text":"This is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an upstream/*.source file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass --upstream-root=<UPSTREAM DIR> to put them somewhere else. If called with the --extract-spec or -e argument, it will extract the spec file from the SRPM and place it into the osg subdir in SVN. If called with the --diff-spec or -d argument, it will extract the spec file and compare it to the existing spec file in the osg subdir. The script hasn't been touched in a while and needs a good deal of cleanup. A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications.","title":"osg-import-srpm"},{"location":"software/osg-build-tools/#common-usage-patterns","text":"","title":"Common Usage Patterns"},{"location":"software/osg-build-tools/#verify-that-all-files-necessary-to-build-the-package-are-in-the-right-place","text":"Run osg-build prebuild <PACKAGEDIR> .","title":"Verify that all files necessary to build the package are in the right place"},{"location":"software/osg-build-tools/#fetch-and-extract-all-source-files-for-examination","text":"Run osg-build prebuild --full-extract <PACKAGEDIR> . Look inside the _upstream_tarball_contents directory.","title":"Fetch and extract all source files for examination"},{"location":"software/osg-build-tools/#get-a-post-patch-version-of-the-upstream-sources-for-examination","text":"Run osg-build prepare <PACKAGEDIR> . Look inside the _build_results directory.","title":"Get a post-patch version of the upstream sources for examination"},{"location":"software/osg-build-tools/#see-which-patches-work-with-a-new-version-of-a-package-update-or-remove-them","text":"Place the new source tarball into the upstream cache, edit the version in the spec file and *.source files as necessary Run osg-build quilt <PACKAGEDIR> Enter the extracted sources inside the _quilt directory. You should see a file called series and a symlink called patches Run quilt series to get a list of patches in order of application Run quilt push to apply the next patch; If the patch applies cleanly, continue. If the patch applies with some fuzz, run quilt refresh to update the offsets in the patch. If the patch does not apply and you wish to remove it, run quilt delete <PATCH NAME> (delete only removes it from the series file, not the disk) If the patch does not apply and you wish to fix it, your options are: 1. Force-apply the patch by running quilt push -f . Then, manually examine the rejected changes (*.rej files) and make the changes to the corresponding original files. Then, run quilt refresh to edit the patch and incorporate your changes. 2. Alternatively, delete the patch from the series file by running quilt delete <PATCH NAME> . Then, use quilt new , quilt edit , and quilt refresh and make a new patch. Consult the quilt(1) manpage for more info. If you have a new patch, run quilt import <PATCHFILE> to add the patch to the series file, and run quilt push to apply it. If you have changes to make to the source code that you want to save as a patch, run quilt new <PATCHNAME> , edit the files, run quilt add <FILE> on each file you edited, then run quilt refresh to recreate the patch. Once you're all done, copy the patches in the patches/ directory to the osg/ dir in the packaging repository, run quilt series to get the application order and update the spec file accordingly.","title":"See which patches work with a new version of a package, update or remove them"},{"location":"software/osg-build-tools/#see-if-a-package-builds-successfully-for-osg-24-main","text":"Run osg-build koji --scratch <PACKAGEDIR> --repo 24-main . You may download the resulting RPMs from kojiweb https://koji.osg-htc.org/koji or pass --getfiles to osg-build koji and they will get downloaded to the _build_results directory.","title":"See if a package builds successfully for OSG 24 main"},{"location":"software/osg-build-tools/#check-for-potential-errors-in-a-package","text":"Run osg-build lint <PACKAGEDIR> .","title":"Check for potential errors in a package"},{"location":"software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-the-osg-24-release-series","text":"Run osg-promote -r 24-main <PACKAGE>","title":"Promote the latest build of a package to testing for the OSG 24 release series"},{"location":"software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-both-the-osg-24-and-25-release-series","text":"Run osg-promote -r 24-main -r 25-main <PACKAGE>","title":"Promote the latest build of a package to testing for both the OSG 24 and 25 release series"},{"location":"software/ospool-containers/","text":"Containers in the OSPool \u00b6 Tim C. started this document in February 2023 to remember things from hallway discussions. If the scope of this page broadens, be sure to update the title and/or this description. Slot Ad Attributes for Containers \u00b6 There are two seemingly similar Slot Ad attributes in the OSPool related to Singularity: HasSingularity is an HTCondor attribute that indicates whether jobs can run within Singularity containers. It is set to true based on a test that HTCondor performs at start-up, although subsequent container invocations could revoke the value upon certain failure conditions. HAS_SINGULARITY is an OSG pool attribute that indicates whether user payload jobs can and will run within Singularity containers. Only Singularity container runtimes are supported. It is set to true based on a test that is run in the pilot scripts. Note that the test is also baked into the OSPool Backfill Containers. The pilot-script test checks more conditions than the HTCondor HasSingularity one. Periodic scripts in the pilot (i.e., STARTD_CRON ) retest some of these conditions; thus, HAS_SINGULARITY may start out true but become false later in the pilot\u2019s lifetime. When an OSPool job asks to run inside a container, requirements are set to check that both HasSingularity and HAS_SINGULARITY are true for any matching Slot Ad. Containers and Pilots \u00b6 Obviously, Backfill Container \u201cpilots\u201d run inside a container runtime, although the specific choice of container runtime technology is up to the site. Within a GlideinWMS pilot, the pilot scripts determine whether a functioning Singularity container is available, which could come from a local install on each Execution Point or from CVMFS. If detected, then HAS_SINGULARITY is set to true and all user payloads will be run in containers. (It is possible to override this behavior through site-specific pilot hackery.) For Backfill Containers, the container itself includes an installation of Apptainer (n\u00e9e Singularity) and it will always be used (in unprivileged mode) instead of any system or other installation. The user job is a vanilla job. Today, though, a pilot runs the \u201cuser job wrapper\u201d script, which is a replacement for the user\u2019s executable that does some stuff and then runs the user\u2019s executable. If the HAS_SINGULARITY attribute is true in the environment of the wrapper, then it runs the actual user payload job in Singularity (or Apptainer). PID Namespaces \u00b6 PID namespaces are a key technology that enables containers to isolate from each other. See, for example, this Ubuntu copy of the man page for pid_namespaces . The root user always has the ability to create PID namespaces, so a privileged container runtime (i.e., not unprivileged Singularity) can always do this. There is a special feature for user PID namespaces , which can be created by unprivileged processes. To work, there is a certain kernel setting that must be set, and then user PID namespaces are available to all. Today, when the GlideinWMS user job wrapper is about to start a user payload job in a Singularity container, the default is to pass a flag to Singularity to use user PID namespaces. However, if user PID namespaces are not available (say, due to the kernel setting), the pilot start-up scripts do not detect this condition automatically. So, there is a special envirnoment variable that can be set on the outer container (e.g., the Backfill Container) to disable this flag: SINGULARITY_DISABLE_PID_NAMESPACES=1 While set on the outer container, it affects only the user job wrapper and how it invokes the inner Singularity container. Mat says that HTCondor 10.2.2 has a feature to detect the lack of user PID namespaces and, in such a case, to avoid using them, but it is not clear that that feature will help anything as long as the GlideinWMS user job wrapper is being used. Containers and GPUs \u00b6 Most, but certainly not all, OSPool payload jobs that request GPUs also request to run within a container. This is probably due to GPU-using software stacks often being complex, and so users often turn to containers for their runtime environment, including ones that we and others pre-build. Note that the Backfill Container images themselves do not include the NVIDIA CUDA drivers (although, we are experimenting with that). Instead, the CUDA drivers that are installed on the host system are mounted inside the running Backfill Container. This scheme, which works at most OSPool sites, allows GPU payload jobs to work without specifically requesting a container, although see above for why many do so anyway.","title":"OSPool Containers"},{"location":"software/ospool-containers/#containers-in-the-ospool","text":"Tim C. started this document in February 2023 to remember things from hallway discussions. If the scope of this page broadens, be sure to update the title and/or this description.","title":"Containers in the OSPool"},{"location":"software/ospool-containers/#slot-ad-attributes-for-containers","text":"There are two seemingly similar Slot Ad attributes in the OSPool related to Singularity: HasSingularity is an HTCondor attribute that indicates whether jobs can run within Singularity containers. It is set to true based on a test that HTCondor performs at start-up, although subsequent container invocations could revoke the value upon certain failure conditions. HAS_SINGULARITY is an OSG pool attribute that indicates whether user payload jobs can and will run within Singularity containers. Only Singularity container runtimes are supported. It is set to true based on a test that is run in the pilot scripts. Note that the test is also baked into the OSPool Backfill Containers. The pilot-script test checks more conditions than the HTCondor HasSingularity one. Periodic scripts in the pilot (i.e., STARTD_CRON ) retest some of these conditions; thus, HAS_SINGULARITY may start out true but become false later in the pilot\u2019s lifetime. When an OSPool job asks to run inside a container, requirements are set to check that both HasSingularity and HAS_SINGULARITY are true for any matching Slot Ad.","title":"Slot Ad Attributes for Containers"},{"location":"software/ospool-containers/#containers-and-pilots","text":"Obviously, Backfill Container \u201cpilots\u201d run inside a container runtime, although the specific choice of container runtime technology is up to the site. Within a GlideinWMS pilot, the pilot scripts determine whether a functioning Singularity container is available, which could come from a local install on each Execution Point or from CVMFS. If detected, then HAS_SINGULARITY is set to true and all user payloads will be run in containers. (It is possible to override this behavior through site-specific pilot hackery.) For Backfill Containers, the container itself includes an installation of Apptainer (n\u00e9e Singularity) and it will always be used (in unprivileged mode) instead of any system or other installation. The user job is a vanilla job. Today, though, a pilot runs the \u201cuser job wrapper\u201d script, which is a replacement for the user\u2019s executable that does some stuff and then runs the user\u2019s executable. If the HAS_SINGULARITY attribute is true in the environment of the wrapper, then it runs the actual user payload job in Singularity (or Apptainer).","title":"Containers and Pilots"},{"location":"software/ospool-containers/#pid-namespaces","text":"PID namespaces are a key technology that enables containers to isolate from each other. See, for example, this Ubuntu copy of the man page for pid_namespaces . The root user always has the ability to create PID namespaces, so a privileged container runtime (i.e., not unprivileged Singularity) can always do this. There is a special feature for user PID namespaces , which can be created by unprivileged processes. To work, there is a certain kernel setting that must be set, and then user PID namespaces are available to all. Today, when the GlideinWMS user job wrapper is about to start a user payload job in a Singularity container, the default is to pass a flag to Singularity to use user PID namespaces. However, if user PID namespaces are not available (say, due to the kernel setting), the pilot start-up scripts do not detect this condition automatically. So, there is a special envirnoment variable that can be set on the outer container (e.g., the Backfill Container) to disable this flag: SINGULARITY_DISABLE_PID_NAMESPACES=1 While set on the outer container, it affects only the user job wrapper and how it invokes the inner Singularity container. Mat says that HTCondor 10.2.2 has a feature to detect the lack of user PID namespaces and, in such a case, to avoid using them, but it is not clear that that feature will help anything as long as the GlideinWMS user job wrapper is being used.","title":"PID Namespaces"},{"location":"software/ospool-containers/#containers-and-gpus","text":"Most, but certainly not all, OSPool payload jobs that request GPUs also request to run within a container. This is probably due to GPU-using software stacks often being complex, and so users often turn to containers for their runtime environment, including ones that we and others pre-build. Note that the Backfill Container images themselves do not include the NVIDIA CUDA drivers (although, we are experimenting with that). Instead, the CUDA drivers that are installed on the host system are mounted inside the running Backfill Container. This scheme, which works at most OSPool sites, allows GPU payload jobs to work without specifically requesting a container, although see above for why many do so anyway.","title":"Containers and GPUs"},{"location":"software/release-planning/","text":"Plans for Future Releases \u00b6 This informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see this page . Unless explicitly noted, bullet points refer to software in the release repo. This page is not meant to track minor bugfixes or updates -- rather, its focus should be new features. OSG 3.4 (May 2017) \u00b6 Package(s) Change in osg-release Notes BeStMan2 Drop Retirement policy edg-mkgridmap Drop SOFTWARE-2600 frontier-squid Modify Version 3 glexec Drop SOFTWARE-2620 GRAM Drop SOFTWARE-2530 GUMS Drop Retirement policy , SOFTWARE-2600 jglobus Drop SOFTWARE-2606 netlogger Drop osg-ce Modify Drop GridFTP , gums-client osg-info-services Drop osg-version Drop singularity Add voms-admin-server Drop Retirement policy Track OSG 3.4 updates through its JIRA epic . Support Policy for OSG 3.3 \u00b6 According to our release support policy and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018. Previous Releases \u00b6 12 November 2013 \u00b6 OSG 3.1 HTCondor-CE with PBS osg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change) OSG 3.2 Initial release HDFS 2.0.0 (already done in Upcoming) HTCondor 8.0.4 glideinWMS 3.2.0 osg-info-services (Note: ReSS will likely be retired around year-end) OSG 3.1 updates Upcoming HTCondor 8.1 with unified RPM BOSCO 10 December 2013 \u00b6 OSG 3.2 RSV-for-VOs Squid must be present on OSG-CE (??? what does this mean?)","title":"Release planning"},{"location":"software/release-planning/#plans-for-future-releases","text":"This informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see this page . Unless explicitly noted, bullet points refer to software in the release repo. This page is not meant to track minor bugfixes or updates -- rather, its focus should be new features.","title":"Plans for Future Releases"},{"location":"software/release-planning/#osg-34-may-2017","text":"Package(s) Change in osg-release Notes BeStMan2 Drop Retirement policy edg-mkgridmap Drop SOFTWARE-2600 frontier-squid Modify Version 3 glexec Drop SOFTWARE-2620 GRAM Drop SOFTWARE-2530 GUMS Drop Retirement policy , SOFTWARE-2600 jglobus Drop SOFTWARE-2606 netlogger Drop osg-ce Modify Drop GridFTP , gums-client osg-info-services Drop osg-version Drop singularity Add voms-admin-server Drop Retirement policy Track OSG 3.4 updates through its JIRA epic .","title":"OSG 3.4 (May 2017)"},{"location":"software/release-planning/#support-policy-for-osg-33","text":"According to our release support policy and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018.","title":"Support Policy for OSG 3.3"},{"location":"software/release-planning/#previous-releases","text":"","title":"Previous Releases"},{"location":"software/release-planning/#12-november-2013","text":"OSG 3.1 HTCondor-CE with PBS osg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change) OSG 3.2 Initial release HDFS 2.0.0 (already done in Upcoming) HTCondor 8.0.4 glideinWMS 3.2.0 osg-info-services (Note: ReSS will likely be retired around year-end) OSG 3.1 updates Upcoming HTCondor 8.1 with unified RPM BOSCO","title":"12 November 2013"},{"location":"software/release-planning/#10-december-2013","text":"OSG 3.2 RSV-for-VOs Squid must be present on OSG-CE (??? what does this mean?)","title":"10 December 2013"},{"location":"software/repository-management/","text":"Repository Management \u00b6 This document attempts to record everything there is to know about repository management for the OSG. Public repositories \u00b6 We host four public-facing repositories at repo.opensciencegrid.org : development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage. testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison. release : This repository contains software that we are willing to support and can be used by the general community. contrib : RPMs contributed from outside the OSG. These repos are updated by the mash script running on repo.opensciencegrid.org . Internal repositories \u00b6 In addition to the public repositories above, we host two repositories on koji.opensciencegrid.org . These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them. minefield : This repository is a copy of development above. prerelease : This repository is a staging area for software that is slated to be in the next release. These repos are updated by the kojira daemon running on koji.opensciencegrid.org . Build repositories \u00b6 Warning this section is out of date; the tags it references no longer exist. The koji task in osg-build uses the osg-3.4-el6-build/osg-3.4-el7-build repo, which is the union of the following repositories: Minefield a.k.a. osg-3.4-el6-development / osg-3.4-el7-development The osg-el6-internal / osg-el7-internal tag (containing build dependencies we do not want to make public) The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist) CentOS and EPEL Koji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies. The tarball creation scripts use the osg-3.4-el6-release-build / osg-3.4-el7-release-build repo, which is the union of the following repositories: Release a.k.a. osg-3.4-el6-release / osg-3.4-el7-release The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist ) CentOS and EPEL","title":"Repository management"},{"location":"software/repository-management/#repository-management","text":"This document attempts to record everything there is to know about repository management for the OSG.","title":"Repository Management"},{"location":"software/repository-management/#public-repositories","text":"We host four public-facing repositories at repo.opensciencegrid.org : development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage. testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison. release : This repository contains software that we are willing to support and can be used by the general community. contrib : RPMs contributed from outside the OSG. These repos are updated by the mash script running on repo.opensciencegrid.org .","title":"Public repositories"},{"location":"software/repository-management/#internal-repositories","text":"In addition to the public repositories above, we host two repositories on koji.opensciencegrid.org . These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them. minefield : This repository is a copy of development above. prerelease : This repository is a staging area for software that is slated to be in the next release. These repos are updated by the kojira daemon running on koji.opensciencegrid.org .","title":"Internal repositories"},{"location":"software/repository-management/#build-repositories","text":"Warning this section is out of date; the tags it references no longer exist. The koji task in osg-build uses the osg-3.4-el6-build/osg-3.4-el7-build repo, which is the union of the following repositories: Minefield a.k.a. osg-3.4-el6-development / osg-3.4-el7-development The osg-el6-internal / osg-el7-internal tag (containing build dependencies we do not want to make public) The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist) CentOS and EPEL Koji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies. The tarball creation scripts use the osg-3.4-el6-release-build / osg-3.4-el7-release-build repo, which is the union of the following repositories: Release a.k.a. osg-3.4-el6-release / osg-3.4-el7-release The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist ) CentOS and EPEL","title":"Build repositories"},{"location":"software/requesting-tokens/","text":"How to Request Tokens \u00b6 As part of the GridFTP and GSI migration , the OSG will be transitioning authentication away from X.509 certificates to the use of bearer tokens such as SciTokens or WLCG JWT . This document is intended as a guide for OSG developers for requesting tokens necessary for software development. Before Starting \u00b6 Before you can request the appropriate tokens, you must have the following: A WLCG INDIGO IAM account belonging to the wlcg , wlcg/pilots , and wlcg/xfers groups. One of the following: The ability to run containers through tools like docker or podman An installation of oidc-agent available as an RPM from the OSG repositories Requesting Tokens Using a Container \u00b6 oidc-agent is a process that runs in the background that can request access and refresh tokens from OpenID Connect token providers. Registering an OIDC profile \u00b6 Start an agent container in the background and name it my-agent to easily run subsequent commands against it: docker run -d --name my-agent opensciencegrid/oidc-agent:3.6-release Generate a local client profile and follow the prompts: docker exec -it my-agent oidc-gen -w device <CLIENT PROFILE> Specify the WLCG INDIGO IAM instance as the client issuer: Issuer [https://iam-test.indigo-datacloud.eu/]: https://wlcg.cloud.cnaf.infn.it/ Request wlcg , offline_access , and other scopes for the capabilities that you need: Capability Scope HTCondor READ compute.read HTCondor WRITE compute.modify compute.cancel compute.create XRootD read storage.read:/ XRootD write storage.modify:/ For example, to request HTCondor READ and WRITE access, specify the following scopes: This issuer supports the following scopes: openid profile email address phone offline_access wlcg iam wlcg.groups Space delimited list of scopes or 'max' [openid profile offline_access]: wlcg offline_access compute.read compute.modify compute.cancel compute.create Note that, prior to HTCondor 8.9.7, the server also needed condor:/ALLOW in all cases. When prompted, open https://wlcg.cloud.cnaf.infn.it/device in a browser, enter the code provided by oidc-gen , and click \"Submit\". On the next page, verify the scopes and client profile name, and click \"Authorize\". Enter a password to encrypt your local client profile. You'll need to remember this if you want to re-use this profile in subsequent sessions. Requesting access tokens \u00b6 Note You must first register a new profile . Request a token using the client profile that you used with oidc-gen : docker exec -it my-agent oidc-token --aud=\"<SERVER AUDIENCE>\" <CLIENT PROFILE> For tokens used against an HTCondor-CE, set <SERVER AUDIENCE> to <CE FQDN>:<CE PORT> . Copy the output of oidc-token into a file on the host where you need SciToken authentication, e.g. an HTCondor or XRootD client. Reloading an OIDC profile \u00b6 Note Required after restarting the running container. You must have an existing registered profile . If your existing container is not already running, start it: docker start my-agent Reload profile: docker exec -it my-agent oidc-add <CLIENT PROFILE> Enter password used to encrypt your <CLIENT PROFILE> created during profile registration. Requesting Tokens with an RPM installation \u00b6 Registering an OIDC profile \u00b6 Start the agent and add the appropriate variables to your environment: eval `oidc-agent` Generate a local client profile and follow the prompts: oidc-gen -w device <CLIENT PROFILE> Specify the WLCG INDIGO IAM instance as the client issuer: Issuer [https://iam-test.indigo-datacloud.eu/]: https://wlcg.cloud.cnaf.infn.it/ Request wlcg , offline_access , and other scopes for the capabilities that you need: Capability Scope HTCondor READ compute.read HTCondor WRITE compute.modify compute.cancel compute.create XRootD read read:/ XRootD write write:/ For example, to request HTCondor READ and WRITE access, specify the following scopes: This issuer supports the following scopes: openid profile email address phone offline_access wlcg iam wlcg.groups Space delimited list of scopes or 'max' [openid profile offline_access]: wlcg offline_access compute.read compute.modify compute.cancel compute.create Note that, prior to HTCondor 8.9.7, the server also needed condor:/ALLOW in all cases. When prompted, open https://wlcg.cloud.cnaf.infn.it/device in a browser, enter the code provided by oidc-gen , and click \"Submit\". On the next page, verify the scopes and client profile name, and click \"Authorize\". Enter a password to encrypt your local client profile. You'll need to remember this if you want to re-use this profile in subsequent sessions. Requesting access tokens \u00b6 Note You must first register a new profile . Request a token using the client profile that you used with oidc-gen : oidc-token --aud=\"<SERVER AUDIENCE>\" <CLIENT PROFILE> For tokens used against an HTCondor-CE, set <SERVER AUDIENCE> to <CE FQDN>:<CE PORT> . Copy the output of oidc-token into a file on the host where you need SciToken authentication, e.g. an HTCondor or XRootD client. Reloading an OIDC profile \u00b6 Note Required if you log out of the running machine. You must have an existing registered profile . If you do not already have a running 'oidc-agent', start one: eval 'oidc-agent' Reload profile: oidc-add <CLIENT PROFILE> Enter password used to encrypt your <CLIENT PROFILE> created during profile registration. Troubleshooting Tokens \u00b6 You can inspect the payload by copy-pasting the token into the \"Encoded\" text box here: http://jwt.io/ . Mouse over the fields and values for details.","title":"How to Request Tokens"},{"location":"software/requesting-tokens/#how-to-request-tokens","text":"As part of the GridFTP and GSI migration , the OSG will be transitioning authentication away from X.509 certificates to the use of bearer tokens such as SciTokens or WLCG JWT . This document is intended as a guide for OSG developers for requesting tokens necessary for software development.","title":"How to Request Tokens"},{"location":"software/requesting-tokens/#before-starting","text":"Before you can request the appropriate tokens, you must have the following: A WLCG INDIGO IAM account belonging to the wlcg , wlcg/pilots , and wlcg/xfers groups. One of the following: The ability to run containers through tools like docker or podman An installation of oidc-agent available as an RPM from the OSG repositories","title":"Before Starting"},{"location":"software/requesting-tokens/#requesting-tokens-using-a-container","text":"oidc-agent is a process that runs in the background that can request access and refresh tokens from OpenID Connect token providers.","title":"Requesting Tokens Using a Container"},{"location":"software/requesting-tokens/#registering-an-oidc-profile","text":"Start an agent container in the background and name it my-agent to easily run subsequent commands against it: docker run -d --name my-agent opensciencegrid/oidc-agent:3.6-release Generate a local client profile and follow the prompts: docker exec -it my-agent oidc-gen -w device <CLIENT PROFILE> Specify the WLCG INDIGO IAM instance as the client issuer: Issuer [https://iam-test.indigo-datacloud.eu/]: https://wlcg.cloud.cnaf.infn.it/ Request wlcg , offline_access , and other scopes for the capabilities that you need: Capability Scope HTCondor READ compute.read HTCondor WRITE compute.modify compute.cancel compute.create XRootD read storage.read:/ XRootD write storage.modify:/ For example, to request HTCondor READ and WRITE access, specify the following scopes: This issuer supports the following scopes: openid profile email address phone offline_access wlcg iam wlcg.groups Space delimited list of scopes or 'max' [openid profile offline_access]: wlcg offline_access compute.read compute.modify compute.cancel compute.create Note that, prior to HTCondor 8.9.7, the server also needed condor:/ALLOW in all cases. When prompted, open https://wlcg.cloud.cnaf.infn.it/device in a browser, enter the code provided by oidc-gen , and click \"Submit\". On the next page, verify the scopes and client profile name, and click \"Authorize\". Enter a password to encrypt your local client profile. You'll need to remember this if you want to re-use this profile in subsequent sessions.","title":"Registering an OIDC profile"},{"location":"software/requesting-tokens/#requesting-access-tokens","text":"Note You must first register a new profile . Request a token using the client profile that you used with oidc-gen : docker exec -it my-agent oidc-token --aud=\"<SERVER AUDIENCE>\" <CLIENT PROFILE> For tokens used against an HTCondor-CE, set <SERVER AUDIENCE> to <CE FQDN>:<CE PORT> . Copy the output of oidc-token into a file on the host where you need SciToken authentication, e.g. an HTCondor or XRootD client.","title":"Requesting access tokens"},{"location":"software/requesting-tokens/#reloading-an-oidc-profile","text":"Note Required after restarting the running container. You must have an existing registered profile . If your existing container is not already running, start it: docker start my-agent Reload profile: docker exec -it my-agent oidc-add <CLIENT PROFILE> Enter password used to encrypt your <CLIENT PROFILE> created during profile registration.","title":"Reloading an OIDC profile"},{"location":"software/requesting-tokens/#requesting-tokens-with-an-rpm-installation","text":"","title":"Requesting Tokens with an RPM installation"},{"location":"software/requesting-tokens/#registering-an-oidc-profile_1","text":"Start the agent and add the appropriate variables to your environment: eval `oidc-agent` Generate a local client profile and follow the prompts: oidc-gen -w device <CLIENT PROFILE> Specify the WLCG INDIGO IAM instance as the client issuer: Issuer [https://iam-test.indigo-datacloud.eu/]: https://wlcg.cloud.cnaf.infn.it/ Request wlcg , offline_access , and other scopes for the capabilities that you need: Capability Scope HTCondor READ compute.read HTCondor WRITE compute.modify compute.cancel compute.create XRootD read read:/ XRootD write write:/ For example, to request HTCondor READ and WRITE access, specify the following scopes: This issuer supports the following scopes: openid profile email address phone offline_access wlcg iam wlcg.groups Space delimited list of scopes or 'max' [openid profile offline_access]: wlcg offline_access compute.read compute.modify compute.cancel compute.create Note that, prior to HTCondor 8.9.7, the server also needed condor:/ALLOW in all cases. When prompted, open https://wlcg.cloud.cnaf.infn.it/device in a browser, enter the code provided by oidc-gen , and click \"Submit\". On the next page, verify the scopes and client profile name, and click \"Authorize\". Enter a password to encrypt your local client profile. You'll need to remember this if you want to re-use this profile in subsequent sessions.","title":"Registering an OIDC profile"},{"location":"software/requesting-tokens/#requesting-access-tokens_1","text":"Note You must first register a new profile . Request a token using the client profile that you used with oidc-gen : oidc-token --aud=\"<SERVER AUDIENCE>\" <CLIENT PROFILE> For tokens used against an HTCondor-CE, set <SERVER AUDIENCE> to <CE FQDN>:<CE PORT> . Copy the output of oidc-token into a file on the host where you need SciToken authentication, e.g. an HTCondor or XRootD client.","title":"Requesting access tokens"},{"location":"software/requesting-tokens/#reloading-an-oidc-profile_1","text":"Note Required if you log out of the running machine. You must have an existing registered profile . If you do not already have a running 'oidc-agent', start one: eval 'oidc-agent' Reload profile: oidc-add <CLIENT PROFILE> Enter password used to encrypt your <CLIENT PROFILE> created during profile registration.","title":"Reloading an OIDC profile"},{"location":"software/requesting-tokens/#troubleshooting-tokens","text":"You can inspect the payload by copy-pasting the token into the \"Encoded\" text box here: http://jwt.io/ . Mouse over the fields and values for details.","title":"Troubleshooting Tokens"},{"location":"software/resurrecting-epel-packages/","text":"Resurrecting EPEL RPMs \u00b6 You will need to be a Koji admin to do these steps. [user@client ~] $ osg-koji --list-permissions --mine Will tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman. EPEL version EPEL Koji tag Our Koji tag 5 dist-5E-epel epelrescue-el5 6 dist-6E-epel epelrescue-el6 7 epel7 epelrescue-el7 Determine the NVR of the build containing the RPM of the package you want. Use the Fedora/EPEL Koji web interface ( https://koji.fedoraproject.org ) to search for it. You can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards. EPEL builds have .el5, .el6, or .el7 in the dist tag. Download all RPMs for all architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms. You have three options for the downloads: Use the links in the web interface Use the koji command-line interface against the Fedora koji: Download fedora-koji.conf , attached to this page Run koji --noauth -c fedora-koji.conf download-build --debuginfo <PACKAGE_NVR> Delete RPMs for architectures we do not care about (see list above) <PACKAGE_NVR> is the Name-Version-Release information about the build, which was determined in the step 1 above Dig around in https://kojipkgs.fedoraproject.org/packages/ On your development machine: Important: Verify that all of the RPMs are signed: [root@client ~] # rpm -K *.rpm | grep -iv gpg should be empty If not, STOP and sign them using the OSG RPM key -- talk to Mat Import the RPMs themselves into the Koji system [user@client ~] $ osg-koji import <RPM_DIRECTORY>/*.rpm Where <RPM_DIRECTORY> is the directory where you have downloaded the rpms. They will not be in any tags at this point Add the package to the whitelist for our koji tag: [user@client ~] $ osg-koji add-pkg <OUR_KOJI_TAG> <PACKAGE> --owner = \"<YOUR_KOJI_USERNAME>\" Where <OUR_KOJI_TAG> is one of those listed in the table at the top of this page, an example of <PACKAGE> is: cvmfs-config-osg-2.4-1.osg34.el6 and <YOUR_KOJI_USERNAME> is the username you use to interact with the Koji system Actually tag the builds: [user@client ~] $ osg-koji tag-pkg <OUR_KOJI_TAG> <PACKAGE> Check the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in. If it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them): for repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do osg-koji regen-repo --nowait $repo done Make a test VM and install the package from minefield to test that it is actually present. Update the epelrescue RPMs table below Removing resurrected RPMs \u00b6 In case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version: Find out the NVR of the build: [user@client ~] $ osg-koji list-tagged <OUR_KOJI_TAG> <PACKAGE> Where <OUR_KOJI_TAG> is one of those listed in the table at the top of this page and an example of <PACKAGE> is: cvmfs-config-osg-2.4-1.osg34.el6 Untag the packages: [user@client ~] $ osg-koji untag-pkg <OUR_KOJI_TAG> <PACKAGE> Why you should not use block-pkg \u00b6 EPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case: EPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5. Later, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5. I used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead. The block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build. I unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again. Policy for epelrescue tags \u00b6 https://jira.opensciencegrid.org/browse/SOFTWARE-2046 Table of epelrescue RPMs \u00b6 Package Distro version Date added Reason added Date removed python-six-1.7.3-1.el6 6 2015-08-12 Dep of osg-build (via mock) 2015-10-14 python-argparse-1.2.1-2.el6 6 2015-09-23 Dep of osg-wn-client (via gfal2) 2015-10-14 python-backports-ssl_match_hostname-3.4.0.2-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-requests-1.1.0-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-urllib3-1.5-7.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 Finding out if a package is still needed in epelrescue \u00b6 Set $pkg to the name of a package to test (e.g. python-six ), and $rhel set to the RHEL version you're testing for (e.g. 5 , 6 , or 7 ). Using Carl's centos-srpms , scientific-srpms , slf-srpms scripts: for script in centos-srpms scientific-srpms slf-srpms; do echo -n $script \": \" $script -$rhel $pkg | grep . || echo none done A dry run of removing the package: osg-koji untag-pkg -n --all epelrescue-el$rhel $pkg Remove the -n when the output of that looks fine.","title":"Resurrecting epel packages"},{"location":"software/resurrecting-epel-packages/#resurrecting-epel-rpms","text":"You will need to be a Koji admin to do these steps. [user@client ~] $ osg-koji --list-permissions --mine Will tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman. EPEL version EPEL Koji tag Our Koji tag 5 dist-5E-epel epelrescue-el5 6 dist-6E-epel epelrescue-el6 7 epel7 epelrescue-el7 Determine the NVR of the build containing the RPM of the package you want. Use the Fedora/EPEL Koji web interface ( https://koji.fedoraproject.org ) to search for it. You can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards. EPEL builds have .el5, .el6, or .el7 in the dist tag. Download all RPMs for all architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms. You have three options for the downloads: Use the links in the web interface Use the koji command-line interface against the Fedora koji: Download fedora-koji.conf , attached to this page Run koji --noauth -c fedora-koji.conf download-build --debuginfo <PACKAGE_NVR> Delete RPMs for architectures we do not care about (see list above) <PACKAGE_NVR> is the Name-Version-Release information about the build, which was determined in the step 1 above Dig around in https://kojipkgs.fedoraproject.org/packages/ On your development machine: Important: Verify that all of the RPMs are signed: [root@client ~] # rpm -K *.rpm | grep -iv gpg should be empty If not, STOP and sign them using the OSG RPM key -- talk to Mat Import the RPMs themselves into the Koji system [user@client ~] $ osg-koji import <RPM_DIRECTORY>/*.rpm Where <RPM_DIRECTORY> is the directory where you have downloaded the rpms. They will not be in any tags at this point Add the package to the whitelist for our koji tag: [user@client ~] $ osg-koji add-pkg <OUR_KOJI_TAG> <PACKAGE> --owner = \"<YOUR_KOJI_USERNAME>\" Where <OUR_KOJI_TAG> is one of those listed in the table at the top of this page, an example of <PACKAGE> is: cvmfs-config-osg-2.4-1.osg34.el6 and <YOUR_KOJI_USERNAME> is the username you use to interact with the Koji system Actually tag the builds: [user@client ~] $ osg-koji tag-pkg <OUR_KOJI_TAG> <PACKAGE> Check the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in. If it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them): for repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do osg-koji regen-repo --nowait $repo done Make a test VM and install the package from minefield to test that it is actually present. Update the epelrescue RPMs table below","title":"Resurrecting EPEL RPMs"},{"location":"software/resurrecting-epel-packages/#removing-resurrected-rpms","text":"In case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version: Find out the NVR of the build: [user@client ~] $ osg-koji list-tagged <OUR_KOJI_TAG> <PACKAGE> Where <OUR_KOJI_TAG> is one of those listed in the table at the top of this page and an example of <PACKAGE> is: cvmfs-config-osg-2.4-1.osg34.el6 Untag the packages: [user@client ~] $ osg-koji untag-pkg <OUR_KOJI_TAG> <PACKAGE>","title":"Removing resurrected RPMs"},{"location":"software/resurrecting-epel-packages/#why-you-should-not-use-block-pkg","text":"EPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case: EPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5. Later, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5. I used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead. The block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build. I unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again.","title":"Why you should not use block-pkg"},{"location":"software/resurrecting-epel-packages/#policy-for-epelrescue-tags","text":"https://jira.opensciencegrid.org/browse/SOFTWARE-2046","title":"Policy for epelrescue tags"},{"location":"software/resurrecting-epel-packages/#table-of-epelrescue-rpms","text":"Package Distro version Date added Reason added Date removed python-six-1.7.3-1.el6 6 2015-08-12 Dep of osg-build (via mock) 2015-10-14 python-argparse-1.2.1-2.el6 6 2015-09-23 Dep of osg-wn-client (via gfal2) 2015-10-14 python-backports-ssl_match_hostname-3.4.0.2-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-requests-1.1.0-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-urllib3-1.5-7.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14","title":"Table of epelrescue RPMs"},{"location":"software/resurrecting-epel-packages/#finding-out-if-a-package-is-still-needed-in-epelrescue","text":"Set $pkg to the name of a package to test (e.g. python-six ), and $rhel set to the RHEL version you're testing for (e.g. 5 , 6 , or 7 ). Using Carl's centos-srpms , scientific-srpms , slf-srpms scripts: for script in centos-srpms scientific-srpms slf-srpms; do echo -n $script \": \" $script -$rhel $pkg | grep . || echo none done A dry run of removing the package: osg-koji untag-pkg -n --all epelrescue-el$rhel $pkg Remove the -n when the output of that looks fine.","title":"Finding out if a package is still needed in epelrescue"},{"location":"software/rpm-development-guide/","text":"RPM Development Guide \u00b6 This page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack. Principles \u00b6 The principles below guide the design and implementation of the technical details that follow. Packages should adhere to community standards (e.g., Fedora Packaging Guidelines when possible, and significant deviations must be documented Every released package must be reproducible from data stored in our system Source code for software should be clearly separable from the packaging of that software Upstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team Building source and binary packages from our system should be easy and efficient External developers should have a clear and effective system for developing and contributing packages We should use standard tools from relevant packaging and development communities when appropriate Contributing Packages \u00b6 We encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors. To participate in the packaging community: You must subscribe to the software-discuss@osg-htc.org email list. To create and edit packages: Obtain access to the OSG Software-Packaging repo . To upload new source tarballs: You must have a CHTC account with write access to the OSG source tarball directory. To build using the OSG's Koji build system, you must be able to get a Kerberos ticket with either an @AD.WISC.EDU or @FNAL.GOV principal; Send email to help@osg-htc.org to request access to the build system. Include your Kerberos principal and desired username in the email. Development Infrastructure \u00b6 This section documents most of what a developer needs to know about our RPM infrastructure: Upstream Source Cache \u2014 a filesystem scheme for caching upstream source files Revision Control System \u2014 where to get and store development files, and how they are organized Build System \u2014 how to build packages from the revision control system Yum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it Upstream Source Cache \u00b6 Source tarballs and other large files used as inputs to RPM builds are stored in a cache area at CHTC. The Koji build system uses this cache; the files are also downloadable from https://sw-upstream.svc.osg-htc.org/upstream/ . The files are stored on osgsw-ap.chtc.wisc.edu , in the directory /osgsw/upstream . To upload files to the cache, you must have shell access to the OSG Software Access Point host, osgsw-ap.chtc.wisc.edu . Send email to help@osg-htc.org to request permission. Upstream Cache Organization \u00b6 Upstream source files are stored in the filesystem as follows: /osgsw/upstream/<PACKAGE>/<VERSION>/<FILE> where: Symbol Definition Example <PACKAGE> Name of the source package the file is used in xrootd <VERSION> Version of the sources (\"Version\" field in the RPM spec file) 5.8.4 <FILE> Filename used in the Source line in the RPM spec file xrootd-5.8.4.tar.gz which leads to the complete example of /osgsw/upstream/xrootd/5.8.4/xrootd-5.8.4.tar.gz Upstream source files are referenced from within the revision control system; see below for details. You will need to know the SHA1 checksum of any files you use from the cache. Do get it, do: $ sha1sum /osgsw/upstream/<PACKAGE>/<VERSION>/<FILE> Git/GitHub Hosted Upstream Files \u00b6 It is also possible to pull sources and spec files from remote Git or GitHub repos instead of our source cache. See the upstream dir info for more information. Revision Control System \u00b6 All packages that the OSG Software Team releases are checked into the Software Packaging repository. Software Packaging Repo Access \u00b6 The OSG Software Packaging repo is located at https://github.com/osg-htc/software-packaging The repo is organized as follows: <SUBTREE>/<PACKAGE> where a subtree corresponds to a set of OSG Software repos such as 25-main , 24-upcoming , etc. For example, the package directory for xrootd for the osg-25-main-* tags are located in 25-main/xrootd The subtree must correspond to the repos being built to. For example, you may not build into osg-25-main-* from the 24-upcoming subtree. Package Directory Organization \u00b6 Within a source package directory, the following files (detailed in separate sections below) may exist: README text file package notes, by and for developers upstream/ directory references to the upstream source cache and other kinds of upstream files osg/ directory overrides and patches of upstream files, plus new files, which contribute to the final OSG source package README \u00b6 This is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the why , not just the what . upstream \u00b6 Within the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents. A reference file is named: <DESCRIPTION>.<TYPE>.source where: Symbol Definition Example <DESCRIPTION> Descriptive label of the source of the referenced file developer , epel , emi <TYPE> Type of referenced file tarball , srpm and contain references to cached files, Git repos, and comments. Comments start with # and continue until the end of the line. It is useful to add the original URL of the upstream file into a comment. Cached files \u00b6 To reference files in the upstream source cache, use the upstream source cache path defined above, without the prefix component, followed by the sha1sum of the file: <PACKAGE>/<VERSION>/<FILE> sha1sum=<SHA1SUM> Obtain the sha1sum by running the sha1sum command with the source file as an argument, i.e. $ sha1sum /osgsw/upstream/<PACKAGE>/<VERSION>/<FILE> Example The reference file for globus-common 's source tarball is named epel.srpm.source and contains: globus-common/16.4/globus-common-16.4-1.el6.src.rpm sha1sum=134478c56c2437c335c20636831f794b66290bec # Downloaded from 'http://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm' GitHub repos \u00b6 Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization and osg-htc GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to technology-team@osg-htc.org . Note See also advanced features for Git and GitHub repos . To reference tags in GitHub repos, use the following syntax (all on one line): type=github repo=<OWNER>/<PROJECT> tag=<TAG> hash=<HASH> where: Symbol Definition Example <OWNER> Owner of the GitHub repo opensciencegrid <PROJECT> Name of the project osg-build <TAG> Git tag to use v1.12.2 <HASH> Full 40-char Git hash of the tag cff50ffe812282552cedae81f3809d3cf7087a3e Note The tarball will be called <PROJECT>-<VERSION>.tar.gz where <VERSION> is <TAG> without the v prefix (if there is one). Example You can refer to the 1.12.2 release of osg-build with this line: type=github repo=opensciencegrid/osg-build tag=v1.12.2 hash=cff50ffe812282552cedae81f3809d3cf7087a3e This results in a tarball named osg-build-1.12.2.tar.gz . In addition, if the repository contains a file called rpm/<PROJECT>.spec , it will be used as the spec file for the build (unless overridden in the osg directory). Git repos \u00b6 Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to technology-team@osg-htc.org . Note You can use a shorter syntax for GitHub repos -- see above. See also advanced features for Git and GitHub repos . To reference tags in Git repos, use the following syntax (all on one line): type=git url=<URL> name=<NAME> tag=<TAG> hash=<HASH> where: Symbol Definition Example <URL> Location of the Git repo https://github.com/opensciencegrid/osg-build.git <NAME> Name of the software (optional) osg-build <TAG> Git tag to use v1.11.2 <HASH> Full 40-char Git hash of the tag 5bcf48c442d21b1e8c93a468d884f84122f7cc9e Note <NAME> is optional; if not present, OSG-Build will use the last component of the URL, without the .git suffix. The tarball will be called <NAME>-<VERSION>.tar.gz where <VERSION> is <TAG> without the v prefix (if there is one). Example The reference file for osg-build 's repo is named osg.github.source and contains: type=git url=https://github.com/opensciencegrid/osg-build.git name=osg-build tag=v1.11.2 hash=5bcf48c442d21b1e8c93a468d884f84122f7cc9e This results in a tarball named osg-build-1.11.2.tar.gz . In addition, if the repository contains a file called rpm/<NAME>.spec , it will be used as the spec file for the build (unless overridden in the osg directory). Typical workflow when building out of GitHub repos \u00b6 Fork the repository of the package that you would like to build Create a new branch in your fork Make, commit, and push changes to your new branch In your fork, tag the commit that you would like to build In the upstream/osg.github.source , change the repo to point at your fork and tag Attempt a scratch build If the build fails, remove the tag and repeat steps 3-6 Submit a PR to merge changes upstream Tag the final version on the upstream fork Build the version that will go through the normal software cycle Note Packaging-only changes should be tagged with a release number of the format v<version>-<release> , e.g. v3.4.23-2 Advanced features for Git and GitHub repos \u00b6 The following features make software development in Git and GitHub more convenient: Support for RPM release numbers in Git tags: If the tag for the software contains a dash, as in v1.12.2-1 , it is assumed that the text after the dash is the RPM release instead of the software version. The RPM release is not included in the tarball. That is, the project osg-build with the tag v1.12.2-1 will result in a tarball named osg-build-1.12.1.tar.gz , not osg-build-1.12.1-1.tar.gz . Can specify tarball name in the .source file: The new tarball attribute allows you to specify the name of the tarball and directory that the repo contents will be put into. The syntax is tarball=<NAME>.tar.gz . The extension must be .tar.gz , no other archive formats are supported. The directory inside the tarball will then be <NAME>/ . Can ignore hash mismatch (scratch and local builds only): For local builds (rpmbuild and mock tasks) and Koji scratch builds, a hash mismatch will result in a warning. Non-scratch Koji builds will still consider it an error. Can use a branch as the tag: The tag attribute can refer to a branch instead of a tag, e.g. tag=master . Combining the last two features can really speed up package development. For example, you can use this to make scratch builds of the current master: type=github repo=<OWNER>/<PROJECT> tarball=<PROJECT>-<VERSION>.tar.gz tag=master hash=0 This might also be useful as part of a continuous integration scheme (e.g. Travis-CI). osg \u00b6 The osg directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files: An RPM .spec file, which overrides any spec file from a referenced source Patch ( .patch ) or replacement files, which override any same-named file from the top-level directory of a referenced source Other files, which must be explicitly placed into the package by the spec file Generated directories \u00b6 The following directories may be generated by our build tool, OSG-Build . They are not under revision control. _upstream_srpm_contents/ expanded contents of a cached upstream source package _upstream_tarball_contents/ expanded contents of all cached upstream source tarballs _final_srpm_contents/ the final contents of the OSG source package _build_results/ OSG source and binary packages resulting from a build _quilt/ expanded, patched contents of the upstream sources, as generated by the quilt tool _upstream_srpm_contents \u00b6 The _upstream_srpm_contents directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use. _upstream_tarball_contents \u00b6 The _upstream_tarball_contents directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the --full-extract option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.). _final_srpm_contents \u00b6 The _final_srpm_contents directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use. _build_results \u00b6 The _build_results directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use. _quilt \u00b6 The _quilt directory contains the unpacked sources after they have been patched using the quilt utility. This allows easier patch development. Packaging Organization Examples \u00b6 Use Case 1: Packaging an Upstream Source Tarball \u00b6 When the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains: rsv/ osg/ rsv.spec upstream/ developer.tarball.source Use Case 2: Passing Through a Source RPM \u00b6 When the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the globus-common source RPM from EPEL. Its package directory contains: globus-common/ upstream/ epel.srpm.source Use Case 3: Modifying a Source RPM \u00b6 When the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the osg directory. For example, we use this mechanism for the globus-ftp-client package, originally obtained from EPEL. Its package directory contains: globus-ftp-client/ osg/ globus-ftp-client.spec 1853-ssh-bin.patch upstream/ epel.srpm.source Build Process \u00b6 All necessary information to create the package will be committed to the OSG Software Packaging repository (see below) The OSG build tools will take those files, create a source RPM, and submit it to our Koji build system Developers may use rpmbuild and mock for faster iterative development before submitting the package to Koji. osg-build may be used as a wrapper script around rpmbuild and mock . OSG Software Repository \u00b6 OSG Operations maintains the Yum repositories that contain our source and binary RPMs at https://repo.osg-htc.org/osg/ and are mirrored at other institutions as well. Release Levels \u00b6 Every package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level ( osg-development ). The members of the OSG Software and Release teams may promote packages through the release levels, as per our Release Policy page . Packaging Conventions \u00b6 In addition to adhering to the Fedora Packaging Guidelines (FPG), we have a few rules and guidelines of our own: When we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows: The version of the original RPM remains unchanged The release is composed of three parts: ORIGINALRELEASE.OSGRELEASE We add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use %{?dist} in the Release field) Example: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes foobar-3.0.5-1.1.osg33.el6 Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like: Release : 1.2 %{?dist} Packaging for Multiple Distro Versions \u00b6 Conditionalizing spec files \u00b6 Some packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this. The following macros are defined: Name Value (EL8) Value (EL9) %rhel 8 9 %el8 1 undefined or 0 %el9 undefined or 0 1 Here's how to use them: %if 0%{?el8} # this code will be executed on EL 8 only %endif %if 0%{?el9} # this code will be executed on EL 9 only %endif %if 0%{?rhel} >= 9 # this code will be executed on EL 9 and newer %endif (the 'else if' macro %elif does not parse on EL8 or older. The 0 before the %{?el9} is required to avoid a syntax error if the macro is undefined.) Renaming or Removing Packages \u00b6 Occasionally we want to cause a package to be removed on update, or replaced by a package with a different name. For the most part, the Fedora Packaging Guidelines page on renames shows how to do that. The exception is that we do not have the equivalent of a fedora-obsolete-packages package, so in order to force the removal of an entire package (not a subpackage), we have to dummy out the package instead -- see below. (This should be a rare situation.) Note After doing a rename or a removal, you must update all the packages and subpackages that require the package being removed or renamed, and change or remove the requirements as appropriate. To find packages that require the old package at run time, set up a host with the OSG repos and install the yum-utils RPM. On EL8 and newer, run: $ repoquery --whatrequires $OLDPACKAGE On EL7, run $ repoquery --plugins --whatrequires $OLDPACKAGE To find packages that require the old package at build time, install osg-build , and do this from a checkout of the OSG repos: $ osg-build prebuild * $ for srpm in */_final_srpm_contents/*.src.rpm ; do \\ echo \"***** $srpm *****\"; \\ rpm -q --requires -p $srpm | grep -w $OLDPACKAGE; \\ done (examine the output to avoid false matches) Note Carefully test these changes, including places where the old package may be brought in indirectly. Dummying out a package \u00b6 In order to forcibly remove an entire package with no replacement, you have to replace the package with one that does nothing. This is because there is no package that will \"obsolete\" the old package. Do the following for the main package and any subpackages it may have: Change the Summary to \"Dummy package\" Change the %description to: This is an empty package created for $REASONS It may safely be removed. Where $REASONS is a description of why you need this dummy package Remove all Requires and Obsoletes lines Do not remove Provides lines Remove %pre and %post scriptlets Unless there is a good reason not to, remove %preun and %postun scriptlets Empty the %files section","title":"RPM Development Guide"},{"location":"software/rpm-development-guide/#rpm-development-guide","text":"This page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack.","title":"RPM Development Guide"},{"location":"software/rpm-development-guide/#principles","text":"The principles below guide the design and implementation of the technical details that follow. Packages should adhere to community standards (e.g., Fedora Packaging Guidelines when possible, and significant deviations must be documented Every released package must be reproducible from data stored in our system Source code for software should be clearly separable from the packaging of that software Upstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team Building source and binary packages from our system should be easy and efficient External developers should have a clear and effective system for developing and contributing packages We should use standard tools from relevant packaging and development communities when appropriate","title":"Principles"},{"location":"software/rpm-development-guide/#contributing-packages","text":"We encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors. To participate in the packaging community: You must subscribe to the software-discuss@osg-htc.org email list. To create and edit packages: Obtain access to the OSG Software-Packaging repo . To upload new source tarballs: You must have a CHTC account with write access to the OSG source tarball directory. To build using the OSG's Koji build system, you must be able to get a Kerberos ticket with either an @AD.WISC.EDU or @FNAL.GOV principal; Send email to help@osg-htc.org to request access to the build system. Include your Kerberos principal and desired username in the email.","title":"Contributing Packages"},{"location":"software/rpm-development-guide/#development-infrastructure","text":"This section documents most of what a developer needs to know about our RPM infrastructure: Upstream Source Cache \u2014 a filesystem scheme for caching upstream source files Revision Control System \u2014 where to get and store development files, and how they are organized Build System \u2014 how to build packages from the revision control system Yum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it","title":"Development Infrastructure"},{"location":"software/rpm-development-guide/#upstream-source-cache","text":"Source tarballs and other large files used as inputs to RPM builds are stored in a cache area at CHTC. The Koji build system uses this cache; the files are also downloadable from https://sw-upstream.svc.osg-htc.org/upstream/ . The files are stored on osgsw-ap.chtc.wisc.edu , in the directory /osgsw/upstream . To upload files to the cache, you must have shell access to the OSG Software Access Point host, osgsw-ap.chtc.wisc.edu . Send email to help@osg-htc.org to request permission.","title":"Upstream Source Cache"},{"location":"software/rpm-development-guide/#upstream-cache-organization","text":"Upstream source files are stored in the filesystem as follows: /osgsw/upstream/<PACKAGE>/<VERSION>/<FILE> where: Symbol Definition Example <PACKAGE> Name of the source package the file is used in xrootd <VERSION> Version of the sources (\"Version\" field in the RPM spec file) 5.8.4 <FILE> Filename used in the Source line in the RPM spec file xrootd-5.8.4.tar.gz which leads to the complete example of /osgsw/upstream/xrootd/5.8.4/xrootd-5.8.4.tar.gz Upstream source files are referenced from within the revision control system; see below for details. You will need to know the SHA1 checksum of any files you use from the cache. Do get it, do: $ sha1sum /osgsw/upstream/<PACKAGE>/<VERSION>/<FILE>","title":"Upstream Cache Organization"},{"location":"software/rpm-development-guide/#gitgithub-hosted-upstream-files","text":"It is also possible to pull sources and spec files from remote Git or GitHub repos instead of our source cache. See the upstream dir info for more information.","title":"Git/GitHub Hosted Upstream Files"},{"location":"software/rpm-development-guide/#revision-control-system","text":"All packages that the OSG Software Team releases are checked into the Software Packaging repository.","title":"Revision Control System"},{"location":"software/rpm-development-guide/#software-packaging-repo-access","text":"The OSG Software Packaging repo is located at https://github.com/osg-htc/software-packaging The repo is organized as follows: <SUBTREE>/<PACKAGE> where a subtree corresponds to a set of OSG Software repos such as 25-main , 24-upcoming , etc. For example, the package directory for xrootd for the osg-25-main-* tags are located in 25-main/xrootd The subtree must correspond to the repos being built to. For example, you may not build into osg-25-main-* from the 24-upcoming subtree.","title":"Software Packaging Repo Access"},{"location":"software/rpm-development-guide/#package-directory-organization","text":"Within a source package directory, the following files (detailed in separate sections below) may exist: README text file package notes, by and for developers upstream/ directory references to the upstream source cache and other kinds of upstream files osg/ directory overrides and patches of upstream files, plus new files, which contribute to the final OSG source package","title":"Package Directory Organization"},{"location":"software/rpm-development-guide/#readme","text":"This is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the why , not just the what .","title":"README"},{"location":"software/rpm-development-guide/#upstream","text":"Within the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents. A reference file is named: <DESCRIPTION>.<TYPE>.source where: Symbol Definition Example <DESCRIPTION> Descriptive label of the source of the referenced file developer , epel , emi <TYPE> Type of referenced file tarball , srpm and contain references to cached files, Git repos, and comments. Comments start with # and continue until the end of the line. It is useful to add the original URL of the upstream file into a comment.","title":"upstream"},{"location":"software/rpm-development-guide/#cached-files","text":"To reference files in the upstream source cache, use the upstream source cache path defined above, without the prefix component, followed by the sha1sum of the file: <PACKAGE>/<VERSION>/<FILE> sha1sum=<SHA1SUM> Obtain the sha1sum by running the sha1sum command with the source file as an argument, i.e. $ sha1sum /osgsw/upstream/<PACKAGE>/<VERSION>/<FILE> Example The reference file for globus-common 's source tarball is named epel.srpm.source and contains: globus-common/16.4/globus-common-16.4-1.el6.src.rpm sha1sum=134478c56c2437c335c20636831f794b66290bec # Downloaded from 'http://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm'","title":"Cached files"},{"location":"software/rpm-development-guide/#github-repos","text":"Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization and osg-htc GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to technology-team@osg-htc.org . Note See also advanced features for Git and GitHub repos . To reference tags in GitHub repos, use the following syntax (all on one line): type=github repo=<OWNER>/<PROJECT> tag=<TAG> hash=<HASH> where: Symbol Definition Example <OWNER> Owner of the GitHub repo opensciencegrid <PROJECT> Name of the project osg-build <TAG> Git tag to use v1.12.2 <HASH> Full 40-char Git hash of the tag cff50ffe812282552cedae81f3809d3cf7087a3e Note The tarball will be called <PROJECT>-<VERSION>.tar.gz where <VERSION> is <TAG> without the v prefix (if there is one). Example You can refer to the 1.12.2 release of osg-build with this line: type=github repo=opensciencegrid/osg-build tag=v1.12.2 hash=cff50ffe812282552cedae81f3809d3cf7087a3e This results in a tarball named osg-build-1.12.2.tar.gz . In addition, if the repository contains a file called rpm/<PROJECT>.spec , it will be used as the spec file for the build (unless overridden in the osg directory).","title":"GitHub repos"},{"location":"software/rpm-development-guide/#git-repos","text":"Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to technology-team@osg-htc.org . Note You can use a shorter syntax for GitHub repos -- see above. See also advanced features for Git and GitHub repos . To reference tags in Git repos, use the following syntax (all on one line): type=git url=<URL> name=<NAME> tag=<TAG> hash=<HASH> where: Symbol Definition Example <URL> Location of the Git repo https://github.com/opensciencegrid/osg-build.git <NAME> Name of the software (optional) osg-build <TAG> Git tag to use v1.11.2 <HASH> Full 40-char Git hash of the tag 5bcf48c442d21b1e8c93a468d884f84122f7cc9e Note <NAME> is optional; if not present, OSG-Build will use the last component of the URL, without the .git suffix. The tarball will be called <NAME>-<VERSION>.tar.gz where <VERSION> is <TAG> without the v prefix (if there is one). Example The reference file for osg-build 's repo is named osg.github.source and contains: type=git url=https://github.com/opensciencegrid/osg-build.git name=osg-build tag=v1.11.2 hash=5bcf48c442d21b1e8c93a468d884f84122f7cc9e This results in a tarball named osg-build-1.11.2.tar.gz . In addition, if the repository contains a file called rpm/<NAME>.spec , it will be used as the spec file for the build (unless overridden in the osg directory).","title":"Git repos"},{"location":"software/rpm-development-guide/#typical-workflow-when-building-out-of-github-repos","text":"Fork the repository of the package that you would like to build Create a new branch in your fork Make, commit, and push changes to your new branch In your fork, tag the commit that you would like to build In the upstream/osg.github.source , change the repo to point at your fork and tag Attempt a scratch build If the build fails, remove the tag and repeat steps 3-6 Submit a PR to merge changes upstream Tag the final version on the upstream fork Build the version that will go through the normal software cycle Note Packaging-only changes should be tagged with a release number of the format v<version>-<release> , e.g. v3.4.23-2","title":"Typical workflow when building out of GitHub repos"},{"location":"software/rpm-development-guide/#advanced-features-for-git-and-github-repos","text":"The following features make software development in Git and GitHub more convenient: Support for RPM release numbers in Git tags: If the tag for the software contains a dash, as in v1.12.2-1 , it is assumed that the text after the dash is the RPM release instead of the software version. The RPM release is not included in the tarball. That is, the project osg-build with the tag v1.12.2-1 will result in a tarball named osg-build-1.12.1.tar.gz , not osg-build-1.12.1-1.tar.gz . Can specify tarball name in the .source file: The new tarball attribute allows you to specify the name of the tarball and directory that the repo contents will be put into. The syntax is tarball=<NAME>.tar.gz . The extension must be .tar.gz , no other archive formats are supported. The directory inside the tarball will then be <NAME>/ . Can ignore hash mismatch (scratch and local builds only): For local builds (rpmbuild and mock tasks) and Koji scratch builds, a hash mismatch will result in a warning. Non-scratch Koji builds will still consider it an error. Can use a branch as the tag: The tag attribute can refer to a branch instead of a tag, e.g. tag=master . Combining the last two features can really speed up package development. For example, you can use this to make scratch builds of the current master: type=github repo=<OWNER>/<PROJECT> tarball=<PROJECT>-<VERSION>.tar.gz tag=master hash=0 This might also be useful as part of a continuous integration scheme (e.g. Travis-CI).","title":"Advanced features for Git and GitHub repos"},{"location":"software/rpm-development-guide/#osg","text":"The osg directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files: An RPM .spec file, which overrides any spec file from a referenced source Patch ( .patch ) or replacement files, which override any same-named file from the top-level directory of a referenced source Other files, which must be explicitly placed into the package by the spec file","title":"osg"},{"location":"software/rpm-development-guide/#generated-directories","text":"The following directories may be generated by our build tool, OSG-Build . They are not under revision control. _upstream_srpm_contents/ expanded contents of a cached upstream source package _upstream_tarball_contents/ expanded contents of all cached upstream source tarballs _final_srpm_contents/ the final contents of the OSG source package _build_results/ OSG source and binary packages resulting from a build _quilt/ expanded, patched contents of the upstream sources, as generated by the quilt tool","title":"Generated directories"},{"location":"software/rpm-development-guide/#_upstream_srpm_contents","text":"The _upstream_srpm_contents directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use.","title":"_upstream_srpm_contents"},{"location":"software/rpm-development-guide/#_upstream_tarball_contents","text":"The _upstream_tarball_contents directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the --full-extract option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.).","title":"_upstream_tarball_contents"},{"location":"software/rpm-development-guide/#_final_srpm_contents","text":"The _final_srpm_contents directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use.","title":"_final_srpm_contents"},{"location":"software/rpm-development-guide/#_build_results","text":"The _build_results directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use.","title":"_build_results"},{"location":"software/rpm-development-guide/#_quilt","text":"The _quilt directory contains the unpacked sources after they have been patched using the quilt utility. This allows easier patch development.","title":"_quilt"},{"location":"software/rpm-development-guide/#packaging-organization-examples","text":"","title":"Packaging Organization Examples"},{"location":"software/rpm-development-guide/#use-case-1-packaging-an-upstream-source-tarball","text":"When the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains: rsv/ osg/ rsv.spec upstream/ developer.tarball.source","title":"Use Case 1: Packaging an Upstream Source Tarball"},{"location":"software/rpm-development-guide/#use-case-2-passing-through-a-source-rpm","text":"When the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the globus-common source RPM from EPEL. Its package directory contains: globus-common/ upstream/ epel.srpm.source","title":"Use Case 2: Passing Through a Source RPM"},{"location":"software/rpm-development-guide/#use-case-3-modifying-a-source-rpm","text":"When the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the osg directory. For example, we use this mechanism for the globus-ftp-client package, originally obtained from EPEL. Its package directory contains: globus-ftp-client/ osg/ globus-ftp-client.spec 1853-ssh-bin.patch upstream/ epel.srpm.source","title":"Use Case 3: Modifying a Source RPM"},{"location":"software/rpm-development-guide/#build-process","text":"All necessary information to create the package will be committed to the OSG Software Packaging repository (see below) The OSG build tools will take those files, create a source RPM, and submit it to our Koji build system Developers may use rpmbuild and mock for faster iterative development before submitting the package to Koji. osg-build may be used as a wrapper script around rpmbuild and mock .","title":"Build Process"},{"location":"software/rpm-development-guide/#osg-software-repository","text":"OSG Operations maintains the Yum repositories that contain our source and binary RPMs at https://repo.osg-htc.org/osg/ and are mirrored at other institutions as well.","title":"OSG Software Repository"},{"location":"software/rpm-development-guide/#release-levels","text":"Every package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level ( osg-development ). The members of the OSG Software and Release teams may promote packages through the release levels, as per our Release Policy page .","title":"Release Levels"},{"location":"software/rpm-development-guide/#packaging-conventions","text":"In addition to adhering to the Fedora Packaging Guidelines (FPG), we have a few rules and guidelines of our own: When we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows: The version of the original RPM remains unchanged The release is composed of three parts: ORIGINALRELEASE.OSGRELEASE We add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use %{?dist} in the Release field) Example: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes foobar-3.0.5-1.1.osg33.el6 Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like: Release : 1.2 %{?dist}","title":"Packaging Conventions"},{"location":"software/rpm-development-guide/#packaging-for-multiple-distro-versions","text":"","title":"Packaging for Multiple Distro Versions"},{"location":"software/rpm-development-guide/#conditionalizing-spec-files","text":"Some packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this. The following macros are defined: Name Value (EL8) Value (EL9) %rhel 8 9 %el8 1 undefined or 0 %el9 undefined or 0 1 Here's how to use them: %if 0%{?el8} # this code will be executed on EL 8 only %endif %if 0%{?el9} # this code will be executed on EL 9 only %endif %if 0%{?rhel} >= 9 # this code will be executed on EL 9 and newer %endif (the 'else if' macro %elif does not parse on EL8 or older. The 0 before the %{?el9} is required to avoid a syntax error if the macro is undefined.)","title":"Conditionalizing spec files"},{"location":"software/rpm-development-guide/#renaming-or-removing-packages","text":"Occasionally we want to cause a package to be removed on update, or replaced by a package with a different name. For the most part, the Fedora Packaging Guidelines page on renames shows how to do that. The exception is that we do not have the equivalent of a fedora-obsolete-packages package, so in order to force the removal of an entire package (not a subpackage), we have to dummy out the package instead -- see below. (This should be a rare situation.) Note After doing a rename or a removal, you must update all the packages and subpackages that require the package being removed or renamed, and change or remove the requirements as appropriate. To find packages that require the old package at run time, set up a host with the OSG repos and install the yum-utils RPM. On EL8 and newer, run: $ repoquery --whatrequires $OLDPACKAGE On EL7, run $ repoquery --plugins --whatrequires $OLDPACKAGE To find packages that require the old package at build time, install osg-build , and do this from a checkout of the OSG repos: $ osg-build prebuild * $ for srpm in */_final_srpm_contents/*.src.rpm ; do \\ echo \"***** $srpm *****\"; \\ rpm -q --requires -p $srpm | grep -w $OLDPACKAGE; \\ done (examine the output to avoid false matches) Note Carefully test these changes, including places where the old package may be brought in indirectly.","title":"Renaming or Removing Packages"},{"location":"software/rpm-development-guide/#dummying-out-a-package","text":"In order to forcibly remove an entire package with no replacement, you have to replace the package with one that does nothing. This is because there is no package that will \"obsolete\" the old package. Do the following for the main package and any subpackages it may have: Change the Summary to \"Dummy package\" Change the %description to: This is an empty package created for $REASONS It may safely be removed. Where $REASONS is a description of why you need this dummy package Remove all Requires and Obsoletes lines Do not remove Provides lines Remove %pre and %post scriptlets Unless there is a good reason not to, remove %preun and %postun scriptlets Empty the %files section","title":"Dummying out a package"},{"location":"software/upcoming-to-main/","text":"Promoting Packages from Upcoming to Main \u00b6 Sometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move: If needed, move the software from upcoming to trunk and release using the usual process: Merge changes to the package in SVN from branches/upcoming to trunk. Build the package from trunk. Follow the normal process to prepare a build for release (including development testing, promotion, etc.). On release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos: Untag from all upcoming repos the version of the package corresponding to the version that was released in main. (Do NOT untag from the osg-upcoming-elN-release-X.Y.Z tags) Also, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos. If you do not have the privileges to untag from upcoming-release, someone on the Release Team can help. (These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos). Unless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.","title":"Upcoming to main"},{"location":"software/upcoming-to-main/#promoting-packages-from-upcoming-to-main","text":"Sometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move: If needed, move the software from upcoming to trunk and release using the usual process: Merge changes to the package in SVN from branches/upcoming to trunk. Build the package from trunk. Follow the normal process to prepare a build for release (including development testing, promotion, etc.). On release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos: Untag from all upcoming repos the version of the package corresponding to the version that was released in main. (Do NOT untag from the osg-upcoming-elN-release-X.Y.Z tags) Also, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos. If you do not have the privileges to untag from upcoming-release, someone on the Release Team can help. (These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos). Unless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.","title":"Promoting Packages from Upcoming to Main"},{"location":"software/user-certs/","text":"User Certificates \u00b6 Note This document describes how to get and set up a personal certificate (also called a grid user certificate). For instructions on how to get host certificates, see the Host Certificates document . Getting a User Certificate \u00b6 This section describes how to get and set up a personal (user) certificate. A user certificate is a type of SSL certificate -- the same technology used for identifying websites when using HTTPS -- but identifying a human instead of a host. This is used to access some grid resources, such as: OSG Topology (for viewing private contact information) The osg-notify tool for sending mass emails The GGUS ticketing system The Koji build system for the OSG Software Stack User certs may also be useful for debugging storage access (such as XRootD). Currently, you can get a user certificate from CILogon. Know your responsibilities \u00b6 If your account or user certificate is compromised, you must notify the issuer of your certificate. In addition, you should update your certificate and revoke the old certificate if any of the information in the certificate (such as name or email address) change. For the CILogon RA send email to ca@cilogon.org . Additional responsibilities required by the CILogon CA are given on their page . Getting a certificate from CILogon \u00b6 You will have to obtain your user certificate using the CILogon web UI . Follow the steps below to get an user certificate: Open the CILogon page, https://cilogon.org , in your browser of choice First, either search for your institution and select it or scroll through list and do the same. . Warning Do not use Google, GitHub, or ORCID as providers since they are not widely supported in the OSG. If your institution is not on the list, please contact your institution's IT support to see if they can support CILogon. Click the Log On button and enter your institutional credentials if prompted. After successfully entering your credentials, click on the \"Create Password-Protected Certificate\" link Enter a password that is at least 12 characters long and then click on the Get New Certificate button. Click the Download Your Certificate button to download your certificate in .p12 format. The certificate will be protected using the password you entered in the previous step. Certificate formats \u00b6 Your user certificate can be stored in a few different formats: CILogon will give you a certificate in the PKCS12 format (file extension .p12 ). The PKCS12 format stores the certificate and private key in a single file along with an optional certificate chain. This is the file format that can be loaded into your browser for accessing Topology, GGUS, or the Koji web interace with. Most command line tools use the PEM format (file extension .pem ). The PEM format stores the public certificate in one file, and the private key in another file. To convert a PKCS12 file to PEM files, do the following: First, extract your user certificate from your PKCS12 file by running the following command. You'll be prompted for the password you used to create the certificate. The invocation assumes that the PKCS12 file is called usercred.p12 . After running, the PEM certificate will be written to usercert.pem . user@host $ openssl pkcs12 -in usercred.p12 -out usercert.pem -nodes -clcerts -nokeys Enter Import Password: MAC verified OK Second, extract the private key by running the following command. You'll be prompted for two different passwords. The first prompt will be for the password that you used to create the certificate. The second prompt will be for the password that will encrypt the PEM certificate that will be created. As before, the invocation assumes that your PKCS12 certificate is located in usercred.p12 . After running, the PEM certificate with your private key will be written to userkey.pem . user@host $ openssl pkcs12 -in usercred.p12 -out userkey.pem -nocerts Enter Import Password: MAC verified OK Enter PEM pass phrase: Verifying - Enter PEM pass phrase: Using Your User Certificate \u00b6 The standard location to place user certificates is in the .globus subdirectory of your home directory: user@host $ mkdir -p ~/.globus user@host $ cp userkey.pem ~/.globus/ user@host $ cp usercert.pem ~/.globus/ user@host $ cp usercred.p12 ~/.globus/ user@host $ chmod go-rwx ~/.globus/userkey.pem ~/.globus/usercred.p12 In order to find the Distinguished Name (DN), issuer and lifetime of a certificate: user@host $ openssl x509 -in ~/.globus/usercert.pem -noout -subject -issuer -enddate To generate a proxy use the command voms-proxy-init . user@host $ voms-proxy-init Revoking Your User Certificate \u00b6 If the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate. In addition, if your name or email address changes, you must revoke your certificate and get a new one with the correct information. If you have a CILogon issued certificate, contact ca@cilogon.org in order revoke your certificate. If you received a certificate from another CA, please contact the CA to initiate a certificate revocation.","title":"User Certificates"},{"location":"software/user-certs/#user-certificates","text":"Note This document describes how to get and set up a personal certificate (also called a grid user certificate). For instructions on how to get host certificates, see the Host Certificates document .","title":"User Certificates"},{"location":"software/user-certs/#getting-a-user-certificate","text":"This section describes how to get and set up a personal (user) certificate. A user certificate is a type of SSL certificate -- the same technology used for identifying websites when using HTTPS -- but identifying a human instead of a host. This is used to access some grid resources, such as: OSG Topology (for viewing private contact information) The osg-notify tool for sending mass emails The GGUS ticketing system The Koji build system for the OSG Software Stack User certs may also be useful for debugging storage access (such as XRootD). Currently, you can get a user certificate from CILogon.","title":"Getting a User Certificate"},{"location":"software/user-certs/#know-your-responsibilities","text":"If your account or user certificate is compromised, you must notify the issuer of your certificate. In addition, you should update your certificate and revoke the old certificate if any of the information in the certificate (such as name or email address) change. For the CILogon RA send email to ca@cilogon.org . Additional responsibilities required by the CILogon CA are given on their page .","title":"Know your responsibilities"},{"location":"software/user-certs/#getting-a-certificate-from-cilogon","text":"You will have to obtain your user certificate using the CILogon web UI . Follow the steps below to get an user certificate: Open the CILogon page, https://cilogon.org , in your browser of choice First, either search for your institution and select it or scroll through list and do the same. . Warning Do not use Google, GitHub, or ORCID as providers since they are not widely supported in the OSG. If your institution is not on the list, please contact your institution's IT support to see if they can support CILogon. Click the Log On button and enter your institutional credentials if prompted. After successfully entering your credentials, click on the \"Create Password-Protected Certificate\" link Enter a password that is at least 12 characters long and then click on the Get New Certificate button. Click the Download Your Certificate button to download your certificate in .p12 format. The certificate will be protected using the password you entered in the previous step.","title":"Getting a certificate from CILogon"},{"location":"software/user-certs/#certificate-formats","text":"Your user certificate can be stored in a few different formats: CILogon will give you a certificate in the PKCS12 format (file extension .p12 ). The PKCS12 format stores the certificate and private key in a single file along with an optional certificate chain. This is the file format that can be loaded into your browser for accessing Topology, GGUS, or the Koji web interace with. Most command line tools use the PEM format (file extension .pem ). The PEM format stores the public certificate in one file, and the private key in another file. To convert a PKCS12 file to PEM files, do the following: First, extract your user certificate from your PKCS12 file by running the following command. You'll be prompted for the password you used to create the certificate. The invocation assumes that the PKCS12 file is called usercred.p12 . After running, the PEM certificate will be written to usercert.pem . user@host $ openssl pkcs12 -in usercred.p12 -out usercert.pem -nodes -clcerts -nokeys Enter Import Password: MAC verified OK Second, extract the private key by running the following command. You'll be prompted for two different passwords. The first prompt will be for the password that you used to create the certificate. The second prompt will be for the password that will encrypt the PEM certificate that will be created. As before, the invocation assumes that your PKCS12 certificate is located in usercred.p12 . After running, the PEM certificate with your private key will be written to userkey.pem . user@host $ openssl pkcs12 -in usercred.p12 -out userkey.pem -nocerts Enter Import Password: MAC verified OK Enter PEM pass phrase: Verifying - Enter PEM pass phrase:","title":"Certificate formats"},{"location":"software/user-certs/#using-your-user-certificate","text":"The standard location to place user certificates is in the .globus subdirectory of your home directory: user@host $ mkdir -p ~/.globus user@host $ cp userkey.pem ~/.globus/ user@host $ cp usercert.pem ~/.globus/ user@host $ cp usercred.p12 ~/.globus/ user@host $ chmod go-rwx ~/.globus/userkey.pem ~/.globus/usercred.p12 In order to find the Distinguished Name (DN), issuer and lifetime of a certificate: user@host $ openssl x509 -in ~/.globus/usercert.pem -noout -subject -issuer -enddate To generate a proxy use the command voms-proxy-init . user@host $ voms-proxy-init","title":"Using Your User Certificate"},{"location":"software/user-certs/#revoking-your-user-certificate","text":"If the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate. In addition, if your name or email address changes, you must revoke your certificate and get a new one with the correct information. If you have a CILogon issued certificate, contact ca@cilogon.org in order revoke your certificate. If you received a certificate from another CA, please contact the CA to initiate a certificate revocation.","title":"Revoking Your User Certificate"}]}